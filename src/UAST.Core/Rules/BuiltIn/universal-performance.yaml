# ╔══════════════════════════════════════════════════════════════════════════════╗
# ║                   UAST-Grep Universal Performance Rules                      ║
# ║              Performance anti-pattern detection for 15+ languages            ║
# ╚══════════════════════════════════════════════════════════════════════════════╝
#
# Generated: 2026-01-04
# Total Rules: 1,137
# Impact Levels: Critical (10x+), High (5-10x), Medium (2-5x), Low (<2x)
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ LANGUAGES COVERED                                                           │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Python (68)      │ JavaScript (61)  │ Java (62)        │ Kotlin (45)       │
# │ C/C++ (75)       │ Go (60)          │ Rust (125)       │ C# (115)          │
# │ Scala (70)       │ Ruby (65)        │ Clojure (65)     │ Lua (62)          │
# │ PowerShell (54)  │ Shell/Bash (60)  │ Infrastructure (52)                  │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ PERFORMANCE CATEGORIES                                                      │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ • Memory Allocation (heap vs stack, pooling, preallocation)                 │
# │ • Loop Optimization (invariants, unrolling, vectorization)                  │
# │ • String Handling (concatenation, interning, builders)                      │
# │ • Collection Operations (capacity, iteration, lookup complexity)            │
# │ • Concurrency (goroutine leaks, lock contention, async overhead)            │
# │ • I/O Operations (buffering, streaming, connection pooling)                 │
# │ • Cache Efficiency (data locality, false sharing, alignment)                │
# │ • Compiler Hints (inlining, type hints, escape analysis)                    │
# │ • Algorithm Complexity (O(n²) patterns, unnecessary work)                   │
# │ • Language-Specific (SIMD, move semantics, lazy evaluation)                 │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# Usage:
#   uast-grep scan -r rules/universal-performance.yaml ./src
#   uast-grep scan -r rules/universal-performance.yaml --language rust ./src
#
# Rule Format:
#   id:       [language]-perf-[descriptive-name]
#   language: target language (c, cpp, go, rust, etc.) or "*" for universal
#   severity: error | warning | info
#   message:  "Performance: Description of the anti-pattern"
#   tags:     [performance, category, impact-level]
#   rule:     UAST pattern (kind, pattern, has, any, all, not)
#   note:     Optimization guidance with benchmarks and examples

# =============================================================================
# UNIVERSAL RULES - Patterns that apply across all languages
# =============================================================================

# -----------------------------------------------------------------------------
# LOOP INEFFICIENCIES - O(n^2) patterns, allocation in loops
# -----------------------------------------------------------------------------

---
id: universal-nested-loop-lookup
language: "*"
severity: error
message: "O(n^2) nested loop pattern - consider using Set/Map for O(1) lookups"
tags:
  - performance
  - loop
  - cpu
  - algorithm
  - critical
rule:
  kind: ForStatement
  has:
    kind: ForStatement
    has:
      any:
        - pattern: "includes"
        - pattern: "indexOf"
        - pattern: "contains"
        - pattern: "in_array"
        - pattern: "find"
        - pattern: "search"
note: |
  Nested loops with linear search create O(n^2) or worse complexity.
  Convert the inner collection to a Set or Map for O(1) lookups.

  Example fix (JavaScript):
    // Before: O(n^2)
    for (item of items) { if (others.includes(item.id)) {...} }
    // After: O(n)
    const otherSet = new Set(others);
    for (item of items) { if (otherSet.has(item.id)) {...} }

---
id: universal-allocation-in-loop
language: "*"
severity: warning
message: "Object allocation inside loop causes GC pressure"
tags:
  - performance
  - loop
  - memory
  - allocation
  - high
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "new "
      - pattern: "malloc"
      - pattern: "alloc"
      - pattern: "make("
note: |
  Creating objects inside loops increases garbage collection pressure.
  Consider object pooling, moving allocation outside the loop, or
  reusing objects where semantically correct.

  Exception: Object creation required for each iteration (e.g., result building)

---
id: universal-regex-in-loop
language: "*"
severity: warning
message: "Regex compilation inside loop - compile once outside"
tags:
  - performance
  - loop
  - regex
  - cpu
  - high
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "new RegExp"
      - pattern: "Pattern.compile"
      - pattern: "re.compile"
      - pattern: "Regex::new"
      - pattern: "Regexp.new"
      - pattern: "qr/"
note: |
  Regex patterns should be compiled once and reused.
  Compiling inside a loop wastes CPU cycles on each iteration.

  Fix: Move regex compilation outside the loop as a constant.

---
id: universal-function-in-condition
language: "*"
severity: warning
message: "Function call in loop condition - cache the result"
tags:
  - performance
  - loop
  - cpu
  - medium
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "len("
      - pattern: ".length"
      - pattern: ".count"
      - pattern: ".size()"
      - pattern: "strlen"
      - pattern: "Count()"
note: |
  Function calls in loop conditions are re-evaluated each iteration.
  If the result doesn't change, cache it before the loop.

  Example:
    // Before
    for (int i = 0; i < list.size(); i++)
    // After
    int size = list.size();
    for (int i = 0; i < size; i++)

---
id: universal-lock-in-loop
language: "*"
severity: warning
message: "Lock acquisition inside loop - move lock outside if possible"
tags:
  - performance
  - concurrency
  - lock
  - contention
  - medium
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "lock("
      - pattern: "synchronized"
      - pattern: "Mutex"
      - pattern: ".lock()"
      - pattern: "acquire"
note: |
  Acquiring and releasing locks for each iteration adds overhead.
  If the entire loop operation can be atomic, lock once outside.

  Exception: When holding the lock for the entire loop would cause
  unacceptable contention or block other threads too long.

# -----------------------------------------------------------------------------
# MEMORY/ALLOCATION - Large allocations, boxing, copies
# -----------------------------------------------------------------------------

---
id: universal-large-object-copy
language: "*"
severity: warning
message: "Large object copy detected - consider pass by reference"
tags:
  - performance
  - memory
  - copy
  - medium
rule:
  any:
    - pattern: "clone()"
    - pattern: ".copy()"
    - pattern: "dup"
    - pattern: "deepcopy"
    - pattern: "structuredClone"
note: |
  Cloning large objects is expensive. Consider:
  - Pass by reference when mutation isn't needed
  - Use structural sharing (immutable data structures)
  - Clone only the parts that will be modified

---
id: universal-closure-capture-loop
language: "*"
severity: warning
message: "Closure capturing variables in loop - creates allocation per iteration"
tags:
  - performance
  - memory
  - closure
  - allocation
  - medium
rule:
  kind: ForStatement
  has:
    any:
      - kind: ArrowFunction
      - kind: LambdaExpression
      - pattern: "=>"
      - pattern: "->"
      - pattern: "lambda"
note: |
  Closures that capture variables from their enclosing scope allocate
  memory for the captured context. In loops, this creates a new
  closure object for each iteration.

  Mitigation:
  - Use method references instead of lambdas where possible
  - Avoid capturing mutable state
  - Consider moving closure creation outside the loop

# -----------------------------------------------------------------------------
# COLLECTION MISUSE - Wrong data structure, multiple enumeration
# -----------------------------------------------------------------------------

---
id: universal-list-as-lookup
language: "*"
severity: warning
message: "Using list/array for lookups - use Set/Map for O(1) access"
tags:
  - performance
  - collection
  - algorithm
  - high
rule:
  kind: CallExpression
  has:
    any:
      - pattern: ".includes("
      - pattern: ".contains("
      - pattern: ".indexOf("
      - pattern: ".index("
note: |
  Linear search in lists/arrays is O(n) per lookup.
  For repeated lookups, convert to Set/Map for O(1) access.

  Critical when:
  - Checking membership multiple times
  - Inside loops (creates O(n^2))
  - Large collections (>100 elements)

---
id: universal-linkedlist-random-access
language: "*"
severity: warning
message: "Random access on linked list - use array-based structure"
tags:
  - performance
  - collection
  - algorithm
  - high
rule:
  any:
    - pattern: "LinkedList"
    - pattern: "linked_list"
note: |
  LinkedList has O(n) random access time. Use ArrayList/Vec/Array for:
  - Index-based access: list[i] or list.get(i)
  - Iteration by index: for (int i = 0; i < n; i++)

  LinkedList is only faster for:
  - Frequent insertion/removal at known positions (with iterator)
  - Queue/deque operations (though ArrayDeque is often still faster)

---
id: universal-count-vs-any
language: "*"
severity: info
message: "Using count/length to check existence - use any/exists/isEmpty"
tags:
  - performance
  - collection
  - algorithm
  - low
rule:
  any:
    - pattern: ".Count() > 0"
    - pattern: ".count() > 0"
    - pattern: ".length > 0"
    - pattern: ".size() > 0"
    - pattern: ".Count() == 0"
    - pattern: ".length == 0"
note: |
  Counting all elements when you only need to know if any exist is wasteful.
  Use short-circuit methods that stop at first match.

  Fix by language:
  - C#: collection.Any() instead of collection.Count() > 0
  - Java: !collection.isEmpty() instead of collection.size() > 0
  - JavaScript: array.length is O(1), but for iterables use .some()

# -----------------------------------------------------------------------------
# I/O PATTERNS - Sync I/O, unbuffered, blocking
# -----------------------------------------------------------------------------

---
id: universal-sync-io-loop
language: "*"
severity: warning
message: "Synchronous I/O inside loop - batch or parallelize"
tags:
  - performance
  - io
  - blocking
  - high
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "readFile"
      - pattern: "writeFile"
      - pattern: "fetch"
      - pattern: "http.get"
      - pattern: "requests.get"
      - pattern: "File.read"
      - pattern: "fread"
      - pattern: "fwrite"
note: |
  Sequential I/O operations are slow. Consider:
  - Batch reads/writes: Read multiple records at once
  - Parallel I/O: Use async/await with Promise.all or similar
  - Buffering: Aggregate writes and flush periodically

---
id: universal-unbuffered-io
language: "*"
severity: warning
message: "Unbuffered I/O operations - use buffered reader/writer"
tags:
  - performance
  - io
  - buffering
  - high
rule:
  any:
    - pattern: "read(1)"
    - pattern: "readByte"
    - pattern: "getc"
    - pattern: "fgetc"
    - pattern: "write(1)"
    - pattern: "putc"
    - pattern: "fputc"
note: |
  Unbuffered I/O performs a system call for each byte.
  Use buffered I/O for significant speedup.

  Fix by language:
  - Python: Use io.BufferedReader/BufferedWriter
  - Java: Use BufferedInputStream/BufferedOutputStream
  - C: Use setvbuf() or fread/fwrite with larger buffers
  - Rust: Use BufReader/BufWriter
  - Go: Use bufio.Reader/Writer

---
id: universal-n-plus-one-query
language: "*"
severity: error
message: "N+1 query pattern - use JOIN or batch loading"
tags:
  - performance
  - database
  - orm
  - io
  - critical
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "SELECT"
      - pattern: "query"
      - pattern: "find("
      - pattern: "get("
      - pattern: "fetch"
      - pattern: "load"
note: |
  The N+1 query problem occurs when loading N items, then executing
  a separate query for each item's related data (N+1 total queries).

  Fix:
  - Use JOIN queries to load related data together
  - Use batch loading (IN clause with all IDs)
  - ORM-specific: Use eager loading/includes
    - JPA: @BatchSize, JOIN FETCH, @EntityGraph
    - Rails: includes(), eager_load()
    - Django: select_related(), prefetch_related()

  Impact: 100+ queries become 1-2 queries

---
id: universal-reflection-hot-path
language: "*"
severity: warning
message: "Reflection in hot path - cache reflected members"
tags:
  - performance
  - reflection
  - jit
  - high
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "GetMethod"
      - pattern: "GetProperty"
      - pattern: "getDeclaredMethod"
      - pattern: "getDeclaredField"
      - pattern: "getattr"
      - pattern: "Method.invoke"
      - pattern: "PropertyInfo"
note: |
  Reflection is 3-100x slower than direct calls and prevents JIT inlining.

  Fix:
  - Cache MethodInfo/PropertyInfo objects outside loops
  - Use compiled expressions or delegates
  - Consider source generators (C#) or annotation processors (Java)

# -----------------------------------------------------------------------------
# CONCURRENCY - Thread creation overhead, atomic operations
# -----------------------------------------------------------------------------

---
id: universal-mutex-for-counter
language: "*"
severity: info
message: "Mutex for simple counter - use atomic operations"
tags:
  - performance
  - concurrency
  - lock
  - medium
rule:
  any:
    - pattern: "synchronized"
    - pattern: "lock("
    - pattern: "pthread_mutex"
note: |
  For simple counter increments, atomic operations are 3-10x faster
  than mutex-based synchronization.

  Use instead:
  - Java: AtomicInteger, AtomicLong
  - C#: Interlocked.Increment
  - C++: std::atomic
  - Go: atomic.AddInt64
  - Rust: AtomicUsize

---
id: universal-thread-create-loop
language: "*"
severity: warning
message: "Thread creation in loop - use thread pool"
tags:
  - performance
  - concurrency
  - thread
  - high
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "new Thread"
      - pattern: "Thread.start"
      - pattern: "pthread_create"
      - pattern: "spawn"
note: |
  Creating threads is expensive (~1ms per thread, 2-8KB stack each).
  Use thread pools for task parallelism.

  Fix by language:
  - Java: ExecutorService with fixed thread pool
  - C#: Task.Run() uses ThreadPool automatically
  - Python: concurrent.futures.ThreadPoolExecutor
  - Rust: Rayon or thread pool crates

# =============================================================================
# PYTHON-SPECIFIC RULES
# =============================================================================

---
id: python-gil-threading
language: python
severity: warning
message: "Python threading for CPU-bound work - use multiprocessing"
tags:
  - performance
  - concurrency
  - python
  - gil
  - high
rule:
  any:
    - pattern: "threading.Thread"
    - pattern: "ThreadPoolExecutor"
note: |
  Python's Global Interpreter Lock (GIL) prevents true parallel CPU execution.
  For CPU-bound tasks, use multiprocessing instead of threading.

  threading is fine for I/O-bound tasks where threads block on I/O.

  Note: Python 3.13+ has experimental free-threaded mode.

---
id: python-string-concat-loop
language: python
severity: error
message: "String concatenation in loop - use join() or list"
tags:
  - performance
  - string
  - memory
  - critical
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "+="
      - pattern: "$str + "
note: |
  String concatenation with += in loops causes O(n^2) time complexity
  because strings are immutable, requiring a full copy on each iteration.

  Fix: Use list and ''.join()
    # Before: O(n^2)
    result = ""
    for item in items:
        result += str(item)

    # After: O(n)
    result = ''.join(str(item) for item in items)

---
id: python-list-append-loop
language: python
severity: info
message: "List append in loop - consider list comprehension"
tags:
  - performance
  - loop
  - python
  - medium
rule:
  kind: ForStatement
  has:
    pattern: ".append("
note: |
  List comprehensions are ~30% faster than append in loops due to
  reduced function call overhead per iteration.

  # Before
  result = []
  for x in items:
      result.append(transform(x))

  # After
  result = [transform(x) for x in items]

---
id: python-list-pop-front
language: python
severity: error
message: "list.pop(0) is O(n) - use collections.deque for O(1)"
tags:
  - performance
  - collection
  - algorithm
  - critical
rule:
  pattern: ".pop(0)"
note: |
  list.pop(0) is O(n) because all elements must be shifted.
  collections.deque.popleft() is O(1).

  For queue operations, deque is 40,000x faster than list.

---
id: python-list-as-queue
language: python
severity: error
message: "Using list as queue - use collections.deque"
tags:
  - performance
  - collection
  - algorithm
  - critical
rule:
  any:
    - pattern: ".insert(0,"
    - pattern: ".pop(0)"
note: |
  list.insert(0, x) and list.pop(0) are O(n) operations.
  collections.deque provides O(1) appendleft() and popleft().

  # Before - O(n) per operation
  queue = []
  queue.insert(0, item)  # or queue.append(item)
  item = queue.pop(0)

  # After - O(1) per operation
  from collections import deque
  queue = deque()
  queue.appendleft(item)
  item = queue.popleft()

---
id: python-list-membership
language: python
severity: warning
message: "Repeated 'in' checks on list - convert to set"
tags:
  - performance
  - collection
  - algorithm
  - high
rule:
  kind: ForStatement
  has:
    pattern: "in $LIST"
note: |
  List membership check is O(n). Set membership is O(1).
  Convert to set when checking membership multiple times.

  # Before - O(n) per check
  for item in items:
      if item in large_list: ...

  # After - O(1) per check
  large_set = set(large_list)
  for item in items:
      if item in large_set: ...

---
id: python-isinstance-union
language: python
severity: info
message: "isinstance with union types is 4x slower - use tuple form"
tags:
  - performance
  - python
  - typing
  - medium
rule:
  pattern: "isinstance($VAR, $TYPE | $TYPE2)"
note: |
  isinstance() with union types (int | str) is 4x slower than tuple form.

  # Slower
  isinstance(x, int | str)

  # Faster
  isinstance(x, (int, str))

---
id: python-no-slots
language: python
severity: info
message: "Data class without __slots__ - consider adding for memory/speed"
tags:
  - performance
  - memory
  - python
  - medium
rule:
  pattern: "class $NAME:"
note: |
  Classes without __slots__ use a dict for instance attributes,
  adding memory overhead and slower attribute access.

  __slots__ reduces memory usage and improves attribute lookup speed.

  @dataclass(slots=True)  # Python 3.10+
  class Point:
      x: float
      y: float

# =============================================================================
# JAVASCRIPT/TYPESCRIPT-SPECIFIC RULES
# =============================================================================

---
id: javascript-dom-in-loop
language: javascript
severity: error
message: "DOM access inside loop causes layout thrashing"
tags:
  - performance
  - dom
  - browser
  - critical
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "document.querySelector"
      - pattern: "document.getElementById"
      - pattern: "getElementsBy"
      - pattern: "offsetWidth"
      - pattern: "offsetHeight"
      - pattern: "getBoundingClientRect"
      - pattern: "innerHTML"
      - pattern: "appendChild"
note: |
  DOM access in loops causes layout thrashing - each read forces
  a synchronous layout recalculation.

  Fix:
  - Batch all reads before any writes
  - Use DocumentFragment for batch insertions
  - Cache DOM element references outside loops
  - Use requestAnimationFrame for visual updates

---
id: javascript-innerhtml-loop
language: javascript
severity: error
message: "innerHTML modification in loop - O(n^2) HTML parsing"
tags:
  - performance
  - dom
  - browser
  - critical
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "innerHTML +="
      - pattern: "innerHTML ="
note: |
  innerHTML += in loops causes O(n^2) HTML parsing and DOM reconstruction.
  Use DocumentFragment or build string and assign once.

  // Before - O(n^2)
  for (item of items) {
    container.innerHTML += `<div>${item}</div>`;
  }

  // After - O(n)
  const fragment = document.createDocumentFragment();
  for (item of items) {
    const div = document.createElement('div');
    div.textContent = item;
    fragment.appendChild(div);
  }
  container.appendChild(fragment);

---
id: javascript-await-loop
language: javascript
severity: warning
message: "Sequential await in loop - use Promise.all for parallel execution"
tags:
  - performance
  - async
  - io
  - high
rule:
  kind: ForStatement
  has:
    pattern: "await"
note: |
  Awaiting inside a loop executes operations sequentially.
  When operations are independent, run them in parallel.

  // Before - Sequential, N * latency
  for (const url of urls) {
    await fetch(url);
  }

  // After - Parallel, max(latencies)
  await Promise.all(urls.map(url => fetch(url)));

---
id: javascript-array-chain
language: javascript
severity: info
message: "Multiple chained array methods - consider single reduce"
tags:
  - performance
  - array
  - iteration
  - medium
rule:
  pattern: "$ARR.filter($$$).map($$$).filter($$$)"
note: |
  Multiple chained array methods create intermediate arrays.
  Consider using a single reduce() or transducer pattern.

  // Before - 3 iterations, 2 intermediate arrays
  arr.filter(x => x > 0).map(x => x * 2).filter(x => x < 100)

  // After - 1 iteration, no intermediate arrays
  arr.reduce((acc, x) => {
    if (x > 0) {
      const doubled = x * 2;
      if (doubled < 100) acc.push(doubled);
    }
    return acc;
  }, []);

---
id: javascript-string-concat-loop
language: javascript
severity: warning
message: "String concatenation in loop - use array.join()"
tags:
  - performance
  - string
  - memory
  - high
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "$STR +="
      - pattern: "$STR = $STR +"
note: |
  String concatenation with += in loops creates O(n^2) allocations.
  Use array and join() for linear time.

  // Before - O(n^2)
  let result = "";
  for (const item of items) {
    result += item;
  }

  // After - O(n)
  const parts = [];
  for (const item of items) {
    parts.push(item);
  }
  const result = parts.join('');

---
id: typescript-barrel-import
language: typescript
severity: warning
message: "Barrel file import can increase bundle size by 85%"
tags:
  - performance
  - bundle
  - import
  - high
rule:
  any:
    - pattern: "import { $NAME } from './index'"
    - pattern: "import { $NAME } from './'"
note: |
  Barrel imports (from './index') load entire module graphs,
  preventing tree-shaking and bloating bundles by up to 85%.

  // Before - loads everything
  import { Button } from './components';

  // After - only loads Button
  import { Button } from './components/Button';

---
id: javascript-full-import
language: javascript
severity: warning
message: "Full library import - use specific imports for tree-shaking"
tags:
  - performance
  - bundle
  - import
  - high
rule:
  any:
    - pattern: "import _ from 'lodash'"
    - pattern: "import * as _ from 'lodash'"
    - pattern: "import moment from 'moment'"
note: |
  Importing entire libraries prevents tree-shaking.
  Use specific imports or smaller alternatives.

  // Before - includes entire library
  import _ from 'lodash';
  _.debounce(fn);

  // After - only includes debounce
  import debounce from 'lodash/debounce';

  // moment.js is 300KB - use dayjs (2KB) or date-fns instead

# =============================================================================
# REACT-SPECIFIC RULES (TSX/JSX)
# =============================================================================

---
id: react-inline-function-prop
language: javascript
severity: warning
message: "Inline function prop - breaks memoization"
tags:
  - performance
  - react
  - render
  - high
rule:
  pattern: "<$COMPONENT $$$onClick={() => $$$}"
note: |
  Inline arrow functions create new function instances on every render,
  breaking React.memo() and PureComponent optimizations.

  // Before - new function every render
  <Button onClick={() => handleClick(id)} />

  // After - stable reference
  const handleButtonClick = useCallback(() => handleClick(id), [id]);
  <Button onClick={handleButtonClick} />

---
id: react-inline-object-prop
language: javascript
severity: warning
message: "Inline object prop - breaks memoization"
tags:
  - performance
  - react
  - render
  - high
rule:
  pattern: "<$COMPONENT $$$style={{ $$$}}"
note: |
  Inline objects create new references on every render,
  breaking memoization.

  // Before - new object every render
  <div style={{ color: 'red' }} />

  // After - stable reference
  const redStyle = useMemo(() => ({ color: 'red' }), []);
  <div style={redStyle} />

---
id: react-missing-key
language: javascript
severity: error
message: "Missing key prop in list - causes full re-render"
tags:
  - performance
  - react
  - render
  - critical
rule:
  pattern: "$ARR.map($ITEM => <$COMPONENT"
  not:
    pattern: "key="
note: |
  Lists without key props force React to re-render all items.
  Use stable, unique keys for efficient reconciliation.

  // Before - O(n) on any change
  items.map(item => <Item data={item} />)

  // After - O(1) updates
  items.map(item => <Item key={item.id} data={item} />)

---
id: react-index-as-key
language: javascript
severity: warning
message: "Array index as key - causes incorrect updates on reorder"
tags:
  - performance
  - react
  - render
  - high
rule:
  pattern: "$ARR.map(($ITEM, $INDEX) => <$COMPONENT key={$INDEX}"
note: |
  Using array index as key causes incorrect updates when items are
  reordered, added, or removed.

  Use stable, unique identifiers from your data instead.

---
id: react-useeffect-no-cleanup
language: javascript
severity: warning
message: "useEffect without cleanup - potential memory leak"
tags:
  - performance
  - react
  - memory
  - leak
  - high
rule:
  pattern: "useEffect(() => { $$$addEventListener$$$}, [])"
  not:
    has:
      pattern: "return"
note: |
  useEffect with subscriptions/listeners but no cleanup function
  causes memory leaks when the component unmounts.

  useEffect(() => {
    window.addEventListener('resize', handler);
    return () => window.removeEventListener('resize', handler); // cleanup
  }, []);

# =============================================================================
# JAVA-SPECIFIC RULES
# =============================================================================

---
id: java-string-concat-loop
language: java
severity: warning
message: "String concatenation in loop - use StringBuilder"
tags:
  - performance
  - string
  - memory
  - high
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "$STR += $VALUE"
      - pattern: "$STR = $STR + $VALUE"
note: |
  String concatenation in loops is O(n^2) because strings are immutable.

  Use StringBuilder:
    StringBuilder sb = new StringBuilder();
    for (String item : items) {
        sb.append(item);
    }
    return sb.toString();

---
id: java-autobox-loop
language: java
severity: warning
message: "Boxing in loop - use primitive types"
tags:
  - performance
  - memory
  - boxing
  - medium
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "Integer $VAR"
      - pattern: "Long $VAR"
      - pattern: "Double $VAR"
note: |
  Wrapper types (Integer, Long) cause boxing/unboxing overhead.
  Each boxing allocates 16 bytes vs 4-8 for primitives.

  Use primitive types (int, long, double) in loops.

---
id: java-linkedlist-random-access
language: java
severity: warning
message: "LinkedList with index access - use ArrayList"
tags:
  - performance
  - collection
  - algorithm
  - high
rule:
  any:
    - pattern: "LinkedList<$T> $VAR"
    - pattern: "new LinkedList<"
note: |
  LinkedList has O(n) random access vs O(1) for ArrayList.
  ArrayList is faster in 99% of cases due to better cache locality.

  Use LinkedList only for frequent insertion/removal at known positions.

---
id: java-hashmap-no-capacity
language: java
severity: info
message: "HashMap without initial capacity when size is known"
tags:
  - performance
  - collection
  - allocation
  - medium
rule:
  pattern: "new HashMap<>()"
note: |
  HashMap without capacity causes multiple resize operations.
  When size is known, use: new HashMap<>(expectedSize / 0.75 + 1)

  Java 19+: Use HashMap.newHashMap(expectedSize)

---
id: java-stream-simple-loop
language: java
severity: info
message: "Stream for simple iteration - for-each may be faster"
tags:
  - performance
  - stream
  - iteration
  - low
rule:
  pattern: ".stream().forEach("
note: |
  For simple iterations, traditional for-each loop is often faster
  due to stream setup overhead.

  Reserve streams for complex transformations and filters.

---
id: java-stream-filter-count
language: java
severity: info
message: ".filter().count() for existence check - use anyMatch()"
tags:
  - performance
  - stream
  - algorithm
  - medium
rule:
  pattern: ".filter($$$).count() > 0"
note: |
  filter().count() > 0 processes all elements.
  anyMatch() short-circuits on first match.

  // Before - processes all elements
  stream.filter(x -> x > 0).count() > 0

  // After - stops at first match
  stream.anyMatch(x -> x > 0)

---
id: java-parallel-stream-small
language: java
severity: info
message: "parallelStream on small collection - overhead exceeds benefit"
tags:
  - performance
  - stream
  - concurrency
  - low
rule:
  pattern: ".parallelStream()"
note: |
  parallelStream() has fork/join overhead. Use only for:
  - Collections with 10,000+ elements
  - CPU-intensive operations per element
  - No blocking I/O (blocks common ForkJoinPool)

# =============================================================================
# C#-SPECIFIC RULES
# =============================================================================

---
id: csharp-async-void
language: csharp
severity: error
message: "async void method - use async Task instead"
tags:
  - performance
  - async
  - error-handling
  - critical
rule:
  pattern: "async void $NAME($$$)"
note: |
  async void methods cannot be awaited and exceptions cannot be caught.
  They're only appropriate for event handlers.

  Use async Task for all other async methods.

---
id: csharp-sync-over-async
language: csharp
severity: error
message: "Blocking on async (.Result/.Wait()) can cause deadlocks"
tags:
  - performance
  - async
  - deadlock
  - critical
rule:
  any:
    - pattern: "$TASK.Result"
    - pattern: "$TASK.Wait()"
    - pattern: "$TASK.GetAwaiter().GetResult()"
note: |
  Synchronously blocking on async operations can cause deadlocks in
  UI and ASP.NET contexts due to SynchronizationContext.

  Propagate async/await up the call stack instead:
    // Bad: var result = GetDataAsync().Result;
    // Good: var result = await GetDataAsync();

---
id: csharp-string-concat-loop
language: csharp
severity: warning
message: "String concatenation in loop - use StringBuilder"
tags:
  - performance
  - string
  - memory
  - high
rule:
  kind: ForStatement
  has:
    pattern: "$STR += $VALUE"
note: |
  String concatenation in loops creates O(n^2) allocations because
  strings are immutable in C#.

  Use StringBuilder:
    var sb = new StringBuilder();
    foreach (var item in items)
        sb.Append(item);
    return sb.ToString();

---
id: csharp-linq-multiple-enum
language: csharp
severity: warning
message: "Multiple enumeration of IEnumerable - materialize to list"
tags:
  - performance
  - linq
  - iteration
  - high
rule:
  pattern: "IEnumerable<$T> $VAR"
note: |
  IEnumerable re-executes the query on each enumeration.
  Materialize to List/Array if accessed multiple times.

  // Before - query runs twice
  IEnumerable<int> query = source.Where(x => x > 0);
  var count = query.Count();  // First enumeration
  var list = query.ToList();  // Second enumeration

  // After - query runs once
  var list = source.Where(x => x > 0).ToList();
  var count = list.Count;

---
id: csharp-linq-count-vs-any
language: csharp
severity: info
message: ".Count() > 0 - use .Any() for existence check"
tags:
  - performance
  - linq
  - algorithm
  - medium
rule:
  pattern: ".Count() > 0"
note: |
  Count() enumerates all elements. Any() stops at first element.

  // Before - O(n)
  collection.Count() > 0

  // After - O(1)
  collection.Any()

---
id: csharp-dict-containskey-indexer
language: csharp
severity: info
message: "ContainsKey + indexer - use TryGetValue for single lookup"
tags:
  - performance
  - collection
  - algorithm
  - medium
rule:
  pattern: |
    if ($DICT.ContainsKey($KEY)) {
      $VAR = $DICT[$KEY];
    }
note: |
  ContainsKey followed by indexer performs two lookups.
  TryGetValue does it in one.

  // Before - two lookups
  if (dict.ContainsKey(key)) {
    var value = dict[key];
  }

  // After - one lookup
  if (dict.TryGetValue(key, out var value)) {
    // use value
  }

---
id: csharp-list-no-capacity
language: csharp
severity: info
message: "List without capacity when size is known"
tags:
  - performance
  - collection
  - allocation
  - medium
rule:
  pattern: "new List<$T>()"
note: |
  List without initial capacity causes multiple resize operations.
  When final size is known, provide capacity.

  // Before - multiple resizes
  var list = new List<int>();

  // After - no resizes
  var list = new List<int>(expectedSize);

# =============================================================================
# GO-SPECIFIC RULES
# =============================================================================

---
id: go-goroutine-leak
language: go
severity: warning
message: "Goroutine without termination signal - potential leak"
tags:
  - performance
  - concurrency
  - memory
  - leak
  - high
rule:
  any:
    - pattern: "go func()"
note: |
  Goroutines without cancellation mechanisms leak memory.
  Each goroutine uses 2-8KB minimum.

  Fix:
  - Use context.Context with cancellation
  - Use channels for signaling
  - Ensure cleanup on component teardown

---
id: go-defer-in-loop
language: go
severity: warning
message: "defer inside loop - defers until function returns"
tags:
  - performance
  - memory
  - allocation
  - high
rule:
  kind: ForStatement
  has:
    pattern: "defer"
note: |
  defer in loop allocates per iteration and defers until function exit.
  For file/resource cleanup, extract to separate function.

  // Before - allocates per iteration, closes all at function end
  for _, f := range files {
    file, _ := os.Open(f)
    defer file.Close()
  }

  // After - closes immediately after processing
  for _, f := range files {
    func() {
      file, _ := os.Open(f)
      defer file.Close()
      // process file
    }()
  }

---
id: go-string-concat-loop
language: go
severity: warning
message: "String concatenation in loop - use strings.Builder"
tags:
  - performance
  - string
  - memory
  - high
rule:
  kind: ForStatement
  has:
    pattern: "$STR += "
note: |
  String concatenation with += in loops is O(n^2) due to immutability.
  Use strings.Builder for O(n) performance.

  // Before - O(n^2)
  var s string
  for _, item := range items {
    s += item
  }

  // After - O(n)
  var sb strings.Builder
  for _, item := range items {
    sb.WriteString(item)
  }
  s := sb.String()

---
id: go-append-no-capacity
language: go
severity: info
message: "Slice append without capacity hint"
tags:
  - performance
  - memory
  - allocation
  - medium
rule:
  pattern: "var $SLICE []$T"
note: |
  Growing slices without capacity hint causes multiple reallocations.
  When size is known, use make with capacity.

  // Before - multiple reallocations
  var s []int
  for i := 0; i < n; i++ {
    s = append(s, i)
  }

  // After - single allocation
  s := make([]int, 0, n)
  for i := 0; i < n; i++ {
    s = append(s, i)
  }

---
id: go-reflect-hot-path
language: go
severity: warning
message: "Reflection in hot path - 10-100x slower"
tags:
  - performance
  - reflection
  - cpu
  - high
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "reflect.TypeOf"
      - pattern: "reflect.ValueOf"
note: |
  Reflection is 10-100x slower than direct access.
  Cache reflected types/values outside hot paths.

---
id: go-interface-in-loop
language: go
severity: info
message: "Interface method call in tight loop - dynamic dispatch overhead"
tags:
  - performance
  - dispatch
  - cpu
  - medium
rule:
  kind: ForStatement
  has:
    pattern: "$IFACE.$METHOD("
note: |
  Interface method calls require dynamic dispatch.
  In tight loops, consider type assertion for concrete type.

# =============================================================================
# RUST-SPECIFIC RULES
# =============================================================================

---
id: rust-unnecessary-clone
language: rust
severity: warning
message: "Unnecessary clone - consider borrowing"
tags:
  - performance
  - memory
  - allocation
  - medium
rule:
  pattern: ".clone()"
note: |
  clone() creates a full copy which may be unnecessary if you only
  need to read the data. Consider using references (&T) instead.

  // May be wasteful: process(data.clone())
  // Better if read-only: process(&data)

---
id: rust-clone-in-loop
language: rust
severity: warning
message: "Clone in loop - O(n) heap allocations"
tags:
  - performance
  - memory
  - allocation
  - high
rule:
  kind: ForStatement
  has:
    pattern: ".clone()"
note: |
  Cloning inside loops creates O(n) heap allocations.
  Consider borrowing or restructuring to avoid cloning.

---
id: rust-box-hot-path
language: rust
severity: warning
message: "Box::new in hot path - heap allocation per iteration"
tags:
  - performance
  - memory
  - allocation
  - high
rule:
  kind: ForStatement
  has:
    pattern: "Box::new"
note: |
  Box::new allocates on the heap. In loops, this creates
  allocation overhead per iteration.

  Consider stack allocation or pooling.

---
id: rust-vec-no-capacity
language: rust
severity: info
message: "Vec without capacity hint"
tags:
  - performance
  - memory
  - allocation
  - medium
rule:
  pattern: "Vec::new()"
note: |
  Vec::new() starts with zero capacity, causing multiple reallocations.
  Use Vec::with_capacity(n) when size is known.

---
id: rust-arc-single-thread
language: rust
severity: info
message: "Arc in single-threaded context - use Rc instead"
tags:
  - performance
  - memory
  - atomic
  - medium
rule:
  pattern: "Arc::new"
note: |
  Arc uses atomic operations for reference counting, which is
  unnecessary overhead in single-threaded code.

  Rc is ~10x faster for single-threaded scenarios.

---
id: rust-collect-iterator
language: rust
severity: info
message: ".collect() immediately consumed - consider iterator chaining"
tags:
  - performance
  - memory
  - allocation
  - medium
rule:
  pattern: ".collect::<Vec<$T>>()"
note: |
  Collecting into a Vec only to iterate again allocates unnecessarily.
  Chain iterator operations instead.

  // Before - allocates intermediate Vec
  items.iter().map(f).collect::<Vec<_>>().iter().for_each(g);

  // After - no intermediate allocation
  items.iter().map(f).for_each(g);

# =============================================================================
# KOTLIN-SPECIFIC RULES
# =============================================================================

---
id: kotlin-globalscope
language: kotlin
severity: warning
message: "GlobalScope.launch - potential memory leak"
tags:
  - performance
  - concurrency
  - memory
  - leak
  - high
rule:
  any:
    - pattern: "GlobalScope.launch"
    - pattern: "GlobalScope.async"
note: |
  GlobalScope creates unstructured concurrency - coroutines outlive
  their callers and can leak memory.

  Use structured concurrency:
  - viewModelScope in ViewModels
  - lifecycleScope in Activities/Fragments
  - Custom CoroutineScope with proper cancellation

---
id: kotlin-runblocking-production
language: kotlin
severity: warning
message: "runBlocking in production code - blocks thread"
tags:
  - performance
  - concurrency
  - blocking
  - high
rule:
  pattern: "runBlocking"
note: |
  runBlocking blocks the calling thread, defeating the purpose of coroutines.
  Use only for main functions or tests.

  In production, use suspend functions and structured concurrency.

---
id: kotlin-wrong-dispatcher
language: kotlin
severity: warning
message: "CPU work on Dispatchers.IO - use Dispatchers.Default"
tags:
  - performance
  - concurrency
  - dispatcher
  - medium
rule:
  pattern: "withContext(Dispatchers.IO)"
note: |
  Dispatchers.IO is for blocking I/O (file, network).
  Dispatchers.Default is for CPU-intensive work.

  Using wrong dispatcher causes thread pool exhaustion.

---
id: kotlin-nullable-primitive
language: kotlin
severity: info
message: "Nullable primitive in hot path - forces boxing"
tags:
  - performance
  - memory
  - boxing
  - medium
rule:
  any:
    - pattern: "Int?"
    - pattern: "Long?"
    - pattern: "Double?"
note: |
  Nullable primitives (Int?, Long?) are always boxed.
  In performance-critical code, consider using non-nullable with
  sentinel values or dedicated types.

---
id: kotlin-list-not-sequence
language: kotlin
severity: info
message: "Multiple operations on List - consider asSequence()"
tags:
  - performance
  - collection
  - iteration
  - medium
rule:
  pattern: "$LIST.filter($$$).map($$$)"
note: |
  Chained operations on List create intermediate collections.
  Use asSequence() for lazy evaluation.

  // Before - intermediate List created
  list.filter { it > 0 }.map { it * 2 }

  // After - lazy, no intermediate List
  list.asSequence().filter { it > 0 }.map { it * 2 }.toList()

# =============================================================================
# POWERSHELL-SPECIFIC RULES
# =============================================================================

---
id: powershell-array-plus-loop
language: powershell
severity: error
message: "Array += in loop - use List<T> with .Add()"
tags:
  - performance
  - collection
  - memory
  - critical
rule:
  kind: ForStatement
  has:
    pattern: "$ARRAY +="
note: |
  PowerShell array += creates a new array each time (O(n^2)).
  For 100K items: 5 minutes with += vs 0.5 seconds with List.

  # Before - O(n^2)
  $result = @()
  foreach ($item in $items) {
    $result += $item
  }

  # After - O(n)
  $result = [System.Collections.Generic.List[object]]::new()
  foreach ($item in $items) {
    $result.Add($item)
  }

---
id: powershell-string-concat-loop
language: powershell
severity: error
message: "String += in loop - use StringBuilder (790x faster)"
tags:
  - performance
  - string
  - memory
  - critical
rule:
  kind: ForStatement
  has:
    pattern: "$STR += "
note: |
  String concatenation in loops is 790x slower than StringBuilder.

  # Before
  $str = ""
  foreach ($item in $items) {
    $str += $item
  }

  # After
  $sb = [System.Text.StringBuilder]::new()
  foreach ($item in $items) {
    [void]$sb.Append($item)
  }
  $str = $sb.ToString()

---
id: powershell-foreach-object-slow
language: powershell
severity: info
message: "ForEach-Object is 6-167x slower than foreach statement"
tags:
  - performance
  - pipeline
  - iteration
  - high
rule:
  pattern: "| ForEach-Object"
note: |
  ForEach-Object (pipeline) is 6-167x slower than foreach statement
  for in-memory collections due to pipeline initialization overhead.

  # Before - pipeline overhead
  $items | ForEach-Object { $_ * 2 }

  # After - direct iteration
  foreach ($item in $items) { $item * 2 }

---
id: powershell-out-null
language: powershell
severity: info
message: "Out-Null has cmdlet overhead - use $null = or [void]"
tags:
  - performance
  - pipeline
  - low
rule:
  pattern: "| Out-Null"
note: |
  Out-Null is a cmdlet with invocation overhead.
  Use assignment to $null or [void] cast instead.

  # Before
  $result | Out-Null

  # After (faster)
  $null = $result
  # Or
  [void]$result

---
id: powershell-filter-right
language: powershell
severity: warning
message: "Filter in pipeline - use -Filter parameter (65% faster)"
tags:
  - performance
  - pipeline
  - io
  - high
rule:
  pattern: "Get-ChildItem $$$| Where-Object"
note: |
  Where-Object filters after all items are retrieved.
  -Filter parameter filters at the source (65% faster).

  # Before - retrieves all, then filters
  Get-ChildItem -Path $dir | Where-Object { $_.Extension -eq '.txt' }

  # After - filters at source
  Get-ChildItem -Path $dir -Filter '*.txt'

# =============================================================================
# SQL-SPECIFIC RULES
# =============================================================================

---
id: sql-select-star
language: sql
severity: info
message: "SELECT * fetches unnecessary columns - specify needed columns"
tags:
  - performance
  - database
  - io
  - medium
rule:
  any:
    - pattern: "SELECT *"
    - pattern: "select *"
note: |
  SELECT * fetches all columns even when only a few are needed.
  This increases:
  - Network I/O transferring unused data
  - Memory usage for result sets
  - Prevents covering index optimization

  Fix: Explicitly list only the columns needed.

---
id: sql-function-on-column
language: sql
severity: warning
message: "Function on indexed column prevents index usage"
tags:
  - performance
  - database
  - index
  - high
rule:
  any:
    - pattern: "WHERE YEAR($COL)"
    - pattern: "WHERE UPPER($COL)"
    - pattern: "WHERE LOWER($COL)"
    - pattern: "WHERE CAST($COL"
note: |
  Functions on indexed columns (non-SARGable) prevent index seeks.

  -- Before (no index usage)
  WHERE YEAR(created_at) = 2024

  -- After (uses index)
  WHERE created_at >= '2024-01-01' AND created_at < '2025-01-01'

---
id: sql-like-leading-wildcard
language: sql
severity: warning
message: "LIKE with leading wildcard prevents index usage"
tags:
  - performance
  - database
  - index
  - high
rule:
  pattern: "LIKE '%$PATTERN"
note: |
  Leading wildcard in LIKE prevents index usage, causing full table scan.

  -- Before (table scan)
  WHERE name LIKE '%smith'

  -- After (can use index if pattern is anchored)
  WHERE name LIKE 'smith%'

  For contains searches, consider full-text search indexes.

---
id: sql-correlated-subquery
language: sql
severity: warning
message: "Correlated subquery - consider JOIN rewrite"
tags:
  - performance
  - database
  - algorithm
  - high
rule:
  pattern: "WHERE $COL = (SELECT"
note: |
  Correlated subqueries execute once per row in outer query,
  creating O(n^2) behavior.

  Rewrite as JOIN for set-based processing.

---
id: sql-or-different-columns
language: sql
severity: info
message: "OR on different columns - consider UNION or restructure"
tags:
  - performance
  - database
  - index
  - medium
rule:
  pattern: "WHERE $COL1 = $VAL OR $COL2 = $VAL"
note: |
  OR conditions on different columns often result in table scans.
  Consider UNION of indexed queries:

  -- May cause scan
  WHERE col1 = 'x' OR col2 = 'y'

  -- Better (each uses index)
  SELECT ... WHERE col1 = 'x'
  UNION
  SELECT ... WHERE col2 = 'y'

# =============================================================================
# HASKELL-SPECIFIC RULES
# =============================================================================

---
id: haskell-foldl-space-leak
language: haskell
severity: error
message: "foldl causes space leak - use foldl' for strict evaluation"
tags:
  - performance
  - memory
  - lazy
  - critical
rule:
  pattern: "foldl"
note: |
  foldl builds up O(n) thunks before evaluation, causing space leaks.
  Use foldl' from Data.List for strict left folds.

  -- Before (space leak)
  foldl (+) 0 [1..1000000]

  -- After (strict)
  import Data.List (foldl')
  foldl' (+) 0 [1..1000000]

---
id: haskell-string-vs-text
language: haskell
severity: warning
message: "String ([Char]) is inefficient - use Text or ByteString"
tags:
  - performance
  - memory
  - string
  - high
rule:
  any:
    - pattern: ":: String"
    - pattern: ":: [Char]"
note: |
  String is a linked list of Char, using ~24 bytes per character.
  Text uses 6 words + packed UTF-16 representation.

  Use Data.Text or Data.ByteString for better performance.

---
id: haskell-list-random-access
language: haskell
severity: warning
message: "List index access (!!) is O(n) - use Vector or Seq"
tags:
  - performance
  - collection
  - algorithm
  - high
rule:
  pattern: "!!"
note: |
  List index access (!!) is O(n). Use:
  - Data.Vector for O(1) random access
  - Data.Sequence for O(log n) access with efficient insertions

---
id: haskell-nub-quadratic
language: haskell
severity: error
message: "nub is O(n^2) - use Data.List.nub or Set"
tags:
  - performance
  - algorithm
  - critical
rule:
  pattern: "nub"
note: |
  Prelude nub is O(n^2). For large lists:
  - Data.List.nub from containers package
  - Data.Set.toList . Data.Set.fromList for Ord types

---
id: haskell-length-empty-check
language: haskell
severity: info
message: "length for empty check is O(n) - use null"
tags:
  - performance
  - algorithm
  - medium
rule:
  pattern: "length $LIST == 0"
note: |
  length traverses entire list (O(n)). null checks in O(1).

  -- Before
  length xs == 0

  -- After
  null xs

# =============================================================================
# ELIXIR-SPECIFIC RULES
# =============================================================================

---
id: elixir-atom-exhaustion
language: elixir
severity: error
message: "Dynamic atom creation - atoms are never garbage collected"
tags:
  - performance
  - memory
  - leak
  - critical
rule:
  any:
    - pattern: "String.to_atom"
    - pattern: "binary_to_atom"
note: |
  Atoms are never garbage collected in the BEAM VM.
  Default limit is ~1M atoms - exhaustion crashes the VM.

  Use String.to_existing_atom/1 or validate input.

---
id: elixir-enum-on-stream
language: elixir
severity: warning
message: "Enum on large data - consider Stream for lazy evaluation"
tags:
  - performance
  - memory
  - collection
  - high
rule:
  pattern: "Enum.$FN"
note: |
  Enum functions eagerly evaluate entire collections.
  For large/infinite data, use Stream for lazy evaluation.

  # Before - loads all into memory
  File.stream!("large.txt") |> Enum.map(&parse/1)

  # After - processes lazily
  File.stream!("large.txt") |> Stream.map(&parse/1) |> Enum.to_list()

---
id: elixir-list-append-loop
language: elixir
severity: warning
message: "++ for list append is O(n) - use [head | tail] pattern"
tags:
  - performance
  - collection
  - algorithm
  - high
rule:
  kind: ForStatement
  has:
    pattern: "++"
note: |
  ++ copies the left list (O(n) per append, O(n^2) total).
  Use [head | acc] and reverse at the end.

  # Before - O(n^2)
  Enum.reduce(items, [], fn item, acc -> acc ++ [item] end)

  # After - O(n)
  items |> Enum.reduce([], fn item, acc -> [item | acc] end) |> Enum.reverse()

# =============================================================================
# SWIFT-SPECIFIC RULES
# =============================================================================

---
id: swift-main-thread-blocking
language: swift
severity: error
message: "Heavy computation on main thread - use background queue"
tags:
  - performance
  - ui
  - blocking
  - critical
rule:
  any:
    - pattern: "@MainActor"
note: |
  Heavy operations on main thread cause UI freezes.
  Must complete in <16ms for 60fps.

  Use Task.detached or DispatchQueue.global() for CPU work.

---
id: swift-closure-retain-cycle
language: swift
severity: warning
message: "Closure capturing self strongly - use [weak self]"
tags:
  - performance
  - memory
  - leak
  - high
rule:
  pattern: "{ $$$self.$$$}"
  not:
    pattern: "[weak self]"
note: |
  Closures capturing self strongly create retain cycles.
  Use [weak self] or [unowned self] in capture list.

  // Before - retain cycle
  closure = { self.doSomething() }

  // After - no retain cycle
  closure = { [weak self] in self?.doSomething() }

---
id: swift-shadow-no-path
language: swift
severity: warning
message: "Shadow without shadowPath causes off-screen rendering"
tags:
  - performance
  - ui
  - render
  - high
rule:
  pattern: ".shadowColor"
  not:
    pattern: ".shadowPath"
note: |
  Shadows without shadowPath require off-screen render pass,
  doubling rendering cost.

  layer.shadowPath = UIBezierPath(rect: bounds).cgPath

# =============================================================================
# SHELL/BASH-SPECIFIC RULES
# =============================================================================

---
id: shell-external-in-loop
language: bash
severity: warning
message: "External command in loop - use built-ins"
tags:
  - performance
  - fork
  - cpu
  - high
rule:
  kind: ForStatement
  has:
    any:
      - pattern: "grep"
      - pattern: "awk"
      - pattern: "sed"
      - pattern: "cut"
      - pattern: "wc"
note: |
  Each external command spawns a subprocess.
  1000 iterations = 1000 forks (10-100x slower than built-ins).

  # Before
  for file in *.txt; do
    count=$(wc -l < "$file")
  done

  # After (bash built-in)
  while IFS= read -r line; do
    ((count++))
  done < "$file"

---
id: shell-useless-cat
language: bash
severity: info
message: "Useless use of cat (UUOC)"
tags:
  - performance
  - fork
  - low
rule:
  pattern: "cat $FILE | $CMD"
note: |
  cat file | cmd spawns unnecessary process.
  Use cmd < file or cmd file instead.

  # Before
  cat file.txt | grep pattern

  # After
  grep pattern file.txt
  # Or
  grep pattern < file.txt

---
id: shell-basename-dirname-command
language: bash
severity: info
message: "basename/dirname command - use parameter expansion"
tags:
  - performance
  - fork
  - low
rule:
  any:
    - pattern: "$(dirname"
    - pattern: "$(basename"
note: |
  basename/dirname commands fork for simple string operations.
  Use parameter expansion instead.

  # Before
  dir=$(dirname "$path")
  base=$(basename "$path")

  # After
  dir="${path%/*}"
  base="${path##*/}"

# =============================================================================
# DOCKER-SPECIFIC RULES
# =============================================================================

---
id: docker-copy-before-deps
language: dockerfile
severity: warning
message: "COPY all before dependency install breaks layer caching"
tags:
  - performance
  - cache
  - build
  - high
rule:
  pattern: |
    COPY . $DEST
    RUN $PKG_INSTALL
note: |
  Copying all source before dependency install invalidates the
  dependency layer on any source change.

  # Before - deps reinstalled on any change
  COPY . /app
  RUN npm install

  # After - deps cached unless package.json changes
  COPY package*.json ./
  RUN npm install
  COPY . .

---
id: docker-no-multistage
language: dockerfile
severity: info
message: "Build tools in production image - use multi-stage builds"
tags:
  - performance
  - size
  - security
  - medium
rule:
  pattern: "FROM $IMAGE"
  not:
    pattern: "AS builder"
note: |
  Single-stage builds include build tools in production image,
  bloating size and attack surface.

  Use multi-stage: build stage + minimal runtime stage.

---
id: docker-apt-no-cleanup
language: dockerfile
severity: info
message: "apt-get without cleanup bloats image"
tags:
  - performance
  - size
  - medium
rule:
  pattern: "apt-get install"
  not:
    pattern: "rm -rf /var/lib/apt/lists/*"
note: |
  apt cache remains in layer, bloating image.
  Always clean in the same RUN instruction.

  RUN apt-get update && \
      apt-get install -y package && \
      rm -rf /var/lib/apt/lists/*

# =============================================================================
# TERRAFORM-SPECIFIC RULES
# =============================================================================

---
id: terraform-count-vs-foreach
language: hcl
severity: warning
message: "count with list - use for_each for stable resource addresses"
tags:
  - performance
  - infrastructure
  - stability
  - high
rule:
  pattern: "count = length($VAR)"
note: |
  count uses index-based addressing. Removing an item shifts all
  subsequent indices, causing unnecessary resource recreation.

  Use for_each with map/set for key-based addressing.

  # Before - removing item[1] recreates item[2+]
  resource "aws_instance" "servers" {
    count = length(var.names)
    name  = var.names[count.index]
  }

  # After - only affected resource is removed
  resource "aws_instance" "servers" {
    for_each = toset(var.names)
    name     = each.key
  }

---
id: terraform-depends-on-overuse
language: hcl
severity: info
message: "Explicit depends_on - let Terraform infer from references"
tags:
  - performance
  - infrastructure
  - low
rule:
  pattern: "depends_on"
note: |
  Explicit depends_on serializes resource creation.
  Let Terraform infer dependencies from attribute references.

  Only use depends_on for hidden dependencies not visible in config.

---
id: c-cpp-perf-non-contiguous-data
language: cpp
severity: warning
message: "Performance: Non-contiguous data access causes cache misses"
tags:
  - performance
  - cache
rule:
  kind: loop_with_pointer_chasing
  pattern: "for (Node* n = $VAR; n; n = n->$NEXT) { ... $VAR->$FIELD ... }"
note: |
  Linked structures cause cache misses due to non-contiguous memory access. Modern CPUs
  prefetch sequential memory but cannot predict pointer chains.

  Remediation: Use contiguous containers (std::vector, arrays). Consider structure-of-arrays
  (SoA) layout for hot fields.

  Performance Impact: 10-100x slower for large datasets

---
id: c-cpp-perf-column-major-access
language: cpp
severity: warning
message: "Performance: Column-major access of row-major array causes cache misses"
tags:
  - performance
  - cache
  - array
rule:
  kind: nested_loop
  pattern: "for ($I...) for ($J...) $ARR[$J][$I]"
note: |
  C/C++ arrays are row-major; accessing columns causes cache misses due to non-sequential
  memory access pattern.

  Remediation: Swap loop order to access memory sequentially (iterate i outer, j inner).

  Performance Impact: 5-50x slower

---
id: c-cpp-perf-large-struct-by-value
language: cpp
severity: warning
message: "Performance: Large struct passed by value causes expensive copying"
tags:
  - performance
  - function-parameter
rule:
  kind: function_parameter
  pattern: "void $FUNC($LARGE_STRUCT $VAR) { ... }"
note: |
  Copying large structs is expensive and causes cache pollution.

  Remediation: Pass by const reference or pointer instead.

  Performance Impact: Proportional to struct size

---
id: c-cpp-perf-false-sharing
language: cpp
severity: warning
message: "Performance: False sharing in multi-threaded code causes cache invalidation"
tags:
  - performance
  - multithreading
  - cache
rule:
  kind: struct_member_declaration
  pattern: "struct { $TYPE $VAR1; $TYPE $VAR2; }"
note: |
  Adjacent variables accessed by different threads on same cache line (64 bytes) cause
  cache invalidation ping-pong between cores.

  Remediation: Align to cache line with alignas(64) or padding.

  Performance Impact: 10-100x slower

---
id: c-cpp-perf-cache-unfriendly-size
language: cpp
severity: info
message: "Performance: Struct size crosses cache line boundary reducing cache efficiency"
tags:
  - performance
  - cache
  - alignment
rule:
  kind: struct_definition
  pattern: "struct $NAME { ... }  /* 65-128 bytes */"
note: |
  Struct size just over cache line boundary (e.g., 65-128 bytes) requires loading two
  cache lines per access.

  Remediation: Pad to exact cache line multiple or reduce size.

  Performance Impact: ~2x cache misses for hot data

---
id: c-cpp-perf-hot-cold-data-mixed
language: cpp
severity: warning
message: "Performance: Hot and cold data mixed in struct pollutes cache"
tags:
  - performance
  - cache
  - data-layout
rule:
  kind: struct_member_declaration
  pattern: "struct { $SMALL_TYPE $HOT; $LARGE_TYPE $COLD; ... }"
note: |
  Hot fields pollute cache with cold data. Frequently accessed data shares cache line
  with rarely-used large fields, reducing effective cache size.

  Remediation: Split struct into hot/cold parts, or use pointer to cold data.

  Performance Impact: Reduced effective cache size

---
id: c-cpp-perf-pointer-chasing
language: cpp
severity: warning
message: "Performance: Multiple levels of pointer chasing in critical path"
tags:
  - performance
  - cache
  - pointer
rule:
  kind: member_access
  pattern: "$A->$B->$C->$D"
note: |
  Each pointer dereference is a potential cache miss. Multiple indirection levels
  incur cumulative latency penalty.

  Remediation: Flatten structure, cache intermediate pointers, or prefetch.

  Performance Impact: ~100 cycles per cache miss

---
id: c-cpp-perf-malloc-in-loop
language: c
severity: warning
message: "Performance: malloc/new in tight loop is expensive"
tags:
  - performance
  - memory-allocation
  - loop
rule:
  kind: loop_with_allocation
  pattern: "for (...) { $ALLOC = malloc(...); ... free($ALLOC); }"
note: |
  Heap allocation is expensive (100s of cycles). Allocating on every iteration
  dominates execution time.

  Remediation: Pre-allocate, use pool allocator, or stack allocation.

  Performance Impact: 100-1000x slower per allocation

---
id: c-cpp-perf-frequent-small-allocs
language: cpp
severity: warning
message: "Performance: Frequent small allocations cause fragmentation overhead"
tags:
  - performance
  - memory-allocation
rule:
  kind: allocation_statement
  pattern: "malloc($SMALL_SIZE)  /* < 64 bytes */"
note: |
  Malloc overhead exceeds data size; memory fragmentation occurs. Each allocation
  has fixed overhead that dominates for small sizes.

  Remediation: Use memory pool or arena allocator, batch allocations.

  Performance Impact: 10-100x slower

---
id: c-cpp-perf-realloc-growth-loop
language: c
severity: warning
message: "Performance: Realloc in growth loop causes O(n²) complexity"
tags:
  - performance
  - memory-allocation
  - loop
rule:
  kind: loop_with_realloc
  pattern: "for (...) { $ARR = realloc($ARR, $SIZE+1); }"
note: |
  O(n²) copy operations when growing by constant increment. Each realloc copies
  all existing elements.

  Remediation: Double capacity on each realloc (exponential growth strategy).

  Performance Impact: O(n²) vs O(n)

---
id: c-cpp-perf-new-delete-vs-placement
language: cpp
severity: info
message: "Performance: Repeated new/delete instead of placement new"
tags:
  - performance
  - memory-allocation
rule:
  kind: allocation_pattern
  pattern: "new $TYPE(...); ... delete $VAR;"
note: |
  Memory can be reused instead of freed for repeated same-sized allocations.
  Placement new avoids allocator overhead.

  Remediation: Use placement new with preallocated buffer or object pool.

  Performance Impact: 10-100x for high-frequency allocation

---
id: c-cpp-perf-vector-missing-reserve
language: cpp
severity: warning
message: "Performance: Vector push_back without reserve causes repeated reallocation"
tags:
  - performance
  - container
  - vector
rule:
  kind: loop_with_push_back
  pattern: "std::vector<$T> $V; for (...) $V.push_back(...);  /* no reserve */"
note: |
  Vector reallocates on growth, copying all elements. Without reserve, growth is
  O(n) copying instead of O(1).

  Remediation: Call reserve(expected_size) before filling loop.

  Performance Impact: O(n) copies vs O(1)

---
id: c-cpp-perf-shrink-to-fit-missing
language: cpp
severity: info
message: "Performance: Vector not shrunk after erase/resize wastes memory"
tags:
  - performance
  - container
  - vector
  - memory
rule:
  kind: vector_resize
  pattern: "$VEC.erase(...);  /* no shrink_to_fit */"
note: |
  Vector retains capacity after erase/resize, wasting memory. Excess capacity
  pollutes cache and wastes space.

  Remediation: Call shrink_to_fit() when size stabilizes.

  Performance Impact: Memory only, minimal runtime

---
id: c-cpp-perf-heap-small-fixed
language: cpp
severity: warning
message: "Performance: Heap allocation for small fixed-size data"
tags:
  - performance
  - memory-allocation
rule:
  kind: allocation_statement
  pattern: "new char[$SMALL_CONSTANT]  /* < 4KB */"
note: |
  Stack allocation is free (just move stack pointer). Heap allocation incurs
  malloc overhead and fragmentation risk.

  Remediation: Use local arrays for small, fixed-size buffers.

  Performance Impact: 100+ cycles saved per allocation

---
id: c-cpp-perf-vla-usage
language: c
severity: info
message: "Performance: Variable-length arrays on stack are fast but risky"
tags:
  - performance
  - memory-allocation
  - stack
rule:
  kind: variable_declaration
  pattern: "char $BUF[$SIZE];  /* variable size */"
note: |
  VLAs are stack-allocated (fast) but can overflow stack if size is unbounded.
  Useful for small, bounded sizes only.

  Remediation: Use if size is bounded and small (<4KB), otherwise heap.

  Performance Impact: Faster than malloc, but crash risk

---
id: c-cpp-perf-alloca-large
language: c
severity: warning
message: "Performance: alloca for large buffers can overflow stack"
tags:
  - performance
  - memory-allocation
  - stack
rule:
  kind: allocation_statement
  pattern: "alloca($LARGE_SIZE)  /* > 4KB or user-controlled */"
note: |
  Stack space is limited (~1-8MB), can crash. alloca is fast but stack overflow
  is catastrophic failure.

  Remediation: Use heap for large or variable-size allocations.

  Performance Impact: Crash risk outweighs performance

---
id: c-cpp-perf-non-vectorizable-loop
language: cpp
severity: warning
message: "Performance: Loop with data dependencies prevents SIMD vectorization"
tags:
  - performance
  - simd
  - loop
rule:
  kind: loop_with_dependency
  pattern: "for (i=1;i<N;i++) a[i] = a[i-1] + b[i];  /* carries dependency */"
note: |
  Modern CPUs process 4-16 elements per instruction with SIMD. Data dependencies
  prevent auto-vectorization.

  Remediation: Break dependencies, use restrict pointers, align data.

  Performance Impact: 4-16x potential speedup missed

---
id: c-cpp-perf-unaligned-simd
language: cpp
severity: warning
message: "Performance: Unaligned data for SIMD operations is slower"
tags:
  - performance
  - simd
  - alignment
rule:
  kind: simd_operation
  pattern: "SIMD_operation(unaligned_memory)"
note: |
  Unaligned loads/stores are slower on many architectures. SIMD requires
  proper alignment for performance.

  Remediation: Use alignas(32) for AVX, alignas(16) for SSE.

  Performance Impact: 2-10x slower for unaligned

---
id: c-cpp-perf-scalar-array-ops
language: cpp
severity: info
message: "Performance: Scalar operations on array without SIMD hints"
tags:
  - performance
  - simd
  - optimization-hint
rule:
  kind: element_by_element_loop
  pattern: "for (i=0;i<N;i++) a[i] = func(b[i]);"
note: |
  Compiler may not auto-vectorize without hints. SIMD can process 4-16 elements
  per instruction.

  Remediation: Use #pragma omp simd, restrict pointers, or intrinsics.

  Performance Impact: 4-16x with vectorization

---
id: c-cpp-perf-mixed-types-vectorization
language: cpp
severity: info
message: "Performance: Mixed float/double or int32/int64 prevents vectorization"
tags:
  - performance
  - simd
  - type
rule:
  kind: mixed_type_operation
  pattern: "for (i...) float_var * double_var"
note: |
  Different data widths require conversion, breaking SIMD. Vectorizers work best
  with uniform types.

  Remediation: Use consistent types in hot loops.

  Performance Impact: Prevents vectorization

---
id: c-cpp-perf-invariant-in-loop
language: cpp
severity: warning
message: "Performance: Invariant computation repeated in loop"
tags:
  - performance
  - loop
rule:
  kind: loop_body
  pattern: "for (i...) result[i] = (X * Y) + data[i];  /* X*Y constant */"
note: |
  Same expression computed every iteration wastes cycles. Should be computed once.

  Remediation: Hoist invariant expressions before loop.

  Performance Impact: Proportional to computation cost

---
id: c-cpp-perf-function-call-in-condition
language: c
severity: warning
message: "Performance: Function call in loop condition causes O(n²) complexity"
tags:
  - performance
  - loop
rule:
  kind: loop_condition
  pattern: "for ($I = 0; $I < strlen($STR); $I++)"
note: |
  strlen is O(n), making entire loop O(n²). String length queried every iteration.

  Remediation: Compute length once before loop.

  Performance Impact: O(n²) vs O(n)

---
id: c-cpp-perf-method-call-in-condition
language: cpp
severity: warning
message: "Performance: Method call in loop condition has call overhead"
tags:
  - performance
  - loop
rule:
  kind: loop_condition
  pattern: "for ($I = 0; $I < $OBJ.size(); $I++)"
note: |
  size() is usually O(1) but still a function call overhead. Inline but still
  overhead per iteration.

  Remediation: Cache size or use range-based for loop.

  Performance Impact: Call overhead per iteration

---
id: c-cpp-perf-loop-unrolling-candidate
language: cpp
severity: info
message: "Performance: Simple loop is candidate for unrolling"
tags:
  - performance
  - loop
  - optimization-hint
rule:
  kind: simple_loop
  pattern: "for (int i=0; i<SMALL_CONST; i++) simple_statement();"
note: |
  Loop unrolling reduces branch overhead per iteration. Helps for small known
  iteration counts.

  Remediation: Use #pragma unroll or manual unrolling for hot loops.

  Performance Impact: 10-30% for small loops

---
id: c-cpp-perf-branch-in-hot-loop
language: cpp
severity: warning
message: "Performance: Branch in hot loop causes misprediction penalty"
tags:
  - performance
  - loop
  - branch
rule:
  kind: conditional_in_loop
  pattern: "for (i...) if (a[i]>0) sum += a[i]; else sum -= a[i];"
note: |
  Branch misprediction costs ~15-20 cycles. Hot loops should minimize branching.

  Remediation: Use branchless techniques (CMOV, ternary), or split loop.

  Performance Impact: 15-20 cycles per mispredict

---
id: c-cpp-perf-modulo-in-loop
language: cpp
severity: warning
message: "Performance: Modulo operation in loop is expensive"
tags:
  - performance
  - loop
  - arithmetic
rule:
  kind: modulo_operation
  pattern: "for (i...) array[i % SIZE]"
note: |
  Integer division/modulo is expensive (~30 cycles). Avoid in tight loops.

  Remediation: Use bit mask if power of 2, or counter reset.

  Performance Impact: 30 cycles per modulo

---
id: c-cpp-perf-division-in-loop
language: cpp
severity: warning
message: "Performance: Non-power-of-2 division in loop is expensive"
tags:
  - performance
  - loop
  - arithmetic
rule:
  kind: division_operation
  pattern: "for (i...) x / $CONST  /* not power of 2 */"
note: |
  Division is expensive (~20-80 cycles), compiler may not optimize away.
  Avoid in hot paths.

  Remediation: Multiply by reciprocal if possible.

  Performance Impact: 20-80 cycles per division

---
id: c-cpp-perf-non-inline-getter
language: cpp
severity: info
message: "Performance: Simple getter/setter not inlined has call overhead"
tags:
  - performance
  - inlining
rule:
  kind: function_definition
  pattern: "int Foo::getValue() { return value_; }  /* in .cpp */"
note: |
  Function call overhead for trivial operations. Declared in .cpp cannot be
  inlined across translation units.

  Remediation: Define in header or mark inline.

  Performance Impact: ~5 cycles call overhead

---
id: c-cpp-perf-virtual-dtor-leaf-class
language: cpp
severity: info
message: "Performance: Virtual destructor on leaf class prevents inlining"
tags:
  - performance
  - virtual
  - inlining
rule:
  kind: destructor_declaration
  pattern: "class Leaf { virtual ~Leaf() { } };  /* never inherited */"
note: |
  Virtual destructor prevents inlining, adds vtable overhead for non-polymorphic
  leaf classes.

  Remediation: Only use virtual destructor when needed for polymorphism.

  Performance Impact: Minor, but code bloat

---
id: c-cpp-perf-inline-large-function
language: cpp
severity: info
message: "Performance: Inline hint on large function increases code size"
tags:
  - performance
  - inlining
rule:
  kind: function_definition
  pattern: "inline void largeFunction() { /* > 20 lines */ }"
note: |
  Large functions increase code size, hurt instruction cache. Compiler should
  decide based on actual benefit.

  Remediation: Remove inline hint, let compiler decide.

  Performance Impact: Increased code size, I-cache misses

---
id: c-cpp-perf-virtual-call-hot-path
language: cpp
severity: warning
message: "Performance: Virtual call in hot path cannot be inlined"
tags:
  - performance
  - virtual
  - hot-path
rule:
  kind: loop_with_virtual_call
  pattern: "for (...) obj->processVirtual();  /* virtual method */"
note: |
  Indirect call prevents inlining and branch prediction. Virtual dispatch
  overhead is significant in tight loops.

  Remediation: Use CRTP, final, or devirtualization.

  Performance Impact: ~10-20 cycles per call, no inlining

---
id: c-cpp-perf-missing-final-leaf
language: cpp
severity: info
message: "Performance: Leaf class missing final keyword prevents devirtualization"
tags:
  - performance
  - virtual
rule:
  kind: class_definition
  pattern: "class Leaf : public Base { virtual void method() override; };  /* no final, never inherited */"
note: |
  final keyword enables compiler to devirtualize calls. Leaf classes should be
  marked final for optimization.

  Remediation: Mark leaf classes final.

  Performance Impact: Enables optimization

---
id: c-cpp-perf-override-without-final
language: cpp
severity: info
message: "Performance: Override without final prevents devirtualization"
tags:
  - performance
  - virtual
rule:
  kind: method_declaration
  pattern: "void method() override;  /* no final, never further overridden */"
note: |
  Compiler can't devirtualize without final. Leaf method overrides should be marked.

  Remediation: Add final to leaf method overrides.

  Performance Impact: Enables inlining

---
id: c-cpp-perf-polymorphic-container
language: cpp
severity: warning
message: "Performance: Polymorphic container has poor cache locality"
tags:
  - performance
  - container
  - cache
rule:
  kind: container_declaration
  pattern: "std::vector<Base*> vec;"
note: |
  std::vector<Base*> with heterogeneous types has poor cache locality and
  virtual call overhead. Pointers scatter in memory.

  Remediation: Use variant, separate containers, or SoA.

  Performance Impact: Cache + virtual overhead

---
id: c-cpp-perf-template-instantiation-explosion
language: cpp
severity: warning
message: "Performance: Template instantiation with many types causes code bloat"
tags:
  - performance
  - template
  - bloat
rule:
  kind: template_instantiation
  pattern: "template<class T> void func(T) { ... };  /* used with many types */"
note: |
  Each instantiation generates code, bloating binary and hurting I-cache.
  Template bloat degrades performance.

  Remediation: Type-erase where possible, explicit instantiation.

  Performance Impact: Increased binary size, I-cache pressure

---
id: c-cpp-perf-template-no-extern
language: cpp
severity: info
message: "Performance: Common template without extern declaration duplicates code"
tags:
  - performance
  - template
rule:
  kind: template_definition
  pattern: "/* header */ template<class T> void f() { ... }  /* no extern template */"
note: |
  Common template without extern template declaration causes duplicate instantiation
  in each translation unit.

  Remediation: Use extern template class Foo<int>; in header, explicit instantiation in one .cpp.

  Performance Impact: Build time, binary size

---
id: c-cpp-perf-complex-template-in-header
language: cpp
severity: info
message: "Performance: Large template function body in header increases build time"
tags:
  - performance
  - template
rule:
  kind: template_definition
  pattern: "/* header */ template<class T> void largeFunc() { /* > 100 lines */ }"
note: |
  Large template function compiled in every translation unit. Causes slow builds
  and code duplication.

  Remediation: Move to .tpp file or use explicit instantiation.

  Performance Impact: Build time

---
id: c-cpp-perf-missing-move-ctor
language: cpp
severity: warning
message: "Performance: Class missing move constructor copies expensive resources"
tags:
  - performance
  - move-semantics
rule:
  kind: class_definition
  pattern: "class MyClass { std::vector<T> data; /* no move ctor */ };"
note: |
  Class with expensive-to-copy resources but no move constructor causes full
  copies instead of moves. Very expensive performance hit.

  Remediation: Implement move constructor and assignment.

  Performance Impact: Full copy vs pointer swap

---
id: c-cpp-perf-copy-in-return
language: cpp
severity: warning
message: "Performance: Copy in return may prevent move"
tags:
  - performance
  - move-semantics
rule:
  kind: return_statement
  pattern: "return $DIFFERENT_VAR;  /* not the declared return */"
note: |
  Return of variable different from declared type may not trigger copy elision
  or move. Move should happen but not guaranteed.

  Remediation: Return local variable directly, or use std::move for rvalue return.

  Performance Impact: Copy vs move

---
id: c-cpp-perf-move-on-local-return
language: cpp
severity: warning
message: "Performance: std::move on local return prevents copy elision"
tags:
  - performance
  - move-semantics
  - rvo
rule:
  kind: return_statement
  pattern: "return std::move($LOCAL);"
note: |
  std::move on return of local variable prevents NRVO/RVO. Compiler cannot
  elide copy when move semantics interfere.

  Remediation: Return local directly without std::move.

  Performance Impact: Move instead of elision

---
id: c-cpp-perf-noexcept-missing-move
language: cpp
severity: warning
message: "Performance: Move constructor/assignment without noexcept uses copy fallback"
tags:
  - performance
  - move-semantics
rule:
  kind: function_declaration
  pattern: "MyClass(MyClass&&) { ... }  /* no noexcept */"
note: |
  STL containers copy instead of move if move constructor can throw exceptions.
  noexcept is critical for move operations.

  Remediation: Add noexcept to move operations.

  Performance Impact: STL uses copy instead of move

---
id: c-cpp-perf-pass-by-value-no-move
language: cpp
severity: warning
message: "Performance: Pass by value without move has extra copy"
tags:
  - performance
  - move-semantics
rule:
  kind: function_definition
  pattern: "void set(string s) { data = s; }  /* no std::move */"
note: |
  Function taking by value, storing without move causes extra copy. Parameter
  copied once, then copied again when stored.

  Remediation: Use std::move(var) when storing parameter.

  Performance Impact: Extra copy

---
id: c-cpp-perf-unnecessary-copy-init
language: cpp
severity: warning
message: "Performance: Copy initialization may prevent copy elision"
tags:
  - performance
  - copy-elision
  - rvo
rule:
  kind: variable_initialization
  pattern: "$TYPE $VAR = $FUNC();  /* copy init vs direct */"
note: |
  Copy initialization may prevent copy elision depending on context. Direct
  initialization is more likely to elide.

  Remediation: Use direct initialization or ensure RVO applies.

  Performance Impact: Potential extra copy

---
id: c-cpp-perf-multiple-return-nrvo
language: cpp
severity: info
message: "Performance: Named return different for each path prevents NRVO"
tags:
  - performance
  - copy-elision
  - rvo
rule:
  kind: function_with_multiple_returns
  pattern: "if (cond) return $RESULT1; else return $RESULT2;  /* different objects */"
note: |
  NRVO cannot apply when different objects returned on different paths. Compiler
  cannot optimize away copies.

  Remediation: Use single return variable or explicit moves.

  Performance Impact: Move instead of elision

---
id: c-cpp-perf-string-concat-loop
language: cpp
severity: warning
message: "Performance: String concatenation in loop is O(n²)"
tags:
  - performance
  - string
  - loop
rule:
  kind: loop_with_string_concat
  pattern: "for (...) result += item;  /* no reserve */"
note: |
  String += in loop causes O(n²) due to repeated reallocation and copying.
  Each append may trigger full buffer copy.

  Remediation: Use reserve() or std::ostringstream.

  Performance Impact: O(n²) vs O(n)

---
id: c-cpp-perf-string-by-value
language: cpp
severity: warning
message: "Performance: String passed by value causes unnecessary copy"
tags:
  - performance
  - string
  - function-parameter
rule:
  kind: function_parameter
  pattern: "void $FUNC(std::string $S)  /* not reference */"
note: |
  String parameter by value copies the entire string, even if only reading.
  Reference or string_view avoids copy.

  Remediation: Use const std::string& or std::string_view.

  Performance Impact: Full copy

---
id: c-cpp-perf-cstr-comparison
language: cpp
severity: info
message: "Performance: c_str() for string comparison is unnecessary"
tags:
  - performance
  - string
rule:
  kind: method_call
  pattern: "strcmp($STR.c_str(), $OTHER)"
note: |
  Unnecessary conversion to C-string. std::string has efficient comparison.

  Remediation: Use == operator or compare() method.

  Performance Impact: Minor, style

---
id: c-cpp-perf-substr-comparison
language: cpp
severity: warning
message: "Performance: substr creates temporary for comparison"
tags:
  - performance
  - string
rule:
  kind: string_operation
  pattern: "$STR.substr(0, $N) == $PREFIX"
note: |
  substr creates temporary string for comparison, causing allocation. Direct
  comparison avoids temporary.

  Remediation: Use compare(0, n, prefix) or starts_with() (C++20).

  Performance Impact: Allocation avoided

---
id: c-cpp-perf-list-random-access
language: cpp
severity: warning
message: "Performance: std::list used for random access is O(n)"
tags:
  - performance
  - container
  - list
rule:
  kind: container_access
  pattern: "std::list<$T> $L; $L[$I];  /* or index-based iteration */"
note: |
  std::list is O(n) for random access. Each [] access traverses from beginning
  or end.

  Remediation: Use std::vector or std::deque.

  Performance Impact: O(n) vs O(1)

---
id: c-cpp-perf-map-sequential-iteration
language: cpp
severity: info
message: "Performance: std::map slower than unordered_map for iteration"
tags:
  - performance
  - container
  - map
rule:
  kind: loop_with_container
  pattern: "for (auto& p : std::map<...> m)  /* no order requirement */"
note: |
  std::map iteration slower due to tree traversal. unordered_map is faster if
  ordering not required.

  Remediation: Use std::unordered_map if order not needed.

  Performance Impact: 2-5x slower iteration

---
id: c-cpp-perf-vector-insert-front
language: cpp
severity: warning
message: "Performance: Vector insert at front in loop is O(n²)"
tags:
  - performance
  - container
  - vector
rule:
  kind: loop_with_insert
  pattern: "for (...) $VEC.insert($VEC.begin(), $VAL)"
note: |
  Vector insert at front is O(n) per insert, O(n²) total. Elements after insert
  point shifted.

  Remediation: Use std::deque or build in reverse.

  Performance Impact: O(n²) vs O(n)

---
id: c-cpp-perf-map-lookup-insert
language: cpp
severity: warning
message: "Performance: Map lookup then insert does two lookups"
tags:
  - performance
  - container
  - map
rule:
  kind: conditional_insert
  pattern: "if ($MAP.find($KEY) == $MAP.end()) $MAP[$KEY] = $VAL;"
note: |
  Two lookups instead of one. find() then insert() both search. try_emplace
  is single operation.

  Remediation: Use try_emplace() or insert() with hint.

  Performance Impact: 2x lookup

---
id: c-cpp-perf-count-then-at
language: cpp
severity: info
message: "Performance: count then at does two lookups instead of one"
tags:
  - performance
  - container
  - map
rule:
  kind: map_access
  pattern: "if ($MAP.count($KEY)) { auto v = $MAP.at($KEY); }"
note: |
  Two lookups for same key. find() once is more efficient than count() + at().

  Remediation: Use find() once.

  Performance Impact: 2x lookup

---
id: c-cpp-perf-set-membership-iteration
language: cpp
severity: info
message: "Performance: std::set for both lookup and iteration"
tags:
  - performance
  - container
  - set
rule:
  kind: container_usage
  pattern: "std::set<$T> s; s.find(...); for (auto x : s)"
note: |
  std::set slower for lookup if also iterating. unordered_set has O(1) lookup
  if ordering not needed.

  Remediation: Use unordered_set unless ordering needed.

  Performance Impact: O(log n) vs O(1) lookup

---
id: c-cpp-perf-unaligned-struct
language: cpp
severity: warning
message: "Performance: Unaligned struct members cause padding waste"
tags:
  - performance
  - alignment
  - memory
rule:
  kind: struct_member_order
  pattern: "struct { char a; double b; char c; };  /* 24 bytes instead of 16 */"
note: |
  Suboptimal member order causes padding. Compiler inserts padding to align
  double, wasting space and cache.

  Remediation: Order members by decreasing size.

  Performance Impact: Struct size, cache efficiency

---
id: c-cpp-perf-simd-missing-align
language: cpp
severity: warning
message: "Performance: SIMD array without alignment specification"
tags:
  - performance
  - simd
  - alignment
rule:
  kind: array_declaration
  pattern: "float arr[SIZE];  /* used with SIMD, no alignas */"
note: |
  SIMD operations on unaligned memory are slower. Alignment to 32-byte or 64-byte
  boundaries is critical.

  Remediation: Use alignas(32) for AVX, alignas(64) for AVX-512.

  Performance Impact: 2-10x for SIMD operations

---
id: c-cpp-perf-unpredictable-branch
language: cpp
severity: warning
message: "Performance: Unpredictable branch in hot path causes misprediction penalty"
tags:
  - performance
  - branch
rule:
  kind: conditional_statement
  pattern: "if ($RANDOM_CONDITION)  /* unpredictable branch in hot path */"
note: |
  Random branches cause ~15-20 cycle misprediction penalty. Hot paths should
  minimize branching or make branches predictable.

  Remediation: Use branchless code (CMOV), or sorting to make predictable.

  Performance Impact: 15-20 cycles per mispredict

---
id: c-cpp-perf-missing-branch-hints
language: cpp
severity: info
message: "Performance: Error handling branch without [[unlikely]] hint"
tags:
  - performance
  - branch
  - optimization-hint
rule:
  kind: conditional_statement
  pattern: "if (ptr == nullptr) { throw; }  /* no [[unlikely]] */"
note: |
  [[likely]] and [[unlikely]] (C++20) help compiler optimize expected path.
  Error paths should be marked unlikely.

  Remediation: Add [[likely]]/[[unlikely]] or use __builtin_expect.

  Performance Impact: Branch layout optimization

---
id: c-cpp-perf-sparse-switch
language: cpp
severity: info
message: "Performance: Switch with sparse case values becomes if-else chain"
tags:
  - performance
  - branch
rule:
  kind: switch_statement
  pattern: "switch (x) { case 1: ... case 1000: ... }  /* sparse values */"
note: |
  Sparse case values prevent jump table optimization, compiler generates if-else
  chain instead.

  Remediation: Use lookup table or remap values to contiguous.

  Performance Impact: O(n) vs O(1)

---
id: c-cpp-perf-unbuffered-io-loop
language: cpp
severity: warning
message: "Performance: endl in loop forces flush, very expensive"
tags:
  - performance
  - io
  - loop
rule:
  kind: loop_with_io
  pattern: "for (...) { cout << x << endl; }"
note: |
  endl flushes buffer, forcing syscall (~1000+ cycles). '\n' alone is much faster.

  Remediation: Use '\n' instead of endl, batch flushes.

  Performance Impact: 1000+ cycles per syscall

---
id: c-cpp-perf-small-io-calls
language: c
severity: warning
message: "Performance: Small read/write calls have syscall overhead"
tags:
  - performance
  - io
rule:
  kind: io_operation
  pattern: "read($FD, $BUF, 1);  /* single byte */"
note: |
  Single-byte I/O causes syscall overhead (~1000+ cycles). Should buffer and
  read/write larger chunks.

  Remediation: Buffer I/O, read/write larger chunks.

  Performance Impact: 1000+ cycles per syscall

---
id: c-cpp-perf-iostream-unsync
language: cpp
severity: warning
message: "Performance: Heavy iostream usage without sync_with_stdio(false)"
tags:
  - performance
  - io
rule:
  kind: io_operation
  pattern: "heavy cin/cout without sync_with_stdio(false)"
note: |
  Synchronized streams maintain compatibility with C stdio, causing slowdown.
  Disable if not mixing C and C++ I/O.

  Remediation: Add ios_base::sync_with_stdio(false); at start.

  Performance Impact: 10x faster cin/cout

---
id: c-cpp-perf-printf-over-puts
language: c
severity: info
message: "Performance: printf for simple output is slower than puts"
tags:
  - performance
  - io
rule:
  kind: io_operation
  pattern: "printf(\"%s\\n\", $STRING)"
note: |
  Format parsing overhead for simple cases. puts() is more efficient for
  string + newline.

  Remediation: Use puts(string) for simple string output.

  Performance Impact: Minor

---
id: c-cpp-perf-exception-normal-flow
language: cpp
severity: warning
message: "Performance: Exception used for normal flow control is expensive"
tags:
  - performance
  - exception
rule:
  kind: exception_handling
  pattern: "try { ... } catch { /* expected condition */ }"
note: |
  Exception handling is expensive (~1000 cycles when thrown). Should only be
  used for actual exceptional conditions.

  Remediation: Use return values for expected failures.

  Performance Impact: 1000+ cycles when thrown

---
id: c-cpp-perf-dynamic-cast-loop
language: cpp
severity: warning
message: "Performance: dynamic_cast in loop has RTTI lookup overhead"
tags:
  - performance
  - rtti
  - loop
rule:
  kind: loop_with_cast
  pattern: "for (...) dynamic_cast<Derived*>(ptr)"
note: |
  RTTI lookup is expensive (~50 cycles per cast). Should not be in tight loops.

  Remediation: Cast once before loop, or use visitor pattern.

  Performance Impact: ~50 cycles per cast

---
id: c-cpp-perf-typeid-hot-path
language: cpp
severity: warning
message: "Performance: typeid comparison in hot path has RTTI overhead"
tags:
  - performance
  - rtti
rule:
  kind: comparison_statement
  pattern: "if (typeid(*obj) == typeid(Derived))  /* in hot path */"
note: |
  RTTI overhead for type comparison. Should avoid in hot paths.

  Remediation: Use virtual method or enum discriminator.

  Performance Impact: RTTI lookup cost

---
id: c-cpp-perf-shared-ptr-by-value
language: cpp
severity: warning
message: "Performance: shared_ptr by value has atomic reference count overhead"
tags:
  - performance
  - smart-pointer
rule:
  kind: function_parameter
  pattern: "void process(shared_ptr<Foo> p)  /* by value */"
note: |
  Passing shared_ptr by value increments/decrements atomic reference count.
  Each call has atomic operation overhead.

  Remediation: Pass by const shared_ptr& or raw pointer if not extending lifetime.

  Performance Impact: Atomic operation overhead

---
id: c-cpp-perf-make-shared-missing
language: cpp
severity: info
message: "Performance: new instead of make_shared does two allocations"
tags:
  - performance
  - smart-pointer
rule:
  kind: allocation_statement
  pattern: "shared_ptr<$T>(new $T(...))"
note: |
  Two separate allocations: one for object, one for control block. make_shared
  does single allocation.

  Remediation: Use std::make_shared<T>(...).

  Performance Impact: One extra allocation

---
id: c-cpp-perf-unique-ptr-stack
language: cpp
severity: info
message: "Performance: unique_ptr for local scope when stack allocation sufficient"
tags:
  - performance
  - smart-pointer
rule:
  kind: variable_declaration
  pattern: "unique_ptr<$T> p(new $T());  /* local scope */"
note: |
  Stack allocation may be simpler and faster for single objects in local scope.

  Remediation: Consider if heap allocation is needed.

  Performance Impact: Heap overhead

---
id: c-cpp-perf-mutex-large-scope
language: cpp
severity: warning
message: "Performance: Lock scope too large causes thread contention"
tags:
  - performance
  - synchronization
  - multithreading
rule:
  kind: critical_section
  pattern: "{ lock_guard<mutex> lg(m); /* I/O or long operations */ }"
note: |
  Holding lock during slow operations (I/O, computation) causes thread contention
  and reduces parallelism. Lock should protect only shared data.

  Remediation: Minimize critical section, lock only shared data access.

  Performance Impact: Thread contention

---
id: c-cpp-perf-lock-unlock-pattern
language: cpp
severity: info
message: "Performance: Direct lock/unlock pattern is not exception-safe"
tags:
  - performance
  - synchronization
rule:
  kind: lock_operation
  pattern: "mutex.lock(); ... mutex.unlock();"
note: |
  Direct lock/unlock not exception-safe and easy to forget unlock. Should use
  RAII guard. Not performance but correctness issue.

  Remediation: Use std::lock_guard or std::unique_lock.

  Performance Impact: Correctness, not performance

---
id: c-cpp-perf-busy-wait
language: cpp
severity: warning
message: "Performance: Busy wait without yield burns CPU"
tags:
  - performance
  - synchronization
  - multithreading
rule:
  kind: spin_loop
  pattern: "while (!$FLAG) { }  /* no yield or sleep */"
note: |
  Busy-wait loop burns CPU cycles without yielding. Wastes power and prevents
  other threads from running.

  Remediation: Use condition variable, atomic with wait, or yield.

  Performance Impact: 100% CPU waste

---
id: c-cpp-perf-excessive-atomics
language: cpp
severity: warning
message: "Performance: Multiple atomic operations can be combined"
tags:
  - performance
  - synchronization
  - atomic
rule:
  kind: atomic_operation
  pattern: "a.fetch_add(1); b.fetch_add(1);  /* separate atomics */"
note: |
  Each atomic operation has memory barrier overhead. Combining operations
  reduces barrier count.

  Remediation: Use compare_exchange or fetch_add combining operations.

  Performance Impact: 10-50 cycles per atomic

---
id: c-cpp-perf-ctor-assignment-vs-init
language: cpp
severity: warning
message: "Performance: Constructor body assignment instead of initializer list"
tags:
  - performance
  - construction
rule:
  kind: constructor_implementation
  pattern: "Foo() { member_ = value; }  /* body assignment */"
note: |
  Constructor body assignment causes default construction of member, then
  assignment. Initializer list constructs directly.

  Remediation: Use member initializer list.

  Performance Impact: Extra default construction

---
id: c-cpp-perf-temporary-creation
language: cpp
severity: warning
message: "Performance: Unnecessary temporary object creation"
tags:
  - performance
  - object-creation
rule:
  kind: function_call
  pattern: "$CONTAINER.push_back(ExpensiveObject(...))  /* temporary */"
note: |
  Function call creating unnecessary temporaries incurs construction and
  destruction overhead.

  Remediation: Use emplace, in-place construction, or references.

  Performance Impact: Object lifecycle overhead
---
id: go-perf-slice-preallocation-missing
language: go
severity: warning
message: "Performance: Slice growing in loop without preallocation causes O(log n) allocations"
tags:
  - performance
  - memory
rule:
  kind: append_stmt
  pattern: |
    for $_ := range $_ {
      $slice = append($slice, $_)
    }
note: |
  Go slices reallocate and copy when growing without preallocation. Use:
  - make([]T, 0, n) to preallocate with capacity
  - make([]T, n) if length is known
  Performance impact: 2-10x improvement depending on slice size

---
id: go-perf-string-concatenation-loop
language: go
severity: error
message: "Performance: String concatenation in loop causes O(n²) time complexity"
tags:
  - performance
  - memory
rule:
  kind: assignment_stmt
  pattern: |
    $result := ""
    for $_ := range $_ {
      $result = $result + $_
    }
note: |
  Strings are immutable; each concatenation allocates new string. Use:
  - strings.Builder{} for efficient building
  - strings.Join() for simple cases
  - bytes.Buffer as alternative
  Performance impact: 100-1000x improvement for large strings

---
id: go-perf-map-preallocation-missing
language: go
severity: warning
message: "Performance: Map growing without capacity hint causes expensive rehashing"
tags:
  - performance
  - memory
rule:
  kind: make_expr
  pattern: "make(map[$_ ]$_)"
note: |
  Maps without size hints start small and rehash as they grow. Rehashing is expensive.
  Use make(map[K]V, initialCapacity) to provide size hint.
  Performance impact: 1.5-3x improvement for large maps

---
id: go-perf-unnecessary-memory-allocation
language: go
severity: info
message: "Performance: Unnecessary memory allocation in hot path increases GC pressure"
tags:
  - performance
  - memory
rule:
  kind: make_expr
  pattern: |
    for $_ := range $_ {
      $buf := make($_, $_)
    }
note: |
  Allocating in loops stresses GC. Move allocations outside loops when possible.
  Use sync.Pool for frequently allocated/discarded objects.
  Also avoid: new(int) when var works, make([]T, 0) vs nil slice.
  Performance impact: Variable, reduces GC pressure

---
id: go-perf-large-struct-copy
language: go
severity: warning
message: "Performance: Large struct copied each iteration in range loop"
tags:
  - performance
  - memory
rule:
  kind: range_stmt
  pattern: |
    for _, $item := range $largeStructs {
      $_(item)
    }
note: |
  Range loops copy each element. For structs >64 bytes, use index access instead.
  Good alternatives:
  - for i := range structs { process(&structs[i]) }
  - Use pointer receivers instead of value receivers
  - Use slice of pointers: []*LargeStruct
  Performance impact: 2-10x for structs >64 bytes

---
id: go-perf-sync-pool-not-used
language: go
severity: warning
message: "Performance: Frequent allocations in hot path should use sync.Pool"
tags:
  - performance
  - memory
rule:
  kind: make_expr
  pattern: |
    func $_(req *http.Request) {
      buf := make([]byte, $_)
    }
note: |
  Frequently allocated objects stress GC. Use sync.Pool to reuse objects:
  - Define pool at package level
  - Get() before use
  - Put() back after use
  - Clear data when reusing (don't leak secrets)
  Performance impact: 2-5x reduction in allocations

---
id: go-perf-inefficient-byte-to-string
language: go
severity: info
message: "Performance: Byte-to-string conversion allocates new string"
tags:
  - performance
  - memory
rule:
  kind: call_expr
  pattern: "string($byteSlice)"
note: |
  string([]byte) allocates a new string. Avoid unnecessary conversions:
  - Use bytes.Equal() for comparison
  - Use bytes.Contains() for substring check
  - Convert once and reuse
  - Go 1.20+: unsafe.String() for read-only use
  Performance impact: Eliminates allocation per conversion

---
id: go-perf-escape-to-heap
language: go
severity: info
message: "Performance: Variable escapes to heap instead of staying on stack"
tags:
  - performance
  - memory
rule:
  kind: return_stmt
  pattern: |
    func $_() *$T {
      $x := $T{}
      return &$x
    }
note: |
  Variables that outlive stack frame are allocated on heap. Understand escapes:
  - Returning pointer to local variable
  - Closure capturing variables
  - Interface{} conversion
  Check with: go build -gcflags="-m"
  Stack allocation is ~10x faster than heap allocation
  Performance impact: 10x faster when kept on stack

---
id: go-perf-goroutine-leak
language: go
severity: error
message: "Performance: Goroutine leak - blocked goroutine consumes memory indefinitely"
tags:
  - performance
  - concurrency
rule:
  kind: go_stmt
  pattern: |
    go func() {
      <-$ch
    }()
note: |
  Goroutine leaks cause memory bloat. Each goroutine uses ~2KB minimum.
  Use context for cancellation:
  - select { case <-ctx.Done(): return }
  - Buffered channels to prevent blocking
  - Always ensure goroutines can exit
  Performance impact: Memory leak prevention

---
id: go-perf-unbuffered-channel-hot-path
language: go
severity: warning
message: "Performance: Unbuffered channel in hot path causes unnecessary blocking"
tags:
  - performance
  - concurrency
rule:
  kind: make_expr
  pattern: "make(chan $_)"
note: |
  Unbuffered channels synchronize sender/receiver. In high-throughput scenarios,
  buffering improves throughput:
  - Use make(chan T, bufferSize)
  - Batch process items into channel
  Performance impact: 2-10x improvement in throughput

---
id: go-perf-excessive-goroutine-creation
language: go
severity: warning
message: "Performance: Creating goroutine per item without concurrency control"
tags:
  - performance
  - concurrency
rule:
  kind: go_stmt
  pattern: |
    for $_, $item := range $items {
      go $_(item)
    }
note: |
  Millions of goroutines consume significant memory/scheduling overhead.
  Use worker pool pattern:
  - Fixed number of workers
  - Queue of jobs
  - Or use semaphore pattern with max concurrent goroutines
  Performance impact: Prevents OOM, improves throughput

---
id: go-perf-lock-contention
language: go
severity: warning
message: "Performance: Lock held too long or single lock for unrelated data"
tags:
  - performance
  - concurrency
rule:
  kind: call_expr
  pattern: |
    $.Lock()
    $_ := expensiveCall()
    $.Unlock()
note: |
  Lock contention serializes operations. Minimize critical sections:
  - Do expensive work before acquiring lock
  - Use separate locks for unrelated data
  - Use RWMutex for read-heavy workloads
  - Use atomic for simple types
  Performance impact: 2-100x improvement under contention

---
id: go-perf-waitgroup-misuse
language: go
severity: warning
message: "Performance: sync.WaitGroup.Add called after goroutine starts (race condition)"
tags:
  - performance
  - concurrency
rule:
  kind: go_stmt
  pattern: |
    go func() {
      wg.Add(1)
      defer wg.Done()
    }()
note: |
  WaitGroup.Add must be called before goroutine starts to prevent race with Wait().
  Correct pattern:
  - wg.Add(1) before go statement
  - Or wg.Add(len(items)) before loop
  Performance impact: Correctness (prevents race condition)

---
id: go-perf-context-not-propagated
language: go
severity: warning
message: "Performance: Context not propagated to dependencies, prevents cancellation"
tags:
  - performance
  - concurrency
rule:
  kind: call_expr
  pattern: "context.Background()"
note: |
  Not propagating context prevents proper cancellation and wastes work.
  Always use context parameter passed to function:
  - Pass ctx to dependencies
  - Check ctx.Done() in loops and operations
  - Enables timeout/cancellation handling
  Performance impact: Prevents wasted work

---
id: go-perf-defer-in-loop
language: go
severity: error
message: "Performance: Defer in loop accumulates until function return"
tags:
  - performance
  - memory
rule:
  kind: defer_stmt
  pattern: |
    for $_ := range $_ {
      $f, _ := os.Open($_)
      defer $f.Close()
    }
note: |
  Defer statements accumulate and execute at function return. In loops:
  - Use explicit close/cleanup in loop body
  - Extract to separate function with defer
  - Use closure: func() { defer close() ... }()
  Performance impact: Prevents memory accumulation

---
id: go-perf-unnecessary-bounds-checking
language: go
severity: info
message: "Performance: Unnecessary bounds checks on array/slice access"
tags:
  - performance
  - compiler
rule:
  kind: index_expr
  pattern: "slice[$_]"
note: |
  Go performs bounds checking. In tight loops, compiler may not eliminate checks.
  Optimization techniques:
  - Use range loop (single bounds check)
  - Use slice assertion: _ = slice[N] (proves len >= N+1)
  - Use subslice: s := slice[:3]; use s[0], s[1], s[2]
  Check with: go build -gcflags="-d=ssa/check_bce/debug=1"
  Performance impact: 5-20% in tight loops

---
id: go-perf-loop-variable-capture
language: go
severity: warning
message: "Performance: Loop variable captured by reference (Go <1.22)"
tags:
  - performance
  - loops
rule:
  kind: go_stmt
  pattern: |
    for $_, $item := range $items {
      go func() {
        $_(item)
      }()
    }
note: |
  Before Go 1.22, loop variable is shared across iterations. Goroutines see final value.
  Go 1.22+ fixes this, but copying is still clearer:
  - Copy: item := item (shadow variable)
  - Or pass as argument: func(item Item) {...}(item)
  Performance impact: Correctness (prevents bugs)

---
id: go-perf-inefficient-loop-exit
language: go
severity: info
message: "Performance: Inefficient loop exit - continues after target found"
tags:
  - performance
  - loops
rule:
  kind: range_stmt
  pattern: |
    for $_ := range $_ {
      if condition {
        $_ = true
      }
    }
note: |
  Continuing iteration after finding target wastes CPU cycles.
  Use break or return for early exit:
  - break: exit loop
  - return: exit function
  - Continue only if necessary
  Performance impact: O(n) to O(1) average case

---
id: go-perf-string-empty-comparison
language: go
severity: info
message: "Performance: String comparison with empty string"
tags:
  - performance
  - loops
rule:
  kind: binary_expr
  pattern: "$s != \"\""
note: |
  Length check is marginally faster than string comparison, but modern
  compilers optimize this equivalence. Both are correct:
  - len(s) > 0
  - s != ""
  Compiler usually optimizes string comparison to length check.
  Performance impact: Negligible (compiler optimizes)

---
id: go-perf-repeated-function-call-loop
language: go
severity: info
message: "Performance: Function called on each loop iteration in condition"
tags:
  - performance
  - loops
rule:
  kind: for_stmt
  pattern: |
    for i := 0; i < len(expensiveCall()); i++ {
      $_
    }
note: |
  Loop condition is evaluated each iteration. Cache expensive results:
  - n := len(expensiveCall()); for i := 0; i < n
  - Use range for slices/maps instead of manual loops
  Performance impact: Depends on function cost

---
id: go-perf-interface-boxing-overhead
language: go
severity: warning
message: "Performance: Value type passed to interface{} causes boxing allocation"
tags:
  - performance
  - types
rule:
  kind: call_expr
  pattern: "$func($intValue)"
note: |
  Passing value types to interface{} causes heap allocation.
  Use generics or specific types:
  - func log[T any](v T) { ... }  (Go 1.18+)
  - func logInt(v int) { ... }
  - Strings/pointers don't require boxing
  Performance impact: Eliminates allocation per call

---
id: go-perf-type-assertion-vs-switch
language: go
severity: info
message: "Performance: Multiple type assertions instead of type switch"
tags:
  - performance
  - types
rule:
  kind: type_assert_expr
  pattern: |
    if $v, ok := $x.(int); ok {
    } else if $v, ok := $x.(string); ok {
    }
note: |
  Type switch is more efficient than multiple assertions as it extracts
  type only once.
  Use: switch v := x.(type) { case int: ... case string: ... }
  Performance impact: Minor improvement with many types

---
id: go-perf-reflection-in-hot-path
language: go
severity: error
message: "Performance: Reflection in hot path is 10-100x slower than direct access"
tags:
  - performance
  - types
rule:
  kind: call_expr
  pattern: "reflect.$_"
note: |
  Reflection is 10-100x slower and prevents compiler optimizations.
  Avoid in hot paths:
  - Use type assertion instead: if i, ok := v.(int) { ... }
  - Use generics: func[T int | int64](v T) { ... }
  - Use direct field access: s.Name not reflect.ValueOf(s).FieldByName("Name")
  Performance impact: 10-100x improvement

---
id: go-perf-empty-interface-comparison
language: go
severity: info
message: "Performance: interface{} comparison is slower than typed comparison"
tags:
  - performance
  - types
rule:
  kind: binary_expr
  pattern: "$a == $b"
note: |
  interface{} comparison requires runtime type checking and deep comparison.
  Use typed comparison:
  - func equals(a, b int) bool { return a == b }
  - func equals[T comparable](a, b T) bool { return a == b }
  Performance impact: 2-5x improvement

---
id: go-perf-fmt-sprintf-simple-conversion
language: go
severity: warning
message: "Performance: fmt.Sprintf for simple conversion is slower than strconv"
tags:
  - performance
  - stdlib
rule:
  kind: call_expr
  pattern: "fmt.Sprintf(\"%d\", $_)"
note: |
  fmt.Sprintf uses reflection and is slower than specialized functions.
  Use strconv package:
  - strconv.Itoa(i) for int to string
  - strconv.FormatInt(i64, 10) for int64
  - strconv.FormatFloat(f, 'f', -1, 64)
  - strconv.FormatBool(b)
  Performance impact: 3-5x faster

---
id: go-perf-bytes-buffer-vs-strings-builder
language: go
severity: info
message: "Performance: bytes.Buffer less efficient than strings.Builder for strings"
tags:
  - performance
  - stdlib
rule:
  kind: variable_decl
  pattern: |
    var $buf bytes.Buffer
    $buf.WriteString($_)
note: |
  strings.Builder is optimized for building strings. bytes.Buffer is for []byte.
  Use strings.Builder for strings:
  - No extra copy when calling String()
  - Optimized internally for string building
  Performance impact: Saves one allocation

---
id: go-perf-inefficient-json-handling
language: go
severity: warning
message: "Performance: json.Marshal/Unmarshal allocates intermediate []byte"
tags:
  - performance
  - stdlib
rule:
  kind: call_expr
  pattern: "json.Marshal($_)"
note: |
  json.Marshal/Unmarshal allocate intermediate []byte. For streams use Encoder/Decoder:
  - json.NewEncoder(w).Encode(v)  (streaming output)
  - json.NewDecoder(r).Decode(&v)  (streaming input)
  - For high performance: jsoniter or sonic packages
  Performance impact: 20-50% improvement for large payloads

---
id: go-perf-regex-compilation-loop
language: go
severity: error
message: "Performance: Regular expression compiled in loop or function called frequently"
tags:
  - performance
  - stdlib
rule:
  kind: call_expr
  pattern: |
    for $_ := range $_ {
      $re := regexp.MustCompile($_)
    }
note: |
  Regex compilation is expensive. Compile once and reuse:
  - Compile at package level: var pattern = regexp.MustCompile(...)
  - Or in init() function
  - Reuse for multiple matches
  Performance impact: 100-1000x improvement

---
id: go-perf-time-parse-format-layout
language: go
severity: info
message: "Performance: Custom time layout instead of predefined layout"
tags:
  - performance
  - stdlib
rule:
  kind: call_expr
  pattern: "time.Parse(\"2006-01-02T15:04:05Z07:00\", $_)"
note: |
  Use predefined time layouts - they're optimized internally.
  Available: time.RFC3339, time.RFC3339Nano, time.RFC1123, time.RFC822, etc.
  Good: time.Parse(time.RFC3339, s)
  Performance impact: Minor improvement

---
id: go-perf-sort-sort-vs-slice
language: go
severity: info
message: "Performance: sort.Sort with sort.Interface vs sort.Slice"
tags:
  - performance
  - stdlib
rule:
  kind: call_expr
  pattern: "sort.Sort($_)"
note: |
  Both are valid. sort.Slice is simpler, sort.Interface can be faster for
  repeated sorts due to better inlining opportunities.
  Use sort.Slice for simplicity in most cases:
  - sort.Slice(people, func(i, j int) bool { return people[i].Age < people[j].Age })
  - Implement sort.Interface only for hot paths
  Performance impact: Similar, sort.Interface may be faster for hot paths

---
id: go-perf-io-copy-vs-manual
language: go
severity: warning
message: "Performance: Manual copy loop instead of io.Copy"
tags:
  - performance
  - stdlib
rule:
  kind: for_stmt
  pattern: |
    for {
      n, $_:= $src.Read($buf)
      $dst.Write($buf[:n])
    }
note: |
  io.Copy is optimized and may use sendfile/splice syscalls for efficient transfers.
  Use io.Copy for streaming:
  - io.Copy(dst, src)
  - io.CopyBuffer(dst, src, buf) if buffer control needed
  - io.CopyN(dst, src, n) for limited copy
  Performance impact: 2-10x for file operations

---
id: go-perf-http-client-reuse
language: go
severity: error
message: "Performance: Creating HTTP client per request prevents connection reuse"
tags:
  - performance
  - http
rule:
  kind: variable_decl
  pattern: |
    func $_(url string) {
      client := &http.Client{}
      client.Get(url)
    }
note: |
  Creating new HTTP clients prevents connection pooling and forces TCP/TLS handshakes.
  Create client once and reuse:
  - Define at package level
  - Configure connection pool: MaxIdleConns, MaxIdleConnsPerHost, IdleConnTimeout
  Performance impact: 10-100x for sequential requests

---
id: go-perf-response-body-not-closed
language: go
severity: error
message: "Performance: HTTP response body not closed causes connection leaks"
tags:
  - performance
  - http
rule:
  kind: call_expr
  pattern: "http.Get($_)"
note: |
  Unclosed response bodies cause connection leaks. Patterns:
  - Missing defer resp.Body.Close()
  - Partial body read prevents connection reuse
  - Must close even if error occurs
  Use: defer resp.Body.Close() after resp, err := http.Get(url)
  Performance impact: Prevents connection leaks

---
id: go-perf-tcp-keepalive-not-configured
language: go
severity: info
message: "Performance: TCP keep-alive not configured for connections"
tags:
  - performance
  - http
rule:
  kind: make_expr
  pattern: "&http.Transport{}"
note: |
  TCP keep-alive prevents connections from being closed by firewalls during idle.
  Configure in custom dialer:
  - DialContext: (&net.Dialer{KeepAlive: 30*time.Second}).DialContext
  - Default http.Transport already has 30s keep-alive
  Performance impact: Prevents connection drops

---
id: go-perf-small-buffer-io-copy
language: go
severity: info
message: "Performance: Small buffer for io.Copy causes more syscalls"
tags:
  - performance
  - http
rule:
  kind: variable_decl
  pattern: "buf := make([]byte, 1024)"
note: |
  For large transfers, bigger buffers reduce syscall overhead.
  Network I/O optimal: 32KB-128KB buffers.
  Use io.CopyBuffer with appropriate size:
  - buf := make([]byte, 128*1024)
  - io.CopyBuffer(dst, src, buf)
  Performance impact: 20-50% for large transfers

---
id: go-perf-sql-connection-not-pooled
language: go
severity: error
message: "Performance: SQL connection opened per query defeats connection pooling"
tags:
  - performance
  - database
rule:
  kind: call_expr
  pattern: |
    func $_() {
      db, _ := sql.Open("mysql", $_)
      defer db.Close()
      db.Query($_)
    }
note: |
  sql.DB is a connection pool. Opening per query defeats pooling.
  Create once at startup:
  - db, _ := sql.Open(driver, dsn)
  - db.SetMaxOpenConns(25)
  - db.SetMaxIdleConns(25)
  - db.SetConnMaxLifetime(5*time.Minute)
  - Reuse db handle for all queries
  Performance impact: 10-100x improvement

---
id: go-perf-query-rows-not-closed
language: go
severity: error
message: "Performance: Query result rows not closed holds database connection"
tags:
  - performance
  - database
rule:
  kind: call_expr
  pattern: |
    rows, _ := db.Query($_)
    for rows.Next() { $_ }
note: |
  Unclosed rows hold database connections, exhausting pool.
  Always close rows:
  - defer rows.Close() after rows, err := db.Query(...)
  - Check rows.Err() after loop
  Performance impact: Prevents connection exhaustion

---
id: go-perf-query-instead-of-queryrow
language: go
severity: info
message: "Performance: Using Query for single row instead of QueryRow"
tags:
  - performance
  - database
rule:
  kind: call_expr
  pattern: "db.Query($_)"
note: |
  QueryRow is optimized for single-row queries with less overhead.
  Use: err := db.QueryRow(sql, args).Scan(&value)
  Compare to: rows, _ := db.Query(...); defer rows.Close(); rows.Next()
  Performance impact: Minor improvement

---
id: go-perf-prepared-statement-not-reused
language: go
severity: warning
message: "Performance: Prepared statement not reused for repeated queries"
tags:
  - performance
  - database
rule:
  kind: for_stmt
  pattern: |
    for $_ := range $_ {
      db.Exec("INSERT INTO $_ VALUES (?)", $_)
    }
note: |
  Each query requires database parsing. Prepared statements are parsed once.
  Pattern for bulk operations:
  - stmt, _ := db.Prepare(sql)
  - defer stmt.Close()
  - for _, item := range items { stmt.Exec(...) }
  - Or use transactions for better performance
  Performance impact: 2-5x for repeated queries

---
id: go-perf-file-io-without-buffering
language: go
severity: warning
message: "Performance: File read/write without buffering causes many syscalls"
tags:
  - performance
  - io
rule:
  kind: call_expr
  pattern: "$f.WriteString($_)"
note: |
  Each Read/Write is a syscall. Buffering reduces syscall overhead dramatically.
  Use bufio package:
  - bufio.NewWriter(f) for writes
  - bufio.NewReader(f) for reads
  - Remember to Flush() writer before closing
  Performance impact: 10-100x improvement

---
id: go-perf-struct-field-ordering
language: go
severity: info
message: "Performance: Suboptimal struct field ordering causes padding waste"
tags:
  - performance
  - memory
rule:
  kind: struct_type
  pattern: |
    type Struct struct {
      a bool
      b int64
      c bool
      d int64
    }
note: |
  Fields aligned to natural alignment. Poor ordering wastes space.
  Order by size (largest first):
  - int64, int64, bool, bool instead of interleaved
  Use fieldalignment linter:
  - go install golang.org/x/tools/go/analysis/passes/fieldalignment/cmd/fieldalignment@latest
  - fieldalignment -fix ./...
  Performance impact: Reduces memory usage

---
id: go-perf-pointer-in-struct-gc-overhead
language: go
severity: info
message: "Performance: Pointer in struct causes GC scanning overhead"
tags:
  - performance
  - memory
rule:
  kind: struct_type
  pattern: |
    type Entry struct {
      Key *string
      Value *Data
    }
note: |
  Pointers require GC scanning. Large collections of pointers increase pause times.
  Prefer embedded values:
  - type Entry struct { Key string; Value Data }
  - Slice of values vs slice of pointers
  - Consider memory arena allocation (Go 1.20+)
  Performance impact: Reduces GC pause times

---
id: go-perf-function-inlining-prevention
language: go
severity: info
message: "Performance: Function with defer or complex control flow may not be inlined"
tags:
  - performance
  - compiler
rule:
  kind: function_decl
  pattern: |
    func $_(x int) int {
      defer func() {}()
      return x
    }
note: |
  Functions with defer, recover, or complex control flow aren't inlined.
  Keep hot functions simple for inlining.
  Check with: go build -gcflags="-m" ./...
  Performance impact: 5-20% in tight loops

---
id: go-perf-interface-method-call-overhead
language: go
severity: info
message: "Performance: Interface method call overhead prevents inlining"
tags:
  - performance
  - compiler
rule:
  kind: call_expr
  pattern: "$interface.Method()"
note: |
  Interface calls use dynamic dispatch, preventing inlining.
  In hot paths, use concrete types or type assertion:
  - if fr, ok := r.(*os.File); ok { /* fast path */ }
  - Or accept interface, convert to concrete type internally
  Performance impact: 2-5x in tight loops

---
id: go-perf-bounds-check-not-eliminated
language: go
severity: info
message: "Performance: Bounds check not eliminated by compiler"
tags:
  - performance
  - compiler
rule:
  kind: index_expr
  pattern: |
    for i := 0; i < len(a); i++ {
      $_ := a[i] + b[i]
    }
note: |
  Compiler can't prove both bounds. Assert equal lengths or use BCE hint:
  - if len(a) != len(b) { panic(...) }
  - _ = b[len(a)-1]  (proves len(b) >= len(a))
  Check with: go build -gcflags="-d=ssa/check_bce/debug=1"
  Performance impact: 5-15% in tight loops

---
id: go-perf-benchmark-missing-reset
language: go
severity: info
message: "Performance: Benchmark setup time included in measurement"
tags:
  - performance
  - benchmarking
rule:
  kind: function_decl
  pattern: |
    func BenchmarkX(b *testing.B) {
      data := loadData()
      for i := 0; i < b.N; i++ { $_ }
    }
note: |
  Setup time is included in measurement, skewing results.
  Reset timer after setup: b.ResetTimer()
  For per-iteration setup, use b.StopTimer()/b.StartTimer()
  Performance impact: Accurate benchmarks

---
id: go-perf-benchmark-result-not-used
language: go
severity: info
message: "Performance: Benchmark result unused - compiler may optimize away"
tags:
  - performance
  - benchmarking
rule:
  kind: function_decl
  pattern: |
    func BenchmarkX(b *testing.B) {
      for i := 0; i < b.N; i++ {
        compute(i)
      }
    }
note: |
  Compiler may optimize away unused computations.
  Use result to prevent optimization:
  - var result int
  - for i := 0; i < b.N; i++ { result = compute(i) }
  - Or use b.ReportAllocs() for allocation benchmarks
  Performance impact: Accurate benchmarks

---
id: go-perf-missing-memory-allocation-reporting
language: go
severity: info
message: "Performance: Benchmark missing allocation reporting"
tags:
  - performance
  - benchmarking
rule:
  kind: function_decl
  pattern: |
    func BenchmarkX(b *testing.B) {
      for i := 0; i < b.N; i++ { $_ }
    }
note: |
  Without ReportAllocs, allocation overhead is hidden.
  Call b.ReportAllocs() at start of benchmark.
  Run with: go test -bench=. -benchmem
  Performance impact: Better visibility into allocations

---
id: go-perf-profile-guided-optimization
language: go
severity: info
message: "Performance: Profile-Guided Optimization not used"
tags:
  - performance
  - compiler
rule:
  kind: comment
  pattern: "ignore"
note: |
  Go 1.20+ supports PGO using real profiling data. Can improve 2-7%.
  Generate profile: go test -cpuprofile=default.pgo -bench=.
  Or collect from production using pprof.
  Build with PGO: go build -pgo=default.pgo ./...
  Go 1.21+: Auto-uses default.pgo if present
  Performance impact: 2-7% improvement

---
id: go-perf-subbenchmark-not-parallel
language: go
severity: info
message: "Performance: Sub-benchmarks should run in parallel"
tags:
  - performance
  - benchmarking
rule:
  kind: method_call
  pattern: "b.Run($_, $_)"
note: |
  Sub-benchmarks run sequentially by default. Use RunParallel for speed:
  - b.RunParallel(func(pb *testing.PB) { ... })
  - Run with: go test -bench=. -cpu=1,2,4,8
  Performance impact: Faster benchmark runs

---
id: go-perf-append-to-exported-slice
language: go
severity: info
message: "Performance: Returning slice that caller may append to causes issues"
tags:
  - performance
  - memory
rule:
  kind: return_stmt
  pattern: "return $items"
note: |
  If returned slice has capacity, append may modify original unexpectedly.
  Solutions:
  - Return copy: result := make([]T, len(items)); copy(result, items)
  - Document that caller must copy
  - Return slice with len == cap
  Performance impact: Correctness and memory isolation

---
id: go-perf-using-len-in-capacity
language: go
severity: info
message: "Performance: Using len() for capacity when filtering allocates unnecessarily"
tags:
  - performance
  - memory
rule:
  kind: make_expr
  pattern: "make([]$_, 0, len($input))"
note: |
  If filtering results, allocating full input capacity wastes memory.
  Solutions:
  - Estimate based on selectivity: make([]T, 0, len(input)/2)
  - Two-pass: count first, then allocate exact amount
  Performance impact: Memory efficiency

---
id: go-perf-channel-direction-not-specified
language: go
severity: info
message: "Performance: Channel direction not specified - enables optimizations"
tags:
  - performance
  - concurrency
rule:
  kind: function_param
  pattern: "chan $_"
note: |
  Specifying channel direction enables optimizations and documents intent.
  Use: chan<- (send-only) or <-chan (receive-only)
  Good: func producer(ch chan<- int) { ch <- 42 }
  Performance impact: Minor, mostly documentation

---
id: go-perf-mutex-copied-with-value
language: go
severity: error
message: "Performance: Mutex/RWMutex copied with value breaks synchronization"
tags:
  - performance
  - concurrency
rule:
  kind: function_param
  pattern: "$_ ($T{"
note: |
  Copying mutex creates new mutex, breaking synchronization.
  Always pass by pointer:
  - func process(c *Counter) { ... }
  - Never: func process(c Counter) { ... }
  Use copylocks linter: go vet ./...
  Performance impact: Correctness (prevents race conditions)

---
id: go-perf-unnecessary-pointer-indirection
language: go
severity: info
message: "Performance: Unnecessary pointer indirection for small types"
tags:
  - performance
  - memory
rule:
  kind: struct_type
  pattern: |
    type Config struct {
      Timeout *int
      Enabled *bool
    }
note: |
  Pointers add indirection overhead. For small types (<64 bytes), values are faster.
  Use values:
  - type Config struct { Timeout int; Enabled bool }
  - Value receivers: func (p Point) Distance() float64
  Keep pointers for: large structs, mutation, shared state
  Performance impact: Better cache utilization

---
id: go-perf-atomic-non-aligned-access
language: go
severity: warning
message: "Performance: 64-bit atomic operations may not be aligned on 32-bit systems"
tags:
  - performance
  - concurrency
rule:
  kind: call_expr
  pattern: "atomic.AddInt64()"
note: |
  On 32-bit systems, 64-bit atomic requires 64-bit alignment or panics.
  Solutions:
  - Put 64-bit fields first: type Counter struct { count int64; name string }
  - Use atomic.Int64 (Go 1.19+): type Counter struct { count atomic.Int64 }
  Performance impact: Correctness on 32-bit systems

---
id: go-perf-map-iteration-order
language: go
severity: info
message: "Performance: Map iteration order is random - extra work if order matters"
tags:
  - performance
  - maps
rule:
  kind: range_stmt
  pattern: |
    for $k, $v := range $m {
      $_
    }
note: |
  Map iteration is randomized. If deterministic order needed:
  - Extract keys, sort, then iterate
  - keys := make([]string, 0, len(m)); for k := range m { keys = append(keys, k) }
  - sort.Strings(keys); for _, k := range keys { fmt.Println(k, m[k]) }
  - Or use ordered map implementation
  Performance impact: Deterministic output (overhead for sorting)

---
id: go-perf-explicit-zero-buffer
language: go
severity: info
message: "Performance: Explicit zero buffer in make(chan) is redundant"
tags:
  - performance
  - concurrency
rule:
  kind: make_expr
  pattern: "make(chan $_, 0)"
note: |
  make(chan T, 0) is equivalent to make(chan T).
  Use the shorter form: make(chan T)
  Performance impact: None (code clarity)

---
id: go-perf-slice-reslicing-capacity
language: go
severity: info
message: "Performance: Reslicing retains original capacity, preventing GC"
tags:
  - performance
  - memory
rule:
  kind: slice_expr
  pattern: "$s = $s[:$_]"
note: |
  Reslicing keeps original capacity. Large backing arrays may not be GC'd.
  Solutions:
  - Create new slice: small := make([]T, n); copy(small, s[:n])
  - Use full slice expression: s = s[:100:100] (sets capacity to 100)
  Performance impact: Memory efficiency

---
id: go-perf-repeated-error-creation
language: go
severity: info
message: "Performance: Creating error string each time allocates new error"
tags:
  - performance
  - memory
rule:
  kind: call_expr
  pattern: "errors.New(\"\")"
note: |
  errors.New creates new error each time. Pre-define errors for reuse.
  Pattern:
  - var ErrEmptyString = errors.New("empty string")
  - if condition { return ErrEmptyString }
  Performance impact: Eliminates allocation

---
id: go-perf-general-memory-efficiency
language: go
severity: info
message: "Performance: General memory efficiency - review allocation patterns"
tags:
  - performance
  - memory
rule:
  kind: variable_decl
  pattern: "ignore"
note: |
  Go memory efficiency tips:
  - Avoid heap allocations in tight loops
  - Use value types for small structs (<64 bytes)
  - Embed values instead of pointers
  - Use sync.Pool for frequently allocated objects
  - Slice/map preallocation
  - Profile before optimizing
  Performance impact: Reduces GC pressure

---
id: go-perf-concurrency-patterns
language: go
severity: info
message: "Performance: Review concurrency patterns for correctness and efficiency"
tags:
  - performance
  - concurrency
rule:
  kind: go_stmt
  pattern: "ignore"
note: |
  Concurrency best practices:
  - Propagate context for cancellation
  - Limit goroutine creation with workers/semaphore
  - Close channels only from sender
  - Buffer channels appropriately for throughput
  - Minimize lock critical sections
  - Use sync.RWMutex for read-heavy workloads
  - Use atomic for simple counters
  Performance impact: Correctness and throughput
---
id: rust-perf-vec-without-capacity
language: rust
severity: warning
message: "Performance: Vec::new() followed by multiple push() calls causes reallocations"
tags:
  - performance
  - memory-allocation
rule:
  kind: call
  pattern: "Vec::new().*push"
note: |
  Vec grows exponentially, causing reallocations. Pre-allocating avoids copying.
  Use Vec::with_capacity(n) when size is known or estimable.
  Reference: https://doc.rust-lang.org/std/vec/struct.Vec.html#capacity-and-reallocation

---
id: rust-perf-string-without-capacity
language: rust
severity: warning
message: "Performance: String::new() followed by multiple push_str() or += causes reallocations"
tags:
  - performance
  - string-handling
rule:
  kind: call
  pattern: "String::new().*push_str|String::new().*\\+="
note: |
  String reallocations copy all existing characters.
  Use String::with_capacity(n) when final size is known.
  Reference: https://doc.rust-lang.org/std/string/struct.String.html#method.with_capacity

---
id: rust-perf-box-small-types
language: rust
severity: info
message: "Performance: Boxing small types has unnecessary heap allocation overhead"
tags:
  - performance
  - memory-allocation
  - smart-pointers
rule:
  kind: type
  pattern: "Box<(u64|i64|u32|i32|u16|i16|u8|i8|f32|f64)>"
note: |
  Heap allocation overhead exceeds benefit for small types.
  Use stack allocation for small types (< ~128 bytes).
  Box is only efficient when size matters or trait objects are needed.

---
id: rust-perf-unnecessary-arc-single-threaded
language: rust
severity: info
message: "Performance: Arc<T> in single-threaded code uses unnecessary atomic operations"
tags:
  - performance
  - concurrency
  - smart-pointers
rule:
  kind: type
  pattern: "Arc<"
note: |
  Arc uses atomic operations even in single-threaded code.
  Use Rc<T> for single-threaded reference counting.
  Reference: https://doc.rust-lang.org/book/ch15-04-rc.html

---
id: rust-perf-frequent-small-allocations
language: rust
severity: error
message: "Performance: Frequent small allocations in hot loops cause fragmentation"
tags:
  - performance
  - memory-allocation
  - loops
rule:
  kind: loop
  pattern: "for.*Vec::with_capacity|for.*Vec::new|for.*String::new"
note: |
  Allocator contention and fragmentation degrade performance.
  Use object pooling, arena allocation, or stack allocation.
  Reuse allocations across loop iterations when possible.
  References: https://docs.rs/typed-arena/, https://docs.rs/bumpalo/

---
id: rust-perf-array-pool-not-used
language: rust
severity: warning
message: "Performance: Allocating temporary byte buffers in loops should use ArrayPool"
tags:
  - performance
  - memory-allocation
rule:
  kind: call
  pattern: "Vec::with_capacity.*for|Vec::new.*for"
note: |
  Temporary buffers can be pooled to avoid allocation overhead.
  Use ArrayPool pattern or SmallVec for small buffers.
  Reference: https://docs.rs/smallvec/

---
id: rust-perf-large-stack-allocations
language: rust
severity: error
message: "Performance: Large stack allocations can overflow stack and slow function calls"
tags:
  - performance
  - memory-allocation
  - dangerous
rule:
  kind: array
  pattern: "\\[u8; [0-9]{6,}\\]|\\[u8; [1-9][0-9]{5,}\\]"
note: |
  Large stack arrays (> 1MB) can overflow stack and degrade performance.
  Use Vec or Box<[T]> for large arrays.
  This is also a stack safety concern for recursive functions.

---
id: rust-perf-repeated-format-allocation
language: rust
severity: warning
message: "Performance: format!() in hot path allocates new String each time"
tags:
  - performance
  - string-handling
  - loops
rule:
  kind: call
  pattern: "for.*format!|loop.*format!"
note: |
  Each format! allocates a new String.
  Use write! to a reused buffer, or std::fmt::Write.
  Cache the formatted result if used multiple times.

---
id: rust-perf-box-when-cow-better
language: rust
severity: info
message: "Performance: Box<str> for sometimes-owned strings should use Cow"
tags:
  - performance
  - string-handling
  - smart-pointers
rule:
  kind: type
  pattern: "Box<str>|Box<\\[T\\]>"
note: |
  Cow avoids allocation when borrowing is possible.
  Use Cow<'a, str> or Cow<'a, [T]> for conditional ownership.
  Reference: https://doc.rust-lang.org/std/borrow/enum.Cow.html

---
id: rust-perf-mem-take-not-used
language: rust
severity: info
message: "Performance: clone() followed by clearing should use mem::take"
tags:
  - performance
  - cloning
rule:
  kind: sequence
  pattern: "clone().*clear()"
note: |
  mem::take swaps with default, avoiding allocation.
  Use std::mem::take(&mut value) for more efficient ownership transfer.
  Reference: https://doc.rust-lang.org/std/mem/fn.take.html

---
id: rust-perf-string-vs-str-parameters
language: rust
severity: warning
message: "Performance: Owning String parameters force callers to allocate"
tags:
  - performance
  - string-handling
  - api-design
rule:
  kind: parameter
  pattern: "fn.*\\(.*s: String.*\\)"
note: |
  Taking owned String forces callers to allocate.
  Use &str for read-only access, impl AsRef<str> for flexibility.
  Reference: https://rust-lang.github.io/api-guidelines/flexibility.html

---
id: rust-perf-repeated-string-concatenation
language: rust
severity: error
message: "Performance: String concatenation in loops may reallocate and copy"
tags:
  - performance
  - string-handling
  - loops
rule:
  kind: sequence
  pattern: "\\+= .*String|= .* \\+ .*String"
note: |
  Each concatenation may reallocate and copy the full string.
  Use String::with_capacity + push_str, or join() for slices.
  Pre-calculate final size for better performance.

---
id: rust-perf-to-string-on-literal
language: rust
severity: info
message: "Performance: to_string() on string literals allocates heap memory"
tags:
  - performance
  - string-handling
rule:
  kind: call
  pattern: "\".*\"\\.to_string|'.*'\\.to_string"
note: |
  Allocates heap memory for compile-time known string.
  Use &str literal directly, or String::from if ownership needed.
  Consider using string concatenation with owned types.

---
id: rust-perf-unnecessary-case-conversion
language: rust
severity: warning
message: "Performance: to_lowercase/to_uppercase creates new Strings for comparison"
tags:
  - performance
  - string-handling
rule:
  kind: call
  pattern: "to_lowercase.*==|to_uppercase.*==|== .*to_lowercase|== .*to_uppercase"
note: |
  Creates two new Strings for comparison.
  Use eq_ignore_ascii_case or unicase crate for case-insensitive comparison.
  Avoids allocation and is more efficient.

---
id: rust-perf-string-from-utf8-inefficiency
language: rust
severity: warning
message: "Performance: String::from_utf8 after to_vec is unnecessary copy of valid UTF-8"
tags:
  - performance
  - string-handling
rule:
  kind: call
  pattern: "String::from_utf8.*as_bytes.*to_vec"
note: |
  Unnecessary copy of already-valid UTF-8 data.
  Clone the string directly or use into_bytes/from_utf8_lossy appropriately.

---
id: rust-perf-chars-count-instead-of-len
language: rust
severity: info
message: "Performance: chars().count() is O(n) iteration instead of O(1) len()"
tags:
  - performance
  - string-handling
rule:
  kind: call
  pattern: "\\.chars\\(\\)\\.count|iter.*chars.*count"
note: |
  chars().count() iterates entire string; len() is O(1) for byte count.
  Use len() for byte count, document if char count is specifically needed.
  For char count, consider storing separately if called frequently.

---
id: rust-perf-split-then-collect
language: rust
severity: info
message: "Performance: split().collect::<Vec> allocates when iterator could work"
tags:
  - performance
  - collections
  - iterators
rule:
  kind: call
  pattern: "\\.split.*\\.collect::<Vec"
note: |
  Collecting allocates; iterator is lazy.
  Work with iterator directly when possible.
  Only collect if you need multiple passes or indexing.

---
id: rust-perf-format-for-simple-concat
language: rust
severity: info
message: "Performance: format!() for simple concatenation has formatting overhead"
tags:
  - performance
  - string-handling
rule:
  kind: call
  pattern: "format!\\(\"\\{\\}\\{\\}\""
note: |
  format! has formatting overhead for simple concatenation.
  Use + operator or push_str for simple cases.
  Or use join() for multiple strings.

---
id: rust-perf-not-using-cow-optional-transform
language: rust
severity: warning
message: "Performance: Cloning string even when unchanged should use Cow"
tags:
  - performance
  - cloning
  - string-handling
rule:
  kind: return
  pattern: "-> String|-> Box<str>"
note: |
  Cow avoids allocation when string isn't modified.
  Return Cow<'_, str> from transformation functions.
  Allows callers to use borrowed data when not modified.

---
id: rust-perf-repeated-trim-calls
language: rust
severity: info
message: "Performance: Repeated trim() calls are redundant"
tags:
  - performance
  - string-handling
rule:
  kind: sequence
  pattern: "\\.trim\\(\\).*\\.trim_start|trim_end.*trim"
note: |
  trim() already trims both ends.
  Use trim() once, then trim_start/trim_end only if needed separately.

---
id: rust-perf-hashmap-without-capacity
language: rust
severity: warning
message: "Performance: HashMap::new() for known-size maps causes resizing overhead"
tags:
  - performance
  - collections
rule:
  kind: call
  pattern: "HashMap::new\\(\\)"
note: |
  HashMap resizing is expensive (rehashes all entries).
  Use HashMap::with_capacity(n) for known-size maps.
  Reference: https://doc.rust-lang.org/std/collections/struct.HashMap.html#method.with_capacity

---
id: rust-perf-vec-contains-instead-of-hashset
language: rust
severity: error
message: "Performance: vec.contains() is O(n) instead of HashSet O(1)"
tags:
  - performance
  - collections
rule:
  kind: call
  pattern: "\\.contains\\(|in_array.*for"
note: |
  Vec lookup is O(n); HashSet is O(1).
  Use HashSet for membership testing.
  Convert to HashSet during initialization for performance-critical code.

---
id: rust-perf-linear-search-instead-of-binary
language: rust
severity: warning
message: "Performance: Linear search in sorted collections instead of binary_search"
tags:
  - performance
  - collections
rule:
  kind: call
  pattern: "\\.iter\\(\\)\\.find|find.*sorted"
note: |
  Binary search is O(log n) vs O(n) for linear.
  Use binary_search on sorted collections.
  Reference: https://doc.rust-lang.org/std/primitive.slice.html#method.binary_search

---
id: rust-perf-inefficient-entry-api
language: rust
severity: warning
message: "Performance: Double HashMap lookup with contains_key + get_mut"
tags:
  - performance
  - collections
rule:
  kind: sequence
  pattern: "contains_key.*get_mut|contains_key.*insert"
note: |
  Double lookup (contains_key + get_mut/insert).
  Use Entry API for single lookup with or_insert/or_default.
  Reference: https://doc.rust-lang.org/std/collections/hash_map/enum.Entry.html

---
id: rust-perf-vec-remove-front
language: rust
severity: error
message: "Performance: vec.remove(0) is O(n) - shifts all elements"
tags:
  - performance
  - collections
rule:
  kind: call
  pattern: "\\.remove\\(0\\)"
note: |
  Removing from front is O(n) - shifts all elements.
  Use VecDeque for queue operations.
  Reference: https://doc.rust-lang.org/std/collections/struct.VecDeque.html

---
id: rust-perf-sort-then-take
language: rust
severity: warning
message: "Performance: Full sort is O(n log n) when only top N needed"
tags:
  - performance
  - collections
rule:
  kind: sequence
  pattern: "\\.sort\\(\\).*\\[.*\\.\\."
note: |
  Full sort is O(n log n) when only top N needed.
  Use select_nth_unstable for partial sorting (O(n)).
  Reference: https://doc.rust-lang.org/std/primitive.slice.html#method.select_nth_unstable

---
id: rust-perf-clone-collection-for-iteration
language: rust
severity: warning
message: "Performance: Cloning collection for iteration is unnecessary"
tags:
  - performance
  - cloning
  - collections
rule:
  kind: call
  pattern: "for.*\\.clone\\(\\)|iter.*clone"
note: |
  Unnecessary clone of entire collection.
  Iterate by reference unless ownership needed.
  Use &collection or collection.iter() instead.

---
id: rust-perf-btreemap-when-hashmap-suffices
language: rust
severity: info
message: "Performance: BTreeMap operations are O(log n) vs HashMap O(1)"
tags:
  - performance
  - collections
rule:
  kind: type
  pattern: "BTreeMap<"
note: |
  BTreeMap operations are O(log n) vs HashMap O(1).
  Use HashMap when ordering isn't required.
  BTreeMap is useful for range queries and sorted iteration.

---
id: rust-perf-inefficient-set-difference
language: rust
severity: info
message: "Performance: Manual set difference instead of using optimized method"
tags:
  - performance
  - collections
rule:
  kind: loop
  pattern: "for.*iter.*contains"
note: |
  Built-in difference() is optimized.
  Use set.difference(&other) or &set - &other.
  Manual iteration may not be as efficient as specialized methods.

---
id: rust-perf-no-frozen-dictionary
language: rust
severity: info
message: "Performance: HashMap built once then read can use frozen pattern"
tags:
  - performance
  - collections
  - compile-time
rule:
  kind: type
  pattern: "HashMap.*const|static.*HashMap"
note: |
  Once built, a static HashMap can be optimized.
  Use phf crate for compile-time maps, or once_cell for runtime initialization.
  Reference: https://docs.rs/phf/

---
id: rust-perf-unnecessary-clone
language: rust
severity: warning
message: "Performance: .clone() when borrow would work allocates unnecessarily"
tags:
  - performance
  - cloning
rule:
  kind: call
  pattern: "\\.clone\\(\\)"
note: |
  Cloning allocates and copies data.
  Use references, or restructure to avoid clone.
  Reference: https://rust-lang.github.io/rust-clippy/master/index.html#unnecessary_clone

---
id: rust-perf-clone-in-loop
language: rust
severity: error
message: "Performance: Cloning inside loop causes N allocations"
tags:
  - performance
  - cloning
  - loops
rule:
  kind: loop
  pattern: "for.*\\.clone|while.*clone"
note: |
  N clones = N allocations in loop.
  Clone once before loop if needed, or use references.
  Consider restructuring to avoid repeated cloning.

---
id: rust-perf-copy-not-derived
language: rust
severity: info
message: "Performance: Small structs with only Copy fields missing Copy derivation"
tags:
  - performance
  - cloning
  - api-design
rule:
  kind: struct
  pattern: "struct.*{.*[ui](8|16|32|64|128|size|ptr).*}"
note: |
  Copy types avoid clone overhead.
  Add #[derive(Copy, Clone)] to small value types.
  Copy requires all fields to be Copy types.

---
id: rust-perf-rc-clone-vs-clone
language: rust
severity: info
message: "Performance: Rc::clone(&x) is idiomatic, x.clone() is equivalent"
tags:
  - performance
  - smart-pointers
rule:
  kind: call
  pattern: "Rc::clone|Arc::clone"
note: |
  Both Rc::clone(&x) and x.clone() are equivalent for Rc/Arc.
  Use x.clone() for consistency with other types.
  Rc::clone is explicit but both perform the same cheap increment.

---
id: rust-perf-to-owned-vs-to-string
language: rust
severity: info
message: "Performance: Inconsistent to_owned() vs to_string() usage"
tags:
  - performance
  - string-handling
rule:
  kind: call
  pattern: "to_owned|to_string"
note: |
  to_string() allocates; to_owned() clones.
  For &str -> String, prefer to_owned().
  Be consistent in API choices for clarity.

---
id: rust-perf-clone-in-option-map
language: rust
severity: info
message: "Performance: opt.map(|x| x.clone()) should use .cloned()"
tags:
  - performance
  - cloning
rule:
  kind: call
  pattern: "map\\(|x.*clone|cloned"
note: |
  cloned() is more idiomatic and may optimize better than map.
  Use .cloned() or .copied() for Copy types.
  Reference: Option methods

---
id: rust-perf-deep-clone-nested
language: rust
severity: error
message: "Performance: Cloning deeply nested structures multiplies clone cost"
tags:
  - performance
  - cloning
rule:
  kind: type
  pattern: "Vec<HashMap<String, Vec|HashMap<.*HashMap"
note: |
  Deep structures multiply clone cost exponentially.
  Use Rc/Arc for sharing, or Cow for copy-on-write.
  Consider structural sharing or alternative data structures.

---
id: rust-perf-clone-bound-instead-borrow
language: rust
severity: info
message: "Performance: Clone trait bound when borrowing would work"
tags:
  - performance
  - cloning
  - api-design
rule:
  kind: generic
  pattern: "<T: Clone>|fn.*T: Clone"
note: |
  Clone bound forces caller to provide owned or clone.
  Accept &T when read-only access needed.
  Use generic bound matching actual usage.

---
id: rust-perf-into-string-forcing-alloc
language: rust
severity: warning
message: "Performance: Into<String> forces allocation from &str"
tags:
  - performance
  - string-handling
  - api-design
rule:
  kind: generic
  pattern: "impl Into<String>"
note: |
  Into<String> forces allocation from &str.
  Use AsRef<str> or impl Into<Cow<'_, str>>.
  Reference: Flexible APIs design

---
id: rust-perf-not-using-cow
language: rust
severity: warning
message: "Performance: Clone then maybe modify should use Cow"
tags:
  - performance
  - cloning
rule:
  kind: return
  pattern: "clone.*maybe"
note: |
  Cow delays clone until mutation.
  Use Cow<'a, T> for possibly-modified data.
  Avoids allocation when borrowing is sufficient.
  Reference: https://doc.rust-lang.org/std/borrow/enum.Cow.html

---
id: rust-perf-collect-then-iterate
language: rust
severity: warning
message: "Performance: collect() then iterate() creates intermediate allocation"
tags:
  - performance
  - iterators
  - collections
rule:
  kind: call
  pattern: "\\.collect::<Vec.*iter|collect.*map"
note: |
  Intermediate collection allocation is unnecessary.
  Chain iterator operations directly.
  Only collect if you need multiple passes or indexing.

---
id: rust-perf-manual-loop-vs-iterator
language: rust
severity: info
message: "Performance: Manual loop that could use iterator combinator"
tags:
  - performance
  - iterators
rule:
  kind: loop
  pattern: "for.*{.*push|for.*{.*let.*if"
note: |
  Iterator methods can be better optimized by compiler.
  Use iterator combinators (map/filter/fold) when natural.
  Manual loops can prevent fusion optimizations.

---
id: rust-perf-filter-map-instead-of-filter-map
language: rust
severity: info
message: "Performance: .filter(|x| x.is_some()).map(|x| x.unwrap()) should use filter_map"
tags:
  - performance
  - iterators
rule:
  kind: call
  pattern: "filter.*is_some.*map.*unwrap|filter.*is_some.*map"
note: |
  filter_map is single pass and cleaner.
  Use .filter_map(|x| x) or .flatten().
  Reference: https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.filter_map

---
id: rust-perf-map-flatten-instead-flat-map
language: rust
severity: info
message: "Performance: .map(f).flatten() should use flat_map(f)"
tags:
  - performance
  - iterators
rule:
  kind: call
  pattern: "\\.map.*\\.flatten|flatten.*map"
note: |
  flat_map combines both operations.
  Use .flat_map(f) for efficiency.
  Reference: https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.flat_map

---
id: rust-perf-chunks-windows-not-used
language: rust
severity: info
message: "Performance: Manual index-based chunking instead of chunks/windows"
tags:
  - performance
  - iterators
rule:
  kind: loop
  pattern: "for i in.*step|for i in.*chunk"
note: |
  chunks() and windows() are optimized.
  Use slice.chunks(n) or slice.windows(n).
  Built-in methods are more efficient than manual indexing.

---
id: rust-perf-collect-before-len
language: rust
severity: warning
message: "Performance: .collect::<Vec<_>>().len() should use .count()"
tags:
  - performance
  - iterators
  - collections
rule:
  kind: call
  pattern: "\\.collect::<Vec.*\\.len|collect.*count"
note: |
  count() doesn't allocate.
  Use .count() directly on iterator.
  Avoids intermediate Vec allocation.

---
id: rust-perf-not-using-enumerate
language: rust
severity: info
message: "Performance: Manual index counter in loop should use enumerate()"
tags:
  - performance
  - iterators
rule:
  kind: loop
  pattern: "let mut i.*for.*{|for i in 0"
note: |
  enumerate() is more idiomatic and may vectorize better.
  Use .enumerate() for indexed iteration.
  Clearer intent and better compiler optimization.

---
id: rust-perf-rev-on-collect
language: rust
severity: info
message: "Performance: Collecting then reversing should collect in reverse"
tags:
  - performance
  - iterators
rule:
  kind: call
  pattern: "\\.collect.*\\.into_iter.*\\.rev|collect.*rev"
note: |
  Can collect directly in reverse.
  Use iter.rev().collect() if order matters.
  Avoid intermediate collection allocation.

---
id: rust-perf-iterator-by-ref-not-used
language: rust
severity: info
message: "Performance: Consuming iterator when partial consumption needed"
tags:
  - performance
  - iterators
rule:
  kind: call
  pattern: "\\.take\\(|iter.*take"
note: |
  by_ref() allows partial consumption then continued use.
  Use iter.by_ref().take(n).
  Reference: https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.by_ref

---
id: rust-perf-cloned-on-copy
language: rust
severity: info
message: "Performance: .cloned() on iterator of Copy types should use .copied()"
tags:
  - performance
  - iterators
rule:
  kind: call
  pattern: "\\.cloned\\(\\).*Copy|\\.cloned\\(\\).*[ui]"
note: |
  copied() is explicit about Copy semantics.
  Use .copied() for Copy types.
  Both work but copied() is more specific.

---
id: rust-perf-blocking-in-async
language: rust
severity: error
message: "Performance: Blocking calls in async context block entire runtime thread"
tags:
  - performance
  - async
  - concurrency
rule:
  kind: call
  pattern: "async.*std::fs|async.*std::io|async.*thread::sleep"
note: |
  Blocking calls block entire async runtime thread.
  Use async equivalents (tokio::fs, tokio::io).
  Or use spawn_blocking for sync code.

---
id: rust-perf-mutex-instead-of-rwlock
language: rust
severity: warning
message: "Performance: Mutex for read-heavy access when RwLock allows concurrency"
tags:
  - performance
  - concurrency
  - synchronization
rule:
  kind: type
  pattern: "Mutex<|std::sync::Mutex"
note: |
  Mutex serializes all access; RwLock allows concurrent reads.
  Use RwLock when reads dominate writes.
  Trade-off: RwLock has higher per-operation cost but better concurrency.

---
id: rust-perf-arc-mutex-non-shared
language: rust
severity: info
message: "Performance: Arc<Mutex<T>> when data accessed by only one task"
tags:
  - performance
  - concurrency
  - smart-pointers
rule:
  kind: type
  pattern: "Arc<Mutex|Arc<RwLock"
note: |
  Atomic operations have overhead for non-shared data.
  Remove Arc if not actually shared across threads.
  Verify actual usage patterns before optimization.

---
id: rust-perf-not-using-parking-lot
language: rust
severity: info
message: "Performance: std::sync::Mutex in performance-critical code"
tags:
  - performance
  - concurrency
rule:
  kind: type
  pattern: "std::sync::Mutex|std::sync::RwLock"
note: |
  parking_lot often outperforms std locks.
  Consider parking_lot::Mutex or parking_lot::RwLock.
  Reference: https://docs.rs/parking_lot/

---
id: rust-perf-holding-lock-across-await
language: rust
severity: error
message: "Performance: Holding sync lock across await causes deadlocks and blocks"
tags:
  - performance
  - async
  - concurrency
  - dangerous
rule:
  kind: sequence
  pattern: "lock.*await|guard.*await"
note: |
  Holding sync lock across await can cause deadlocks and blocks.
  Use tokio::sync::Mutex for async contexts.
  Or restructure to release lock before await.

---
id: rust-perf-spawning-task-per-item
language: rust
severity: warning
message: "Performance: Task spawning per item has overhead, use bounded concurrency"
tags:
  - performance
  - async
  - concurrency
rule:
  kind: loop
  pattern: "for.*tokio::spawn|for.*spawn"
note: |
  Task spawning has overhead; batching is more efficient.
  Use FuturesUnordered, join_all, or bounded concurrency.
  Use buffer_unordered for controlled parallelism.

---
id: rust-perf-unbounded-channel
language: rust
severity: warning
message: "Performance: Unbounded channels can consume unlimited memory"
tags:
  - performance
  - concurrency
rule:
  kind: call
  pattern: "unbounded_channel|mpsc::unbounded"
note: |
  Unbounded channels can consume unlimited memory.
  Use bounded channel with backpressure.
  Provides natural throttling mechanism.

---
id: rust-perf-polling-instead-events
language: rust
severity: warning
message: "Performance: Polling loop wastes CPU cycles"
tags:
  - performance
  - concurrency
rule:
  kind: loop
  pattern: "loop.*sleep|while.*sleep"
note: |
  Polling wastes CPU cycles.
  Use channels, condvars, or async events.
  Event-driven approach is more efficient.

---
id: rust-perf-not-using-thread-local
language: rust
severity: warning
message: "Performance: Mutex-protected per-thread cache should use thread_local"
tags:
  - performance
  - concurrency
  - synchronization
rule:
  kind: call
  pattern: "Mutex.*thread|thread.*Mutex"
note: |
  thread_local avoids synchronization entirely.
  Use thread_local! macro for per-thread data.
  Reference: https://doc.rust-lang.org/std/macro.thread_local.html

---
id: rust-perf-atomic-on-large-data
language: rust
severity: info
message: "Performance: Atomic operations work best with simple counters"
tags:
  - performance
  - concurrency
rule:
  kind: type
  pattern: "Atomic.*<(String|Vec|Box|Rc|Arc)>"
note: |
  Atomic operations work best with simple counters.
  Use Mutex<T> for complex state updates.
  Atomic limitations prevent use with large types.

---
id: rust-perf-struct-alignment
language: rust
severity: info
message: "Performance: Struct fields with poor alignment causes padding and cache misses"
tags:
  - performance
  - cache-efficiency
rule:
  kind: struct
  pattern: "struct.*u8.*u64|struct.*u16.*u64"
note: |
  Poor alignment causes padding and cache misses.
  Order fields by decreasing size.
  Or use #[repr(C)] carefully for specific layouts.

---
id: rust-perf-large-enum-variants
language: rust
severity: warning
message: "Performance: Enum variants with large inline data cause cache issues"
tags:
  - performance
  - cache-efficiency
rule:
  kind: enum
  pattern: "enum.*\\[u8; [0-9]{3,}\\]|enum.*Vec<"
note: |
  Enum size is largest variant; large data causes cache issues.
  Box large enum variants.
  Enum variants are stack-allocated and sized to largest member.

---
id: rust-perf-array-of-structs-vs-soa
language: rust
severity: warning
message: "Performance: Array of Structs has poor cache locality, use Struct of Arrays"
tags:
  - performance
  - cache-efficiency
rule:
  kind: type
  pattern: "Vec<.*{.*}>"
note: |
  AoS causes cache misses when only part of struct needed.
  Use SoA layout for hot data.
  Data-oriented design improves cache locality.

---
id: rust-perf-linkedlist-usage
language: rust
severity: warning
message: "Performance: LinkedList has poor cache locality"
tags:
  - performance
  - collections
  - cache-efficiency
rule:
  kind: type
  pattern: "LinkedList<"
note: |
  LinkedList has poor cache locality.
  Use VecDeque or Vec in most cases.
  Reference: https://github.com/rust-lang/rfcs/issues/1595

---
id: rust-perf-multiple-pointer-indirection
language: rust
severity: error
message: "Performance: Multiple levels of indirection cause cache misses"
tags:
  - performance
  - cache-efficiency
rule:
  kind: type
  pattern: "Vec<Box<Vec<Box|Box<Vec<Box"
note: |
  Each indirection causes potential cache miss.
  Flatten data structures, use indices instead of pointers.
  Trade-off between flexibility and performance.

---
id: rust-perf-not-using-smallvec
language: rust
severity: info
message: "Performance: Small collections should use SmallVec"
tags:
  - performance
  - collections
rule:
  kind: type
  pattern: "Vec<.*>|for.*Vec::new"
note: |
  Small inline storage avoids heap allocation.
  Use smallvec::SmallVec<[T; N]> for usually-small collections.
  Reference: https://docs.rs/smallvec/

---
id: rust-perf-hashmap-high-load-factor
language: rust
severity: warning
message: "Performance: HashMap with high load factor degrades to linear search"
tags:
  - performance
  - collections
rule:
  kind: type
  pattern: "HashMap<"
note: |
  High load factor degrades to linear search.
  Pre-size HashMap, or use different hash algorithm.
  Monitor collision rates for performance-critical code.

---
id: rust-perf-not-prefetching
language: rust
severity: info
message: "Performance: Random access loop without prefetch"
tags:
  - performance
  - cache-efficiency
rule:
  kind: loop
  pattern: "for.*\\[.*random|for.*shuffle"
note: |
  Prefetching can hide memory latency.
  Use std::intrinsics::prefetch_* or restructure access patterns.
  Advanced optimization for performance-critical code.

---
id: rust-perf-non-contiguous-iteration
language: rust
severity: info
message: "Performance: Non-contiguous memory access is slower"
tags:
  - performance
  - cache-efficiency
rule:
  kind: call
  pattern: "HashMap.*iter|BTreeMap.*iter.*sort"
note: |
  Non-contiguous memory access is slower.
  Collect to Vec and sort if iteration performance matters.
  Trade-off: sorting cost vs iteration efficiency.

---
id: rust-perf-trait-object-indirection
language: rust
severity: warning
message: "Performance: Trait object indirection hurts cache locality"
tags:
  - performance
  - cache-efficiency
rule:
  kind: type
  pattern: "Vec<Box<dyn|dyn.*Vec"
note: |
  Virtual dispatch and indirection hurt cache.
  Use enum dispatch or generics.
  Trade-off: flexibility vs performance.

---
id: rust-perf-division-by-power-of-two
language: rust
severity: info
message: "Performance: Division by power-of-two could use bitwise shift"
tags:
  - performance
  - numeric-operations
rule:
  kind: binary
  pattern: "/ 2|/ 4|/ 8|/ 16|/ 32|/ 64|/ 256"
note: |
  Compiler usually optimizes this automatically.
  For clarity, use x >> n for division by 2^n (unsigned).
  Compiler optimization makes explicit shifts unnecessary.

---
id: rust-perf-modulo-power-of-two
language: rust
severity: info
message: "Performance: Modulo power-of-two could use bitwise AND"
tags:
  - performance
  - numeric-operations
rule:
  kind: binary
  pattern: "% 256|% 512|% 128|% 16"
note: |
  Bitwise AND is faster than modulo for powers of two.
  Use x & (n-1) for modulo by power of two.
  Compiler may optimize this automatically.

---
id: rust-perf-no-simd
language: rust
severity: error
message: "Performance: Scalar operations on large arrays missing SIMD optimization"
tags:
  - performance
  - numeric-operations
rule:
  kind: loop
  pattern: "for.*\\[.*f32|for.*\\[.*f64|for.*\\[.*i32"
note: |
  SIMD can process 4-16x elements simultaneously.
  Use std::simd, packed_simd, or simdeez.
  Significant speedups for numeric processing.

---
id: rust-perf-float-equality
language: rust
severity: info
message: "Performance: Float == comparison may prevent optimization"
tags:
  - performance
  - numeric-operations
rule:
  kind: binary
  pattern: "f64.*==|f32.*==|== .*f64|== .*f32"
note: |
  Float comparison is imprecise; may also prevent optimization.
  Use epsilon comparison: f64::abs(a - b) < EPSILON.
  NaN behavior requires special handling.

---
id: rust-perf-pow-small-integer
language: rust
severity: info
message: "Performance: pow() for small integer powers should use multiplication"
tags:
  - performance
  - numeric-operations
rule:
  kind: call
  pattern: "\\.powi\\(2\\)|\\.pow\\(2\\)|\\.powi\\(3\\)"
note: |
  Multiplication is faster than general power.
  Use direct multiplication for small powers: x * x instead of x.pow(2).
  Compiler may not always optimize this.

---
id: rust-perf-no-fused-multiply-add
language: rust
severity: info
message: "Performance: a * b + c in hot path should use fused multiply-add"
tags:
  - performance
  - numeric-operations
rule:
  kind: binary
  pattern: "\\* .* \\+"
note: |
  FMA is single instruction with better precision.
  Use f64::mul_add(a, b, c).
  Reference: https://doc.rust-lang.org/std/primitive.f64.html#method.mul_add

---
id: rust-perf-checked-arithmetic-release
language: rust
severity: info
message: "Performance: Checked arithmetic in release mode adds branches"
tags:
  - performance
  - numeric-operations
rule:
  kind: call
  pattern: "checked_add|checked_mul|checked_sub"
note: |
  Checked operations add branches.
  Use unchecked or wrapping when overflow impossible.
  Profile before removing checks for security.

---
id: rust-perf-not-using-wrapping
language: rust
severity: info
message: "Performance: Arithmetic where overflow is expected should use wrapping"
tags:
  - performance
  - numeric-operations
rule:
  kind: binary
  pattern: "overflow|wrap"
note: |
  wrapping_add is explicit about overflow behavior.
  Use wrapping_* for intentional wrap-around.
  Documents intent and may enable optimizations.

---
id: rust-perf-float-in-integer-context
language: rust
severity: info
message: "Performance: Using f64 for counting or indexing"
tags:
  - performance
  - numeric-operations
rule:
  kind: type
  pattern: "let.*f64.*count|let.*f32.*index"
note: |
  Integer operations are faster and exact.
  Use appropriate integer types for counting/indexing.
  Floats have precision issues for integer-like usage.

---
id: rust-perf-sqrt-distance-comparison
language: rust
severity: info
message: "Performance: sqrt() is expensive when squared comparison suffices"
tags:
  - performance
  - numeric-operations
rule:
  kind: call
  pattern: "sqrt\\(\\).*<|sqrt\\(\\).*>|< .*sqrt"
note: |
  Square root is expensive.
  Compare squared values: distance_sq < threshold * threshold.
  Avoids sqrt computation for many comparisons.

---
id: rust-perf-rc-multithreaded
language: rust
severity: info
message: "Performance: Rc<T> in multithreaded code (compile error anyway)"
tags:
  - performance
  - concurrency
  - smart-pointers
rule:
  kind: type
  pattern: "Rc<|thread::spawn.*Rc"
note: |
  Rc isn't thread-safe; Arc is required.
  Use Arc<T> for shared ownership across threads.
  Compiler prevents this but worth noting pattern.

---
id: rust-perf-arc-refcell
language: rust
severity: error
message: "Performance: Arc<RefCell<T>> isn't thread-safe, use Arc<Mutex<T>>"
tags:
  - performance
  - concurrency
  - smart-pointers
rule:
  kind: type
  pattern: "Arc<RefCell|RefCell<Arc"
note: |
  RefCell isn't thread-safe; use Mutex with Arc.
  Use Arc<Mutex<T>> or Arc<RwLock<T>>.
  RefCell only works in single-threaded contexts.

---
id: rust-perf-unnecessary-box-trait-object
language: rust
severity: info
message: "Performance: Box<dyn Trait> when references would work"
tags:
  - performance
  - smart-pointers
rule:
  kind: type
  pattern: "Box<dyn"
note: |
  Box adds allocation overhead.
  Use references or generics when lifetime allows.
  Box<dyn T> necessary for ownership and type erasure.

---
id: rust-perf-refcell-borrow-hot-loop
language: rust
severity: warning
message: "Performance: RefCell::borrow() in hot loop has runtime overhead"
tags:
  - performance
  - smart-pointers
rule:
  kind: loop
  pattern: "for.*borrow\\(\\)|while.*borrow"
note: |
  Runtime borrow checking has overhead.
  Borrow once before loop.
  Avoids repeated runtime checks.

---
id: rust-perf-weak-upgrade-hot-path
language: rust
severity: info
message: "Performance: Weak::upgrade() in performance-critical code"
tags:
  - performance
  - smart-pointers
rule:
  kind: call
  pattern: "upgrade\\(\\)"
note: |
  Upgrade involves atomic operations and Option handling.
  Cache strong reference if validity is known.
  Trade-off: safety vs performance.

---
id: rust-perf-box-slice-immutable
language: rust
severity: info
message: "Performance: Box<[T]> for mutable collection lacks flexibility"
tags:
  - performance
  - collections
  - smart-pointers
rule:
  kind: type
  pattern: "Box<\\["
note: |
  Box<[T]> can't grow; may require reallocation.
  Use Vec unless you specifically need fixed capacity.
  Vec is usually more flexible.

---
id: rust-perf-nested-rc-arc
language: rust
severity: info
message: "Performance: Nested Rc/Arc creating double reference counting"
tags:
  - performance
  - smart-pointers
rule:
  kind: type
  pattern: "Rc<Rc|Arc<Arc"
note: |
  Unnecessary reference counting overhead.
  Flatten to single Rc/Arc layer.
  Verify nesting is actually needed.

---
id: rust-perf-cell-non-copy
language: rust
severity: info
message: "Performance: Cell<T> only works with Copy types"
tags:
  - performance
  - smart-pointers
rule:
  kind: type
  pattern: "Cell<String|Cell<Vec|Cell<Box"
note: |
  Cell only works with Copy types.
  Use RefCell for non-Copy types.
  Cell is zero-cost for Copy types.

---
id: rust-perf-arc-unwrap-vs-get-mut
language: rust
severity: info
message: "Performance: Arc::get_mut is more direct when uniqueness known"
tags:
  - performance
  - smart-pointers
rule:
  kind: call
  pattern: "try_unwrap"
note: |
  Arc::get_mut is more direct when there's only one reference.
  Or use Arc::make_mut() for automatic cloning.
  More idiomatic and clearer intent.

---
id: rust-perf-arc-clone-unnecessary
language: rust
severity: info
message: "Performance: Cloning Arc unnecessarily when borrowing would work"
tags:
  - performance
  - smart-pointers
rule:
  kind: call
  pattern: "arc.*clone\\(\\)|Arc::clone.*arc"
note: |
  Clone increments atomic counter.
  Use &*arc or arc.as_ref() when borrowing suffices.
  Verify actual ownership needs.

---
id: rust-perf-dynamic-dispatch-hot-path
language: rust
severity: warning
message: "Performance: Virtual dispatch in tight loop prevents inlining"
tags:
  - performance
  - function-calls
rule:
  kind: call
  pattern: "&dyn.*method|vtable"
note: |
  Virtual dispatch prevents inlining.
  Use generics or enum dispatch.
  Static dispatch enables more optimizations.

---
id: rust-perf-not-inlining-small
language: rust
severity: info
message: "Performance: Small functions in hot paths without inline hint"
tags:
  - performance
  - function-calls
rule:
  kind: function
  pattern: "fn.*{.*}|fn.*= {.*}"
note: |
  Function call overhead can dominate for tiny functions.
  Add #[inline] or #[inline(always)] hints.
  Compiler won't always inline without hints.

---
id: rust-perf-closure-large-capture
language: rust
severity: warning
message: "Performance: Closure capturing large environment by value"
tags:
  - performance
  - function-calls
rule:
  kind: closure
  pattern: "\\|.*\\| {.*self|\\|.*\\| {.*data"
note: |
  Each closure invocation may copy data.
  Capture by reference, or move to avoid repeated copying.
  Profile for actual impact on performance.

---
id: rust-perf-generic-code-bloat
language: rust
severity: info
message: "Performance: Heavily generic function causes monomorphization"
tags:
  - performance
  - function-calls
rule:
  kind: generic
  pattern: "<.*,.*,.*>"
note: |
  Monomorphization creates many copies, increasing binary size.
  Use trait objects or extract non-generic core.
  Trade-off: runtime performance vs binary size.

---
id: rust-perf-non-tail-recursive
language: rust
severity: warning
message: "Performance: Non-tail-recursive function with deep recursion"
tags:
  - performance
  - function-calls
rule:
  kind: recursion
  pattern: "fn.*{.*self.*{|recursive"
note: |
  Stack overflow risk; no TCO guarantee in Rust.
  Convert to iterative or use trampolining.
  Rust doesn't guarantee tail-call optimization.

---
id: rust-perf-pass-large-struct-value
language: rust
severity: warning
message: "Performance: Passing large struct by value for read-only"
tags:
  - performance
  - function-calls
rule:
  kind: parameter
  pattern: "fn.*\\(data: .*Struct"
note: |
  Copying large structs is expensive.
  Pass by reference: fn process(data: &LargeStruct).
  Or use sized generic bounds for different types.

---
id: rust-perf-return-large-value
language: rust
severity: info
message: "Performance: Large return values may not benefit from RVO"
tags:
  - performance
  - function-calls
rule:
  kind: return
  pattern: "-> \\[u8; [0-9]{3,}\\]|-> Vec<"
note: |
  Large return values may not be optimized (RVO).
  Return Box or use output parameter.
  Compiler RVO support varies by optimization level.

---
id: rust-perf-tiny-function-called-once
language: rust
severity: info
message: "Performance: Tiny function called once has function overhead"
tags:
  - performance
  - function-calls
rule:
  kind: function
  pattern: "fn.*\\(\\).*{.*}"
note: |
  Function call overhead for trivial operations.
  Consider inlining manually or using #[inline(always)].
  Measure impact before optimizing.

---
id: rust-perf-callback-heavy
language: rust
severity: info
message: "Performance: Callback heavy code closure indirection"
tags:
  - performance
  - function-calls
rule:
  kind: parameter
  pattern: "Fn\\(|FnMut\\(|FnOnce\\("
note: |
  Closure indirection can prevent optimization.
  Use iterators or async for composition.
  Virtual dispatch on closures has overhead.

---
id: rust-perf-no-const-fn
language: rust
severity: info
message: "Performance: Repeated runtime calculation of constant"
tags:
  - performance
  - function-calls
  - compile-time
rule:
  kind: function
  pattern: "fn.*calculate|fn.*compute|fn.*get"
note: |
  const fn moves computation to compile time.
  Mark pure functions as const fn.
  Reference: https://doc.rust-lang.org/reference/const_eval.html

---
id: rust-perf-debug-symbols-release
language: rust
severity: info
message: "Performance: Debug symbols in release profile causes bloat"
tags:
  - performance
  - compile-time
rule:
  kind: config
  pattern: "[profile.release].*debug"
note: |
  Debug info increases binary size.
  Use separate profile for debugging.
  Cargo.toml can specify different debug settings per profile.

---
id: rust-perf-no-lto
language: rust
severity: warning
message: "Performance: LTO not enabled prevents cross-crate inlining"
tags:
  - performance
  - compile-time
rule:
  kind: config
  pattern: "lto.*false|lto.*=.*false"
note: |
  LTO enables cross-crate inlining.
  Add lto = true or lto = "thin" for release.
  Reference: https://doc.rust-lang.org/cargo/reference/profiles.html#lto

---
id: rust-perf-codegen-units
language: rust
severity: info
message: "Performance: Default codegen-units prevents maximum optimization"
tags:
  - performance
  - compile-time
rule:
  kind: config
  pattern: "codegen-units|lto"
note: |
  Single codegen unit enables more optimization.
  Add codegen-units = 1 for maximum optimization.
  Trade-off: compile time vs runtime performance.

---
id: rust-perf-no-target-cpu-native
language: rust
severity: info
message: "Performance: Default target CPU misses architecture optimizations"
tags:
  - performance
  - compile-time
rule:
  kind: config
  pattern: "target-cpu|RUSTFLAGS"
note: |
  Native CPU enables architecture-specific optimizations.
  Add RUSTFLAGS="-C target-cpu=native" for local builds.
  Not portable across different CPUs.

---
id: rust-perf-unused-features
language: rust
severity: info
message: "Performance: Unused features in dependencies increase code"
tags:
  - performance
  - compile-time
rule:
  kind: config
  pattern: "default-features.*true|dependencies"
note: |
  Unused features increase compile time and binary size.
  Use default-features = false and enable only needed features.
  Reduces dependency compilation.

---
id: rust-perf-no-strip-binary
language: rust
severity: info
message: "Performance: Release binary with symbols increases size"
tags:
  - performance
  - compile-time
rule:
  kind: config
  pattern: "\\[profile.release\\]|strip"
note: |
  Symbol tables increase binary size.
  Add strip = true in release profile.
  Only strip when debug symbols not needed.

---
id: rust-perf-panic-unwind
language: rust
severity: info
message: "Performance: Panic unwinding has runtime cost"
tags:
  - performance
  - compile-time
rule:
  kind: config
  pattern: "panic.*unwind|panic.*="
note: |
  Panic unwinding has runtime cost.
  Use panic = "abort" when unwinding not needed.
  Reduces binary size and removes unwinding tables.

---
id: rust-perf-compile-time-regex
language: rust
severity: info
message: "Performance: Regex::new() at runtime for static pattern"
tags:
  - performance
  - compile-time
rule:
  kind: call
  pattern: "Regex::new\\(|Pattern::compile"
note: |
  Compile-time regex avoids runtime compilation.
  Use lazy_static! or once_cell, or regex_macro crate.
  Precompiled patterns are faster.

---
id: rust-perf-no-build-script-caching
language: rust
severity: info
message: "Performance: Build script regenerating unchanged outputs"
tags:
  - performance
  - compile-time
rule:
  kind: config
  pattern: "build.rs|cargo:rerun"
note: |
  Unnecessary rebuilds slow compilation.
  Use println!("cargo:rerun-if-changed=...").
  Reference: Build script caching

---
id: rust-perf-type-inference-slow-compile
language: rust
severity: info
message: "Performance: Long type inference chains increase compile time"
tags:
  - performance
  - compile-time
rule:
  kind: type
  pattern: "iter.*map.*map.*map|chain.*chain.*chain"
note: |
  Complex type inference increases compile time.
  Add explicit type annotations.
  Helps both compile time and readability.

---
id: rust-perf-unbuffered-file-io
language: rust
severity: error
message: "Performance: Unbuffered file I/O makes system call per read()"
tags:
  - performance
  - io
rule:
  kind: call
  pattern: "File::read\\(|read_exact"
note: |
  Each read() is a system call.
  Wrap in BufReader or BufWriter.
  Buffering dramatically improves I/O performance.

---
id: rust-perf-small-buffer-size
language: rust
severity: warning
message: "Performance: Small buffer size increases syscall frequency"
tags:
  - performance
  - io
rule:
  kind: call
  pattern: "BufReader::with_capacity\\([0-9]{1,3},|BufWriter::with_capacity\\([0-9]{1,3},"
note: |
  Small buffers increase syscall frequency.
  Use at least 8KB buffer (default is 8KB).
  Larger buffers reduce system call overhead.

---
id: rust-perf-repeated-file-open
language: rust
severity: warning
message: "Performance: Opening same file in loop has syscall overhead"
tags:
  - performance
  - io
rule:
  kind: loop
  pattern: "for.*File::open|while.*File::open"
note: |
  File open has syscall overhead.
  Open once, seek if needed.
  Reuse file handles.

---
id: rust-perf-sync-write-no-buffer
language: rust
severity: error
message: "Performance: Sync write without buffering makes system call per write"
tags:
  - performance
  - io
rule:
  kind: loop
  pattern: "for.*write_all|while.*write"
note: |
  Each write is a system call.
  Use BufWriter and flush at end.
  Significantly improves write performance.

---
id: rust-perf-no-mmap-large-files
language: rust
severity: warning
message: "Performance: Reading large file entirely into memory"
tags:
  - performance
  - io
rule:
  kind: call
  pattern: "read_to_string|read_to_end"
note: |
  Memory mapping can be more efficient for large files.
  Consider memmap2 crate for random access.
  Reference: https://docs.rs/memmap2/

---
id: rust-perf-blocking-dns-async
language: rust
severity: error
message: "Performance: Blocking DNS resolution in async context"
tags:
  - performance
  - async
  - io
rule:
  kind: call
  pattern: "async.*ToSocketAddrs|async.*resolve|async.*to_socket_addrs"
note: |
  DNS resolution blocks the thread.
  Use tokio::net::lookup_host or trust-dns.
  Async DNS is essential for async code.

---
id: rust-perf-no-http-connection-reuse
language: rust
severity: error
message: "Performance: Creating new HTTP client per request"
tags:
  - performance
  - io
rule:
  kind: loop
  pattern: "for.*reqwest::get|for.*http::Client::new"
note: |
  TCP/TLS handshake per request is expensive.
  Reuse Client instance (has connection pool).
  Significant performance improvement.

---
id: rust-perf-no-tcp-nodelay
language: rust
severity: info
message: "Performance: TCP socket with Nagle's algorithm delays small packets"
tags:
  - performance
  - io
rule:
  kind: call
  pattern: "TcpStream|set_nodelay"
note: |
  Nagle delays small packets.
  Set TCP_NODELAY for latency-sensitive connections.
  Trade-off: throughput vs latency.

---
id: rust-perf-repeated-serialization
language: rust
severity: warning
message: "Performance: Serializing before each send is repeated"
tags:
  - performance
  - io
rule:
  kind: loop
  pattern: "for.*to_vec|for.*to_string|while.*to_vec"
note: |
  Repeated serialization is expensive.
  Use serde_json::to_writer with reused buffer.
  Or serialize once then clone.

---
id: rust-perf-no-zero-copy-io
language: rust
severity: info
message: "Performance: Read to buffer then write misses zero-copy opportunities"
tags:
  - performance
  - io
rule:
  kind: sequence
  pattern: "read.*write|copy"
note: |
  Zero-copy APIs avoid user-space copying.
  Use tokio::io::copy or OS-specific APIs.
  Platform-dependent optimization.

---
id: rust-perf-benchmark-debug-build
language: rust
severity: error
message: "Performance: Benchmarking with debug build disables optimizations"
tags:
  - performance
  - benchmarking
rule:
  kind: config
  pattern: "cargo.*test|\\[profile.test\\]"
note: |
  Debug builds disable optimizations.
  Always benchmark with --release.
  Fundamental for valid performance measurement.

---
id: rust-perf-micro-optimization-no-profile
language: rust
severity: info
message: "Performance: Optimizing without profiling targets wrong code"
tags:
  - performance
  - benchmarking
rule:
  kind: comment
  pattern: "TODO.*optimize|FIXME.*fast|optimize"
note: |
  Premature optimization often targets wrong code.
  Profile first with perf, flamegraph, or criterion.
  Data-driven optimization is most effective.

---
id: rust-perf-no-black-box-benchmark
language: rust
severity: warning
message: "Performance: Benchmark result not consumed, compiler eliminates code"
tags:
  - performance
  - benchmarking
rule:
  kind: call
  pattern: "bench|test"
note: |
  Compiler may eliminate dead code.
  Use std::hint::black_box() or criterion's helpers.
  Essential for valid benchmark results.

---
id: rust-perf-benchmark-setup-inside
language: rust
severity: warning
message: "Performance: One-time setup inside timed section skews results"
tags:
  - performance
  - benchmarking
rule:
  kind: sequence
  pattern: "bench.*setup|test.*allocate"
note: |
  Setup skews results.
  Use criterion's iter_batched for setup.
  Separate setup from timed code.

---
id: rust-perf-no-cache-warmup
language: rust
severity: info
message: "Performance: Single iteration benchmark, first run includes cache warming"
tags:
  - performance
  - benchmarking
rule:
  kind: loop
  pattern: "for i in 0.*1|for _ in 0.*1"
note: |
  First run includes cache warming.
  Use multiple iterations with warm-up.
  Affects measurement validity.
---
id: csharp-perf-sync-over-async
language: csharp
severity: warning
message: "Performance: Blocking on async code with .Result, .Wait(), or .GetAwaiter().GetResult() risks deadlocks in synchronization contexts"
tags:
  - performance
  - async
rule:
  kind: property_access
  pattern: |
    (
      (expression).Result |
      (expression).Wait() |
      (expression).GetAwaiter().GetResult()
    )
note: |
  Synchronously blocking on async code can cause deadlocks in ASP.NET, WPF, WinForms contexts.
  Fix: Propagate async up the call chain using await instead of blocking.
  Reference: CA2007

---
id: csharp-perf-async-void
language: csharp
severity: warning
message: "Performance: Async void methods swallow exceptions and prevent caller from awaiting"
tags:
  - performance
  - async
rule:
  kind: method_declaration
  pattern: "async void MethodName"
note: |
  Async void methods should only be used for event handlers. They swallow exceptions and prevent caller from awaiting.
  Fix: Use async Task for all async methods except event handlers.
  Reference: CA2007, ASYNC0001

---
id: csharp-perf-unnecessary-async-await
language: csharp
severity: info
message: "Performance: Unnecessary async/await adds overhead when single await is final statement"
tags:
  - performance
  - async
rule:
  kind: method_declaration
  pattern: "async Task Method() { return await Task.FromResult(x); }"
note: |
  Single await at end with immediate return adds state machine overhead. Fix: Return Task directly when no exception handling or using blocks needed.
  Reference: CA2007

---
id: csharp-perf-missing-configure-await
language: csharp
severity: warning
message: "Performance: Missing ConfigureAwait(false) in library code captures synchronization context unnecessarily"
tags:
  - performance
  - async
rule:
  kind: invocation
  pattern: "await (task)"
note: |
  In library code, ConfigureAwait(false) avoids unnecessary context capture.
  Fix: Use await expression.ConfigureAwait(false) in libraries, not app code.
  Reference: CA2007

---
id: csharp-perf-task-run-in-aspnet
language: csharp
severity: warning
message: "Performance: Task.Run in ASP.NET double-hops thread pool unnecessarily"
tags:
  - performance
  - async
rule:
  kind: invocation
  pattern: "Task.Run(async () => ...)"
note: |
  ASP.NET already runs on thread pool. Task.Run adds unnecessary work.
  Fix: Use async I/O directly or synchronous code; avoid double-hop.
  Reference: ASP.NET Performance

---
id: csharp-perf-async-lambda-no-await
language: csharp
severity: warning
message: "Performance: Async lambda without await allocates state machine unnecessarily"
tags:
  - performance
  - async
rule:
  kind: lambda_expression
  pattern: "async () => { SyncMethod(); }"
note: |
  Async lambda without any await is synchronous but allocates state machine.
  Fix: Remove async keyword if no awaits are needed.
  Reference: CA1849

---
id: csharp-perf-task-factory-start-new
language: csharp
severity: warning
message: "Performance: Task.Factory.StartNew with async lambda returns Task<Task>, not awaited"
tags:
  - performance
  - async
rule:
  kind: invocation
  pattern: "Task.Factory.StartNew(async () => ...)"
note: |
  Returns Task<Task>, not Task. Inner task not awaited by default.
  Fix: Use Task.Run or properly unwrap the result.
  Reference: Stephen Toub's blog

---
id: csharp-perf-missing-cancellation-token
language: csharp
severity: warning
message: "Performance: Missing CancellationToken parameter prevents operation cancellation"
tags:
  - performance
  - async
rule:
  kind: method_declaration
  pattern: "async Task MethodAsync()"
note: |
  Async operations without CancellationToken parameter have no way to cancel long-running work, wasting resources.
  Fix: Accept and forward CancellationToken through async call chains.
  Reference: CA1068

---
id: csharp-perf-parallel-foreach-async
language: csharp
severity: warning
message: "Performance: Parallel.ForEach with async delegate fires-and-forgets, doesn't await"
tags:
  - performance
  - async
rule:
  kind: invocation
  pattern: "Parallel.ForEach(items, async item => ...)"
note: |
  Parallel.ForEach doesn't await async delegates, firing-and-forgetting them.
  Fix: Use await Task.WhenAll(items.Select(async item => ...)) or Channels.
  Reference: CA1812

---
id: csharp-perf-task-whenall-enumerable
language: csharp
severity: warning
message: "Performance: Task.WhenAll with IEnumerable may enumerate multiple times internally"
tags:
  - performance
  - async
rule:
  kind: invocation
  pattern: "Task.WhenAll(tasks)"
note: |
  When tasks is IEnumerable, WhenAll internally enumerates multiple times, adding overhead.
  Fix: Materialize to array first: await Task.WhenAll(tasks.ToArray()).
  Reference: .NET Performance

---
id: csharp-perf-string-concat-loop
language: csharp
severity: warning
message: "Performance: String concatenation in loop causes O(n²) complexity"
tags:
  - performance
  - strings
rule:
  kind: assignment
  pattern: "result += string"
note: |
  Each concatenation creates new string. Multiple concatenations have O(n²) complexity.
  Fix: Use StringBuilder for loop concatenation.
  Reference: CA1845, CA1822

---
id: csharp-perf-string-format-vs-interpolation
language: csharp
severity: info
message: "Performance: String.Format is less readable than string interpolation"
tags:
  - performance
  - strings
rule:
  kind: invocation
  pattern: 'String.Format("{0} {1}", a, b)'
note: |
  String interpolation is more readable and can be optimized by compiler.
  Fix: Use $"{a} {b}" string interpolation instead.
  Reference: IDE0071

---
id: csharp-perf-substring-allocation
language: csharp
severity: warning
message: "Performance: Substring allocates new string; use Span for read-only access"
tags:
  - performance
  - strings
rule:
  kind: invocation
  pattern: "str.Substring(start, length)"
note: |
  Substring allocates new string in hot paths. Use Span for read-only access.
  Fix: Use str.AsSpan(start, length) for read-only access.
  Reference: CA1846

---
id: csharp-perf-string-contains-no-comparison
language: csharp
severity: info
message: "Performance: String.Contains without StringComparison parameter"
tags:
  - performance
  - strings
rule:
  kind: invocation
  pattern: 'str.Contains("text")'
note: |
  Without StringComparison, defaults to case-sensitive ordinal. Be explicit about comparison intent.
  Fix: Use str.Contains("text", StringComparison.OrdinalIgnoreCase) when case-insensitive needed.
  Reference: CA1307

---
id: csharp-perf-tolower-toupper-comparison
language: csharp
severity: warning
message: "Performance: ToLower/ToUpper for comparison allocates new strings unnecessarily"
tags:
  - performance
  - strings
rule:
  kind: invocation
  pattern: "str.ToLower() == other.ToLower()"
note: |
  ToLower and ToUpper allocate new strings just for comparison.
  Fix: Use string.Equals(str, other, StringComparison.OrdinalIgnoreCase).
  Reference: CA1862

---
id: csharp-perf-startswith-endswith-no-ordinal
language: csharp
severity: info
message: "Performance: StartsWith/EndsWith without StringComparison uses culture-sensitive comparison"
tags:
  - performance
  - strings
rule:
  kind: invocation
  pattern: 'str.StartsWith("prefix")'
note: |
  Culture-sensitive comparison is slower than ordinal. Specify comparison for performance.
  Fix: Use str.StartsWith("prefix", StringComparison.Ordinal) for performance.
  Reference: CA1310

---
id: csharp-perf-string-split-single-part
language: csharp
severity: warning
message: "Performance: String.Split to get single part allocates entire array"
tags:
  - performance
  - strings
rule:
  kind: invocation
  pattern: "str.Split(',')[0]"
note: |
  Split allocates array and all substrings when only one part needed.
  Fix: Use str.AsSpan().Slice(0, str.IndexOf(',')) for single part extraction.
  Reference: CA1847

---
id: csharp-perf-repeated-string-replace
language: csharp
severity: warning
message: "Performance: Repeated String.Replace chains allocate intermediate strings"
tags:
  - performance
  - strings
rule:
  kind: invocation
  pattern: 'str.Replace("a", "b").Replace("c", "d")'
note: |
  Each Replace allocates new string. Multiple replaces compound allocation overhead.
  Fix: Use StringBuilder or single regex replace for multiple replacements.
  Reference: CA1845

---
id: csharp-perf-tostring-on-string
language: csharp
severity: info
message: "Performance: ToString() call on string variable is redundant"
tags:
  - performance
  - strings
rule:
  kind: invocation
  pattern: "stringVariable.ToString()"
note: |
  Unnecessary method call on already string value. Fix: Remove redundant .ToString().
  Reference: CA1860

---
id: csharp-perf-string-concat-small
language: csharp
severity: info
message: "Performance: StringBuilder may be overhead for small number of concatenations"
tags:
  - performance
  - strings
rule:
  kind: variable_declaration
  pattern: "new StringBuilder()"
note: |
  String.Concat is optimized for small numbers of strings (2-3). StringBuilder better for larger counts.
  Fix: Use string.Concat(a, b, c) or interpolation for small counts.
  Reference: .NET Performance Guidelines

---
id: csharp-perf-list-capacity-not-set
language: csharp
severity: warning
message: "Performance: List capacity not set causes repeated allocations and copies as list grows"
tags:
  - performance
  - collections
rule:
  kind: object_creation
  pattern: "new List<T>()"
note: |
  List grows by doubling, causing allocations and copies on each resize. If expected size is known, specify it.
  Fix: Specify capacity: new List<T>(expectedCount).
  Reference: CA1829

---
id: csharp-perf-array-concat-accumulation
language: csharp
severity: warning
message: "Performance: Array.Concat in loop causes O(n²) complexity"
tags:
  - performance
  - collections
rule:
  kind: assignment
  pattern: "array = array.Concat(item).ToArray()"
note: |
  Creates new array on each operation. Accumulating items has O(n²) complexity.
  Fix: Use List<T> for growing collections, convert to array once at end.
  Reference: CA1825

---
id: csharp-perf-multiple-enumeration
language: csharp
severity: warning
message: "Performance: Multiple enumeration of IEnumerable re-executes operations"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: |
    (
      .Count() > 0 && .First() |
      foreach on same IEnumerable (multiple times)
    )
note: |
  IEnumerable may re-execute expensive operations on each enumeration.
  Fix: Materialize with .ToList() or use .Any() and cache result.
  Reference: CA1851

---
id: csharp-perf-count-when-any-suffices
language: csharp
severity: warning
message: "Performance: Count() enumerates entire sequence; Any() stops at first element"
tags:
  - performance
  - collections
rule:
  kind: comparison
  pattern: "collection.Count() > 0"
note: |
  Count() enumerates entire sequence checking existence. Any() stops at first match.
  Fix: Use collection.Any() instead of collection.Count() > 0.
  Reference: CA1827

---
id: csharp-perf-orderby-before-where
language: csharp
severity: warning
message: "Performance: OrderBy before Where sorts entire collection before filtering"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: "collection.OrderBy(x => x.Prop).Where(x => x.Cond)"
note: |
  Sorting entire collection before filtering reduces query set is inefficient.
  Fix: Filter first: .Where(x => x.Cond).OrderBy(x => x.Prop).
  Reference: LINQ Performance

---
id: csharp-perf-tolist-array-in-loop
language: csharp
severity: warning
message: "Performance: ToList/ToArray inside loop materializes collection per iteration"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: |
    foreach(var x in collection.ToList())
    {
      Use(collection.ToList());
    }
note: |
  Materialization inside loop creates allocation per iteration, severe overhead.
  Fix: Materialize once before loop, reuse materialized collection.
  Reference: CA1851

---
id: csharp-perf-linq-in-hot-path
language: csharp
severity: warning
message: "Performance: LINQ methods in hot path cause allocation overhead per iteration"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: ".Select() | .Where() | (other LINQ)"
note: |
  LINQ has per-iteration allocation overhead. In hot paths called millions of times, use explicit loops.
  Fix: Replace with explicit for/foreach loops with pre-sized collections.
  Reference: .NET Performance

---
id: csharp-perf-dict-containskey-then-index
language: csharp
severity: warning
message: "Performance: Dictionary ContainsKey then index lookup is two hash lookups"
tags:
  - performance
  - collections
rule:
  kind: if_statement
  pattern: "if (dict.ContainsKey(k)) { var v = dict[k]; }"
note: |
  Two hash lookups instead of one. TryGetValue is atomic single lookup.
  Fix: Use if (dict.TryGetValue(k, out var v)) { ... }.
  Reference: CA1854

---
id: csharp-perf-unnecessary-tolist-linq
language: csharp
severity: warning
message: "Performance: Intermediate ToList in LINQ chain breaks lazy evaluation"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: ".Where(...).ToList().Select(...)"
note: |
  Intermediate ToList breaks lazy evaluation and allocates intermediate collection.
  Fix: Remove intermediate materialization unless specifically needed.
  Reference: CA1851

---
id: csharp-perf-list-find-vs-linq
language: csharp
severity: info
message: "Performance: List.Find may avoid allocating enumerator vs LINQ"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: "list.Find(x => x.Prop == val)"
note: |
  List.Find avoids allocating enumerator in some cases vs LINQ FirstOrDefault.
  Reference: .NET Performance

---
id: csharp-perf-list-contains-o-n
language: csharp
severity: warning
message: "Performance: List.Contains is O(n); use HashSet for repeated lookups"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: "list.Contains(item)"
note: |
  List.Contains is O(n) linear search. HashSet.Contains is O(1) constant time.
  Fix: Convert to HashSet<T> for repeated containment checks.
  Reference: CA1860

---
id: csharp-perf-selectmany-vs-nested-loops
language: csharp
severity: warning
message: "Performance: SelectMany on large collections allocates intermediate results"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: "SelectMany(x => x.Items).Where(...)"
note: |
  SelectMany allocates intermediate results. For very large collections, nested foreach more efficient.
  Fix: Use nested foreach loops for hot path performance.
  Reference: .NET Performance

---
id: csharp-perf-concurrent-dict-getadd-lambda
language: csharp
severity: warning
message: "Performance: ConcurrentDictionary GetOrAdd lambda allocates even when key exists"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: "dict.GetOrAdd(key, k => new Value())"
note: |
  Lambda allocates even when key already exists. Fix: Use GetOrAdd overload with lazy factory or check first.
  Reference: CA1863

---
id: csharp-perf-enumerable-range-allocation
language: csharp
severity: warning
message: "Performance: Enumerable.Range allocates iterator; for loop doesn't"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: "Enumerable.Range(0, n).Select(...)"
note: |
  Range allocates iterator object. For loop is more efficient in hot paths.
  Fix: Use for (int i = 0; i < n; i++) in hot paths.
  Reference: .NET Performance

---
id: csharp-perf-toarray-vs-tolist-final
language: csharp
severity: info
message: "Performance: ToArray preferred over ToList if result won't be modified"
tags:
  - performance
  - collections
rule:
  kind: invocation
  pattern: "enumerable.ToList()"
note: |
  ToList has slight wrapper overhead if result is never modified. ToArray is cleaner final form.
  Fix: Use ToArray if result won't be modified.
  Reference: CA1825

---
id: csharp-perf-missing-span-substring
language: csharp
severity: warning
message: "Performance: Substring allocates; use Span for zero-copy substring"
tags:
  - performance
  - span
rule:
  kind: invocation
  pattern: 'str.Substring(0, 5).Contains("x")'
note: |
  Substring allocates new string. Span<char> provides zero-copy view.
  Fix: Use str.AsSpan(0, 5).Contains("x").
  Reference: CA1846

---
id: csharp-perf-parsing-without-span
language: csharp
severity: warning
message: "Performance: Parsing substring allocates intermediate string"
tags:
  - performance
  - span
rule:
  kind: invocation
  pattern: "int.Parse(str.Substring(0, 5))"
note: |
  Substring creates allocation before parsing. Span allows direct parsing without allocation.
  Fix: Use int.Parse(str.AsSpan(0, 5)).
  Reference: .NET Performance

---
id: csharp-perf-array-copy-vs-span-slice
language: csharp
severity: info
message: "Performance: Array.Copy can often be replaced with Span.Slice for read-only ops"
tags:
  - performance
  - span
rule:
  kind: invocation
  pattern: "Array.Copy(src, dest, len)"
note: |
  Span.Slice provides zero-copy view of subsequence. More efficient for read-only operations.
  Fix: Use Span<T> for read-only operations instead of copying.
  Reference: CA1846

---
id: csharp-perf-memorystream-parsing
language: csharp
severity: warning
message: "Performance: MemoryStream adds overhead; use Span for byte array parsing"
tags:
  - performance
  - span
rule:
  kind: object_creation
  pattern: "new MemoryStream(bytes)"
note: |
  MemoryStream adds wrapper overhead. Span works directly on bytes.
  Fix: Use ReadOnlySpan<byte> for parsing byte arrays.
  Reference: .NET Performance

---
id: csharp-perf-stackalloc-unlimited-size
language: csharp
severity: warning
message: "Performance: Unlimited stackalloc with variable size causes stack overflow risk"
tags:
  - performance
  - span
rule:
  kind: allocation_expression
  pattern: "stackalloc byte[size]"
note: |
  Large stackalloc causes stack overflow. Limit size or use heap allocation for large buffers.
  Fix: Use size <= 256 ? stackalloc byte[size] : new byte[size].
  Reference: CA2014

---
id: csharp-perf-closure-allocation
language: csharp
severity: warning
message: "Performance: Capturing local variable in lambda allocates closure class"
tags:
  - performance
  - allocation
rule:
  kind: lambda_expression
  pattern: "items.Where(x => x.Value == localVar)"
note: |
  Capturing local variable allocates closure class per lambda. Avoid capturing when possible.
  Fix: Make localVar static/const or pass via parameter if possible.
  Reference: CA1859

---
id: csharp-perf-params-array-allocation
language: csharp
severity: warning
message: "Performance: Each params array call allocates new array"
tags:
  - performance
  - allocation
rule:
  kind: method_declaration
  pattern: "Method(params object[] args)"
note: |
  Each call allocates new array. Frequent calls cause overhead.
  Fix: Provide overloads with fixed parameter counts.
  Reference: CA1825

---
id: csharp-perf-boxing-value-types
language: csharp
severity: warning
message: "Performance: Boxing value types allocates heap memory"
tags:
  - performance
  - allocation
rule:
  kind: cast_expression
  pattern: "object o = 42"
note: |
  Boxing allocates heap memory for value types. Use generics to avoid.
  Fix: Use generic constraints like IEquatable<T> instead of interface boxing.
  Reference: CA1859

---
id: csharp-perf-tuple-vs-valuetuple
language: csharp
severity: warning
message: "Performance: Tuple<T> is class (heap); use value tuples (a,b,c) instead"
tags:
  - performance
  - allocation
rule:
  kind: invocation
  pattern: "Tuple.Create(a, b, c)"
note: |
  Tuple<T> is reference type (heap allocation). Value tuples are stack-allocated.
  Fix: Use value tuples (a, b, c) for local use.
  Reference: CA2263

---
id: csharp-perf-anonymous-type-hot-loop
language: csharp
severity: warning
message: "Performance: Anonymous types are classes; allocated on heap in hot loops"
tags:
  - performance
  - allocation
rule:
  kind: object_creation
  pattern: "new { x.Name, x.Value }"
note: |
  Anonymous types are classes allocated on heap. Hot loops cause GC pressure.
  Fix: Use value tuples or defined structs.
  Reference: .NET Performance

---
id: csharp-perf-string-join-with-select
language: csharp
severity: warning
message: "Performance: String.Join with Select allocates from each ToString call"
tags:
  - performance
  - allocation
rule:
  kind: invocation
  pattern: 'String.Join(",", items.Select(x => x.ToString()))'
note: |
  Select creates iterator and each ToString allocates string.
  Fix: Use StringBuilder or span-based approaches for hot paths.
  Reference: CA1845

---
id: csharp-perf-regex-compilation
language: csharp
severity: warning
message: "Performance: Regex without RegexOptions.Compiled interprets pattern each time"
tags:
  - performance
  - allocation
rule:
  kind: object_creation
  pattern: "new Regex(pattern)"
note: |
  Regex without compilation interprets pattern each invocation in hot path.
  Fix: Use static Regex field with RegexOptions.Compiled.
  Reference: CA2249

---
id: csharp-perf-stringreader-writer
language: csharp
severity: info
message: "Performance: StringReader allocates wrapper; Span could work directly"
tags:
  - performance
  - allocation
rule:
  kind: object_creation
  pattern: "new StringReader(text)"
note: |
  StringReader allocates reader wrapper. Span could handle simple parsing directly.
  Fix: Use text.AsSpan() for simple parsing.
  Reference: .NET Performance

---
id: csharp-perf-event-handler-delegate
language: csharp
severity: info
message: "Performance: Lambda event handlers allocate new delegate on each subscription"
tags:
  - performance
  - allocation
rule:
  kind: lambda_expression
  pattern: "button.Click += (s, e) => Handle()"
note: |
  Each subscription allocates new delegate. Can accumulate memory.
  Fix: Use method group or cache delegate instance.
  Reference: CA1849

---
id: csharp-perf-expression-compile-loop
language: csharp
severity: warning
message: "Performance: Expression.Compile() is expensive; should be cached"
tags:
  - performance
  - allocation
rule:
  kind: invocation
  pattern: "expression.Compile()"
note: |
  Expression compilation is expensive. In loops, causes repeated compilation.
  Fix: Compile once and cache the compiled delegate.
  Reference: .NET Performance

---
id: csharp-perf-large-struct-by-value
language: csharp
severity: warning
message: "Performance: Large struct (>16 bytes) passed by value is copied repeatedly"
tags:
  - performance
  - valuetypes
rule:
  kind: parameter
  pattern: "struct (>16 bytes) by value"
note: |
  Large structs copied on each method pass. Use in parameter for pass-by-reference.
  Fix: Pass by in reference: Method(in largeStruct).
  Reference: CA1815

---
id: csharp-perf-mutable-struct-properties
language: csharp
severity: warning
message: "Performance: Mutable struct properties cause accidental copies and bugs"
tags:
  - performance
  - valuetypes
rule:
  kind: property_declaration
  pattern: "struct Point { public int X { get; set; } }"
note: |
  Property mutation on copy doesn't affect original. Use readonly struct for value types.
  Fix: Use readonly struct for immutable value types.
  Reference: CA1815

---
id: csharp-perf-struct-interface-boxing
language: csharp
severity: warning
message: "Performance: Passing struct as interface reference boxes the struct"
tags:
  - performance
  - valuetypes
rule:
  kind: assignment
  pattern: "IEquatable<T> equatable = myStruct"
note: |
  Interface reference boxes the struct, defeating value type benefits.
  Fix: Use generic constraints where T : IEquatable<T> instead.
  Reference: CA1815

---
id: csharp-perf-struct-missing-gethashcode
language: csharp
severity: warning
message: "Performance: Struct without GetHashCode override uses slow reflection"
tags:
  - performance
  - valuetypes
rule:
  kind: type_declaration
  pattern: "struct without GetHashCode()"
note: |
  Default struct GetHashCode uses reflection, very slow. Override it properly.
  Fix: Override GetHashCode() with efficient implementation.
  Reference: CA1815

---
id: csharp-perf-valuetask-result-access
language: csharp
severity: warning
message: "Performance: ValueTask.Result without IsCompleted check can block"
tags:
  - performance
  - valuetypes
rule:
  kind: property_access
  pattern: "valueTask.Result"
note: |
  ValueTask may not be completed. Result access blocks thread.
  Fix: Await properly or check IsCompleted first.
  Reference: CA2012

---
id: csharp-perf-arraypool-not-returned
language: csharp
severity: warning
message: "Performance: ArrayPool.Rent without Return defeats pooling purpose"
tags:
  - performance
  - pooling
rule:
  kind: invocation
  pattern: "ArrayPool<T>.Shared.Rent(size)"
note: |
  Not returning arrays to pool defeats pooling benefit. Rent without corresponding Return is a leak.
  Fix: Use try/finally to ensure ArrayPool<T>.Shared.Return(array).
  Reference: CA1851

---
id: csharp-perf-arraypool-unnecessary-clear
language: csharp
severity: info
message: "Performance: Clearing ArrayPool return adds overhead if not security-critical"
tags:
  - performance
  - pooling
rule:
  kind: invocation
  pattern: "ArrayPool.Return(array, clearArray: true)"
note: |
  Clearing adds overhead. Only use clearArray: true for security-sensitive data.
  Fix: Only use clearArray: true for sensitive data.
  Reference: .NET Performance

---
id: csharp-perf-new-array-vs-pool
language: csharp
severity: warning
message: "Performance: New large array allocation adds GC pressure; use ArrayPool"
tags:
  - performance
  - pooling
rule:
  kind: object_creation
  pattern: "new byte[largeSize]"
note: |
  Large allocations trigger GC pressure and fragmentation.
  Fix: Use ArrayPool<byte>.Shared.Rent(largeSize).
  Reference: CA1861

---
id: csharp-perf-missing-objectpool
language: csharp
severity: warning
message: "Performance: Creating expensive objects in hot path causes GC churn"
tags:
  - performance
  - pooling
rule:
  kind: object_creation
  pattern: "new ExpensiveObject()"
note: |
  Repeated creation of expensive objects causes GC pressure. Use object pooling.
  Fix: Use ObjectPool<T> from Microsoft.Extensions.ObjectPool.
  Reference: .NET Performance

---
id: csharp-perf-recyclable-memory-stream
language: csharp
severity: warning
message: "Performance: MemoryStream for large temporary streams reallocates buffers"
tags:
  - performance
  - pooling
rule:
  kind: object_creation
  pattern: "new MemoryStream()"
note: |
  MemoryStream allocates and resizes buffers. RecyclableMemoryStream pools memory.
  Fix: Use Microsoft.IO.RecyclableMemoryStream for large temporary streams.
  Reference: RecyclableMemoryStream docs

---
id: csharp-perf-task-fromresult-overuse
language: csharp
severity: info
message: "Performance: Each Task.FromResult call allocates new Task"
tags:
  - performance
  - task
rule:
  kind: invocation
  pattern: "Task.FromResult"
note: |
  Each call allocates new Task. Cache completed tasks for repeated use.
  Fix: private static readonly Task<int> CachedResult = Task.FromResult(0);
  Reference: CA1849

---
id: csharp-perf-valuetask-double-await
language: csharp
severity: warning
message: "Performance: ValueTask can only be awaited once"
tags:
  - performance
  - task
rule:
  kind: invocation
  pattern: |
    (
      await valueTask;
      await valueTask;
    )
note: |
  ValueTask may not be re-awaitable depending on underlying task. Awaiting twice is unsafe.
  Fix: Store result or convert to Task if needing multiple awaits.
  Reference: CA2012

---
id: csharp-perf-task-delay-zero
language: csharp
severity: info
message: "Performance: Task.Delay(0) allocates unnecessarily; use Task.Yield()"
tags:
  - performance
  - task
rule:
  kind: invocation
  pattern: "Task.Delay(0)"
note: |
  Delay(0) just yields but allocates timer. Task.Yield() is lighter weight.
  Fix: Use Task.Yield() for yielding to other tasks.
  Reference: .NET Performance

---
id: csharp-perf-valuetask-always-sync
language: csharp
severity: info
message: "Performance: ValueTask optimized for sometimes-sync; return cached Task if always sync"
tags:
  - performance
  - task
rule:
  kind: return_statement
  pattern: "return new ValueTask<T>"
note: |
  ValueTask overhead not worth it if always completes synchronously.
  Fix: Return result directly or use cached Task for always-sync methods.
  Reference: .NET Performance

---
id: csharp-perf-task-whenany-loop
language: csharp
severity: warning
message: "Performance: Task.WhenAny in loop allocates task per call"
tags:
  - performance
  - task
rule:
  kind: invocation
  pattern: "Task.WhenAny(tasks)"
note: |
  WhenAny allocates task wrapper on each invocation. Loop allocation compounds.
  Fix: Use Channels or completion source for racing tasks.
  Reference: .NET Performance

---
id: csharp-perf-loh-allocation
language: csharp
severity: warning
message: "Performance: Allocations >85KB go to Large Object Heap causing fragmentation"
tags:
  - performance
  - gc
rule:
  kind: object_creation
  pattern: "new byte[85000+]"
note: |
  Large Object Heap causes fragmentation and gen 2 GC. Pool large arrays instead.
  Fix: Use ArrayPool<T> for large allocations.
  Reference: .NET GC docs

---
id: csharp-perf-finalizers
language: csharp
severity: warning
message: "Performance: Finalizers add GC overhead and delay object reclamation"
tags:
  - performance
  - gc
rule:
  kind: method_declaration
  pattern: "~ClassName()"
note: |
  Finalizers add GC overhead and delay object reclamation. Use IDisposable properly.
  Fix: Implement IDisposable; finalizer only as safety net.
  Reference: CA1821

---
id: csharp-perf-gc-collect
language: csharp
severity: warning
message: "Performance: Forcing GC.Collect() usually harms performance"
tags:
  - performance
  - gc
rule:
  kind: invocation
  pattern: "GC.Collect()"
note: |
  Forcing GC collection is rarely correct and usually harms perf. Let GC manage itself.
  Fix: Investigate root cause of memory issues; avoid explicit collection.
  Reference: CA1812

---
id: csharp-perf-pinned-objects
language: csharp
severity: warning
message: "Performance: Long-term pinned objects fragment heap and prevent compaction"
tags:
  - performance
  - gc
rule:
  kind: invocation
  pattern: "GCHandle.Alloc(obj, GCHandleType.Pinned)"
note: |
  Pinning fragments heap and prevents compaction. Unpin ASAP.
  Fix: Unpin quickly; use stackalloc for short-lived pinned data.
  Reference: .NET Performance

---
id: csharp-perf-event-handler-memory-leak
language: csharp
severity: warning
message: "Performance: Event handlers not unsubscribed keep objects alive (memory leak)"
tags:
  - performance
  - gc
rule:
  kind: event_subscription
  pattern: "+= (event handler)"
note: |
  Event subscriptions keep objects alive. Unsubscribed handlers leak memory.
  Fix: Unsubscribe in Dispose; use weak event patterns.
  Reference: CA1806

---
id: csharp-perf-sync-file-io
language: csharp
severity: warning
message: "Performance: Synchronous file I/O blocks thread in async context"
tags:
  - performance
  - io
rule:
  kind: invocation
  pattern: "File.ReadAllBytes(path)"
note: |
  Synchronous I/O blocks thread. In async context, blocks async pipeline.
  Fix: Use File.ReadAllBytesAsync(path) in async code.
  Reference: CA1849

---
id: csharp-perf-unbuffered-stream
language: csharp
severity: warning
message: "Performance: Unbuffered stream with tiny reads causes many system calls"
tags:
  - performance
  - io
rule:
  kind: object_creation
  pattern: "new FileStream(path, ...)"
note: |
  Many small reads are slower than buffered I/O.
  Fix: Use BufferedStream or larger buffer size.
  Reference: .NET I/O Performance

---
id: csharp-perf-multiple-small-writes
language: csharp
severity: warning
message: "Performance: Multiple small writes add system call overhead"
tags:
  - performance
  - io
rule:
  kind: invocation
  pattern: "stream.Write(...)"
note: |
  Each Write is system call. Multiple small writes are inefficient.
  Fix: Buffer writes, use BufferedStream, or BinaryWriter.
  Reference: .NET I/O Performance

---
id: csharp-perf-file-copy-optimization
language: csharp
severity: info
message: "Performance: File.Copy uses OS-level optimizations"
tags:
  - performance
  - io
rule:
  kind: invocation
  pattern: |
    (
      using (var src = ...)
      using (var dst = ...)
      src.CopyTo(dst)
    )
note: |
  Manual copy via streams loses OS optimizations. File.Copy is optimized.
  Fix: Use File.Copy(source, destination) for file copies.
  Reference: .NET I/O Performance

---
id: csharp-perf-httpclient-per-request
language: csharp
severity: warning
message: "Performance: HttpClient per request exhausts port pool"
tags:
  - performance
  - io
rule:
  kind: object_creation
  pattern: "new HttpClient()"
note: |
  Creating HttpClient per request causes socket exhaustion and port starvation.
  Fix: Reuse single HttpClient instance or use IHttpClientFactory.
  Reference: CA2000, HttpClient docs

---
id: csharp-perf-jsonserializer-options-per-call
language: csharp
severity: warning
message: "Performance: JsonSerializerOptions not cached rebuilds metadata each time"
tags:
  - performance
  - json
rule:
  kind: invocation
  pattern: "JsonSerializer.Deserialize<T>(json, new JsonSerializerOptions())"
note: |
  Options rebuilt per call. Cache JsonSerializerOptions as static field.
  Fix: private static readonly JsonSerializerOptions Options = new(...);
  Reference: System.Text.Json Performance

---
id: csharp-perf-newtonsoft-vs-system-json
language: csharp
severity: warning
message: "Performance: Newtonsoft slower than System.Text.Json in hot paths"
tags:
  - performance
  - json
rule:
  kind: invocation
  pattern: "JsonConvert.DeserializeObject<T>(json)"
note: |
  System.Text.Json is generally faster for hot path serialization.
  Fix: Consider migrating from Newtonsoft to System.Text.Json.
  Reference: .NET JSON Performance

---
id: csharp-perf-jsondocument-not-disposed
language: csharp
severity: warning
message: "Performance: JsonDocument not disposed leaks pooled memory"
tags:
  - performance
  - json
rule:
  kind: invocation
  pattern: "JsonDocument.Parse(json)"
note: |
  JsonDocument pools memory; not disposing leaks resources.
  Fix: Use using var doc = JsonDocument.Parse(json);
  Reference: CA2000

---
id: csharp-perf-jsonnode-vs-jsondocument
language: csharp
severity: info
message: "Performance: JsonDocument read-only; use JsonNode for mutation"
tags:
  - performance
  - json
rule:
  kind: invocation
  pattern: "JsonDocument"
note: |
  JsonDocument is read-only (faster). JsonNode allows mutations.
  Fix: Use JsonNode when modifications needed.
  Reference: System.Text.Json docs

---
id: csharp-perf-serialize-to-stream
language: csharp
severity: warning
message: "Performance: Serializing to string allocates intermediate string"
tags:
  - performance
  - json
rule:
  kind: invocation
  pattern: "JsonSerializer.Serialize(obj)"
note: |
  String intermediate allocates. Serialize directly to stream.
  Fix: Use JsonSerializer.Serialize(stream, obj) instead.
  Reference: System.Text.Json Performance

---
id: csharp-perf-gettype-in-loop
language: csharp
severity: warning
message: "Performance: GetType() per item slower than OfType<T>()"
tags:
  - performance
  - reflection
rule:
  kind: invocation
  pattern: "items.Where(x => x.GetType() == typeof(T))"
note: |
  GetType() per item is slow. OfType<T>() is optimized.
  Fix: Use items.OfType<T>() instead.
  Reference: .NET Performance

---
id: csharp-perf-reflection-in-hot-path
language: csharp
severity: warning
message: "Performance: Reflection in hot path should be cached"
tags:
  - performance
  - reflection
rule:
  kind: invocation
  pattern: "type.GetMethod(\"Name\")"
note: |
  Reflection is slow. Hot path calls should cache results.
  Fix: Cache MethodInfo in static field; use compiled delegates.
  Reference: .NET Performance

---
id: csharp-perf-methodinfo-invoke-overhead
language: csharp
severity: warning
message: "Performance: MethodInfo.Invoke has boxing and validation overhead"
tags:
  - performance
  - reflection
rule:
  kind: invocation
  pattern: "methodInfo.Invoke(obj, args)"
note: |
  Invoke has significant overhead. Create delegate for repeated invocation.
  Fix: Use CreateDelegate to compile to delegate, invoke directly.
  Reference: .NET Performance

---
id: csharp-perf-attribute-retrieval
language: csharp
severity: warning
message: "Performance: GetCustomAttributes uses reflection; cache in hot path"
tags:
  - performance
  - reflection
rule:
  kind: invocation
  pattern: "type.GetCustomAttributes()"
note: |
  Attribute retrieval uses reflection. Cache results at startup.
  Fix: Cache attribute values at module/type load time.
  Reference: .NET Performance

---
id: csharp-perf-type-getproperties-loop
language: csharp
severity: warning
message: "Performance: GetProperties allocates array each time"
tags:
  - performance
  - reflection
rule:
  kind: invocation
  pattern: "obj.GetType().GetProperties()"
note: |
  GetProperties allocates new array per call. Cache results.
  Fix: Cache PropertyInfo[] or use source generators.
  Reference: .NET Performance

---
id: csharp-perf-exceptions-for-control-flow
language: csharp
severity: warning
message: "Performance: Exceptions for control flow; use TryParse instead"
tags:
  - performance
  - exceptions
rule:
  kind: try_statement
  pattern: "try { Parse(str); } catch { return default; }"
note: |
  Exceptions are expensive. Should not be used for normal control flow.
  Fix: Use TryParse methods: int.TryParse(str, out var val).
  Reference: CA1031

---
id: csharp-perf-catch-exception-base
language: csharp
severity: warning
message: "Performance: Catching Exception base catches all including critical"
tags:
  - performance
  - exceptions
rule:
  kind: catch_clause
  pattern: "catch (Exception ex)"
note: |
  Catches all exceptions including critical ones. Be specific.
  Fix: Catch specific exception types.
  Reference: CA1031

---
id: csharp-perf-rethrow-losing-stack
language: csharp
severity: warning
message: "Performance: Retherow with 'throw ex' resets stack trace"
tags:
  - performance
  - exceptions
rule:
  kind: throw_statement
  pattern: "throw ex;"
note: |
  Resets stack trace, loses origin information. Use throw; instead.
  Fix: Use throw; to rethrow with original stack trace.
  Reference: CA2200

---
id: csharp-perf-new-exception-on-rethrow
language: csharp
severity: warning
message: "Performance: Creating new exception on rethrow loses original details"
tags:
  - performance
  - exceptions
rule:
  kind: throw_statement
  pattern: 'throw new Exception(ex.Message);'
note: |
  Creates new exception, loses original exception chain and stack.
  Fix: Use throw new Exception("message", ex) with inner exception.
  Reference: CA2201

---
id: csharp-perf-exception-dispatch-info
language: csharp
severity: info
message: "Performance: ExceptionDispatchInfo preserves stack trace across contexts"
tags:
  - performance
  - exceptions
rule:
  kind: invocation
  pattern: "ExceptionDispatchInfo"
note: |
  ExceptionDispatchInfo preserves original stack trace when rethrowing from different context.
  Reference: .NET docs

---
id: csharp-perf-thread-sleep-in-async
language: csharp
severity: warning
message: "Performance: Thread.Sleep in async blocks thread instead of releasing"
tags:
  - performance
  - threading
rule:
  kind: invocation
  pattern: "Thread.Sleep(1000)"
note: |
  Thread.Sleep blocks thread, defeating async benefit. Use Task.Delay.
  Fix: Use await Task.Delay(1000) in async code.
  Reference: CA1849

---
id: csharp-perf-lock-on-this
language: csharp
severity: warning
message: "Performance: Lock on 'this' allows external code to deadlock"
tags:
  - performance
  - threading
rule:
  kind: lock_statement
  pattern: "lock (this)"
note: |
  External code can lock on same reference. Use private lock object.
  Fix: private readonly object _lock = new();
  Reference: CA2002

---
id: csharp-perf-lock-on-type
language: csharp
severity: warning
message: "Performance: Lock on Type object shared across AppDomains"
tags:
  - performance
  - threading
rule:
  kind: lock_statement
  pattern: "lock (typeof(MyClass))"
note: |
  Type objects shared across AppDomains. Cross-AppDomain deadlock risk.
  Fix: Use private static lock object.
  Reference: CA2002

---
id: csharp-perf-thread-creation-loop
language: csharp
severity: warning
message: "Performance: Creating threads in loop is expensive"
tags:
  - performance
  - threading
rule:
  kind: object_creation
  pattern: "new Thread(() => ...).Start()"
note: |
  Thread creation is expensive. Use ThreadPool or Task.Run.
  Fix: Use Task.Run or ThreadPool instead of explicit threads.
  Reference: .NET Threading

---
id: csharp-perf-spin-wait-unlimited
language: csharp
severity: warning
message: "Performance: Unlimited spin wait burns CPU and wastes power"
tags:
  - performance
  - threading
rule:
  kind: while_statement
  pattern: "while (!condition) { }"
note: |
  Hot spin loop burns CPU. Use proper synchronization or SpinWait struct.
  Fix: Use SpinWait struct or proper synchronization primitives.
  Reference: .NET Threading

---
id: csharp-perf-readerwriterlock-old
language: csharp
severity: warning
message: "Performance: ReaderWriterLock has poor performance; use ReaderWriterLockSlim"
tags:
  - performance
  - threading
rule:
  kind: object_creation
  pattern: "new ReaderWriterLock()"
note: |
  ReaderWriterLock has poor scalability. ReaderWriterLockSlim is faster.
  Fix: Use ReaderWriterLockSlim instead.
  Reference: CA2002

---
id: csharp-perf-volatile-overuse
language: csharp
severity: info
message: "Performance: Volatile prevents optimizations; use lock/Interlocked if needed"
tags:
  - performance
  - threading
rule:
  kind: field_declaration
  pattern: "volatile"
note: |
  Volatile prevents compiler optimizations. Use lock or Interlocked for thread safety.
  Fix: Use lock or Interlocked for thread-safe access.
  Reference: .NET Threading

---
id: csharp-perf-lazy-unnecessary-thread-safety
language: csharp
severity: info
message: "Performance: Lazy<T> thread safety not needed in single-threaded code"
tags:
  - performance
  - lazy
rule:
  kind: object_creation
  pattern: "new Lazy<T>(() => new T())"
note: |
  Default mode uses lock for thread safety. Single-threaded code doesn't need it.
  Fix: Use LazyThreadSafetyMode.None if guaranteed single-threaded.
  Reference: .NET Performance

---
id: csharp-perf-double-checked-locking-incorrect
language: csharp
severity: warning
message: "Performance: Incorrect double-checked locking pattern (missing inner check/volatile)"
tags:
  - performance
  - lazy
rule:
  kind: if_statement
  pattern: "if (_instance == null) { lock(_lock) { _instance = new T(); } }"
note: |
  Missing inner null check and volatile keyword. Use Lazy<T> instead.
  Fix: Use Lazy<T> or correct double-checked locking with volatile.
  Reference: .NET Threading

---
id: csharp-perf-lazy-initializer
language: csharp
severity: info
message: "Performance: LazyInitializer.EnsureInitialized optimized for lazy init pattern"
tags:
  - performance
  - lazy
rule:
  kind: variable_declaration
  pattern: "manual lazy initialization"
note: |
  LazyInitializer is optimized for lazy init pattern with null check.
  Fix: Use LazyInitializer.EnsureInitialized(ref field, () => new T()).
  Reference: .NET Performance

---
id: csharp-perf-n-plus-one-query
language: csharp
severity: warning
message: "Performance: Loading related entities in loop causes N+1 query problem"
tags:
  - performance
  - database
rule:
  kind: for_statement
  pattern: "foreach (var entity in entities)"
note: |
  Loading related entities in loop generates N additional queries.
  Fix: Use Include/ThenInclude for eager loading.
  Reference: EF Core Performance

---
id: csharp-perf-missing-asnotracking
language: csharp
severity: warning
message: "Performance: EF change tracking overhead on read-only queries"
tags:
  - performance
  - database
rule:
  kind: invocation
  pattern: "dbContext.Set<T>()"
note: |
  Change tracking adds overhead for read-only scenarios.
  Fix: Use .AsNoTracking() for read-only queries.
  Reference: EF Core Performance

---
id: csharp-perf-complex-linq-to-sql
language: csharp
severity: warning
message: "Performance: Complex LINQ may translate to inefficient SQL"
tags:
  - performance
  - database
rule:
  kind: invocation
  pattern: ".Where(...).Select(...)"
note: |
  ORM may generate suboptimal SQL. Profile generated queries.
  Fix: Use FromSqlRaw for complex queries; profile query plans.
  Reference: EF Core Performance

---
id: csharp-perf-connection-not-pooled
language: csharp
severity: warning
message: "Performance: Opening connections without pooling is expensive"
tags:
  - performance
  - database
rule:
  kind: object_creation
  pattern: "new SqlConnection()"
note: |
  Connection creation is expensive. Connection pooling is default/recommended.
  Fix: Ensure connection string enables pooling (default true).
  Reference: ADO.NET Performance

---
id: csharp-perf-select-all-columns-single-use
language: csharp
severity: warning
message: "Performance: Selecting all columns then single property retrieves unnecessary data"
tags:
  - performance
  - database
rule:
  kind: invocation
  pattern: ".ToList().Select(x => x.SingleProp)"
note: |
  Retrieves all columns from database. Project first.
  Fix: Use .Select(x => x.SingleProp).ToList() to select before materializing.
  Reference: EF Core Performance

---
id: csharp-perf-datetime-now-repeated
language: csharp
severity: info
message: "Performance: Multiple DateTime.Now calls are system calls; cache if used repeatedly"
tags:
  - performance
  - miscellaneous
rule:
  kind: property_access
  pattern: "DateTime.Now"
note: |
  Each call is system call and time may differ between calls.
  Fix: Capture once: var now = DateTime.UtcNow; for repeated use.
  Reference: .NET Performance

---
id: csharp-perf-enum-tostring-hot-path
language: csharp
severity: warning
message: "Performance: Enum.ToString uses reflection; cache in hot path"
tags:
  - performance
  - miscellaneous
rule:
  kind: invocation
  pattern: "enumValue.ToString()"
note: |
  Enum.ToString uses reflection. Hot paths should cache string values.
  Fix: Cache enum string values or use source generator.
  Reference: .NET Performance

---
id: csharp-perf-guid-newguid-comparison
language: csharp
severity: info
message: "Performance: Creating new Guid just to compare is suspicious"
tags:
  - performance
  - miscellaneous
rule:
  kind: comparison
  pattern: "Guid.NewGuid() == Guid.Empty"
note: |
  Always creates new Guid. Review logic - probably backwards.
  Reference: CA2017

---
id: csharp-perf-assembly-getexecuting-loop
language: csharp
severity: warning
message: "Performance: Assembly.GetExecutingAssembly is reflection call; cache result"
tags:
  - performance
  - miscellaneous
rule:
  kind: invocation
  pattern: "Assembly.GetExecutingAssembly()"
note: |
  Reflection call on each invocation. Cache in static field.
  Fix: private static readonly Assembly ExecutingAssembly = Assembly.GetExecutingAssembly();
  Reference: .NET Performance

---
id: csharp-perf-process-getcurrent-repeated
language: csharp
severity: info
message: "Performance: Process.GetCurrentProcess creates new wrapper each time"
tags:
  - performance
  - miscellaneous
rule:
  kind: invocation
  pattern: "Process.GetCurrentProcess()"
note: |
  Creates new Process wrapper each call. Cache if used repeatedly.
  Fix: Cache the Process object in static field.
  Reference: .NET Performance

---
id: csharp-perf-path-combine-repeated
language: csharp
severity: info
message: "Performance: Repeated Path.Combine of same elements should build once"
tags:
  - performance
  - miscellaneous
rule:
  kind: invocation
  pattern: "Path.Combine(basePath, folder, subfolder)"
note: |
  Repeated string operations. Build base path once before loop.
  Fix: Build base path once outside loop.
  Reference: .NET Performance

---
id: csharp-perf-random-instance-per-call
language: csharp
severity: warning
message: "Performance: Creating Random instance per call is poor seeding"
tags:
  - performance
  - miscellaneous
rule:
  kind: object_creation
  pattern: "new Random().Next()"
note: |
  Creates new Random each call with poor randomness. Use shared instance.
  Fix: Use Random.Shared (.NET 6+) or shared instance.
  Reference: CA2017

---
id: csharp-perf-environment-variable-repeated
language: csharp
severity: info
message: "Performance: Reading same environment variable multiple times"
tags:
  - performance
  - miscellaneous
rule:
  kind: invocation
  pattern: "Environment.GetEnvironmentVariable(name)"
note: |
  System call each time. Cache if not expected to change.
  Fix: Cache value at startup if known not to change.
  Reference: .NET Performance

---
id: csharp-perf-configuration-getsection-hot
language: csharp
severity: warning
message: "Performance: IConfiguration.GetSection in hot path should bind at startup"
tags:
  - performance
  - miscellaneous
rule:
  kind: invocation
  pattern: 'Configuration.GetSection("x")'
note: |
  Configuration access has overhead in hot paths.
  Fix: Bind to options class at startup, inject bound options.
  Reference: ASP.NET Performance

---
id: csharp-perf-log-level-string-formatting
language: csharp
severity: warning
message: "Performance: String formatting happens even if log level disabled"
tags:
  - performance
  - miscellaneous
rule:
  kind: invocation
  pattern: "_logger.Log(LogLevel.Debug, ...)"
note: |
  String formatting happens before level check. Check level first.
  Fix: Use if (_logger.IsEnabled(LogLevel.Debug)) or use structured logging.
  Reference: .NET Logging Performance
---
id: scala-perf-list-random-access
language: scala
severity: warning
message: "Performance: List has O(n) random access; use Vector or Array for indexed access"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $list($INDEX)
    $list.apply($INDEX)
note: |
  List is a linked list with O(n) random access complexity. Use Vector which provides O(log32 n)
  access or Array for O(1) access. Example: val items: Vector[Int] = Vector(1, 2, 3); items(5)

---
id: scala-perf-list-append-mix
language: scala
severity: warning
message: "Performance: List append is O(n); use Vector for mixed prepend/append operations"
tags:
  - performance
  - collections
rule:
  kind: binary_operator
  pattern: |
    $list :+ $element
    $list = $list :+ $element
note: |
  Appending to List is O(n) because List is optimized for prepend only. Use Vector for O(log32 n)
  append or accumulate in reverse then reverse once. Example:
  val vector = vector :+ element // O(log32 n)

---
id: scala-perf-list-length-in-loop
language: scala
severity: error
message: "Performance: Calling List.length in loop condition is O(n^2); cache the length"
tags:
  - performance
  - collections
  - loops
rule:
  kind: loop
  pattern: |
    for (i <- 0 until $list.length)
    while (i < $list.length)
    if ($list.length > $N)
note: |
  List.length is O(n) because it traverses the list. Calling it repeatedly in loop conditions
  creates O(n^2) complexity. Cache the length before the loop: val len = list.length

---
id: scala-perf-array-dynamic-growth
language: scala
severity: warning
message: "Performance: Array resize requires full copy; use ArrayBuffer for dynamic growth"
tags:
  - performance
  - collections
rule:
  kind: assignment
  pattern: |
    var $array = Array[$T]($INIT)
    $array = $array :+ $ELEMENT
    $array = Array.concat($array, $OTHER)
note: |
  Array resize requires copying all elements. Use ArrayBuffer which provides amortized O(1) append.
  Example: val buffer = ArrayBuffer[Int](); buffer += element; buffer.toArray when done

---
id: scala-perf-treemap-when-order-not-needed
language: scala
severity: info
message: "Performance: TreeMap has O(log n) operations; use HashMap for O(1) if ordering not required"
tags:
  - performance
  - collections
rule:
  kind: new_expression
  pattern: |
    TreeMap[$K, $V]()
    $map.get($KEY)
    $map.updated($KEY, $VALUE)
note: |
  TreeMap operations are O(log n). If ordering is not required, HashMap provides O(1) average case.
  Example: val map = HashMap[String, Int](); map.get(key) // O(1) average

---
id: scala-perf-immutable-in-tight-loops
language: scala
severity: warning
message: "Performance: Building immutable collections in loops creates many intermediate objects"
tags:
  - performance
  - collections
  - loops
rule:
  kind: assignment
  pattern: |
    var $result = List.empty[$T]
    for ($ELEM <- $COLLECTION) {
      $result = $result :+ $ELEM
    }
note: |
  Each append creates a new List. Use ListBuffer for mutable accumulation, then convert:
  val buffer = ListBuffer.empty[T]; for (elem <- collection) buffer += elem; buffer.toList

---
id: scala-perf-list-contains-vs-set
language: scala
severity: warning
message: "Performance: List.contains is O(n); use Set.contains which is O(1) average"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $list.contains($ELEMENT)
    $list.exists(_ == $ELEMENT)
note: |
  List.contains requires linear search. Convert to Set for constant-time lookup:
  val set = elements.toSet; set.contains(element) // O(1) average

---
id: scala-perf-range-vs-list
language: scala
severity: info
message: "Performance: Range is lazy and memory-efficient; avoid converting to List for large ranges"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    List.range(0, $N)
    ($N to $M).toList
note: |
  Range is lazy and requires O(1) memory. Converting to List allocates the full sequence.
  Keep as Range when possible: val range = 0 until 1000000; range.foreach { ... }

---
id: scala-perf-chained-filter-map
language: scala
severity: info
message: "Performance: Chained filter and map create intermediate collections; use collect"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $collection.filter($PRED).map($FUNC)
note: |
  Each operation creates intermediate collection. Use collect for combined operation:
  collection.collect { case x if predicate(x) => transform(x) }
  Or use view for lazy evaluation: collection.view.filter(pred).map(func).toList

---
id: scala-perf-size-check-before-isempty
language: scala
severity: info
message: "Performance: size/length can be O(n); use isEmpty which is O(1)"
tags:
  - performance
  - collections
rule:
  kind: comparison_operator
  pattern: |
    if ($collection.size == 0)
    if ($collection.size > 0)
    if ($collection.length == 0)
note: |
  size/length can be O(n) for some collections. isEmpty is guaranteed O(1).
  Use: if (collection.isEmpty) or if (collection.nonEmpty)

---
id: scala-perf-head-last-on-empty
language: scala
severity: warning
message: "Performance: head/last throw exceptions and may be O(n); use headOption/lastOption"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $collection.head
    $collection.last
note: |
  head/last throw NoSuchElementException on empty, and last is O(n) for List.
  Use: collection.headOption or collection.lastOption; headOption.getOrElse(default)

---
id: scala-perf-repeated-collection-traversal
language: scala
severity: warning
message: "Performance: Multiple traversals when single fold could compute all values"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    val $sum = $list.sum
    val $count = $list.size
    val $avg = $sum / $count
note: |
  Multiple traversals are inefficient. Use foldLeft to compute multiple values in single pass:
  val (sum, count) = list.foldLeft((0, 0)) { case ((s, c), x) => (s + x, c + 1) }

---
id: scala-perf-groupby-mapvalues
language: scala
severity: info
message: "Performance: mapValues creates lazy view; materialize with map or toMap if needed"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $collection.groupBy($KEY).mapValues($FUNC)
note: |
  mapValues in Scala 2.13+ creates a lazy view that recomputes on each access.
  For strict evaluation: collection.groupBy(keyFunc).map { case (k, v) => k -> transform(v) }
  Or explicitly materialize: .mapValues(transform).toMap

---
id: scala-perf-foldright-on-list
language: scala
severity: warning
message: "Performance: foldRight on List is not tail-recursive; can cause stack overflow"
tags:
  - performance
  - functional
rule:
  kind: method_invocation
  pattern: |
    $list.foldRight($ZERO)($OP)
    $list.:\($ZERO)($OP)
note: |
  foldRight on List requires O(n) stack space. Use foldLeft which is tail-recursive:
  list.foldLeft(zero)(op) or list.reverse.foldLeft(zero)((acc, x) => op(x, acc))

---
id: scala-perf-distinct-on-large-collections
language: scala
severity: warning
message: "Performance: distinct uses Set internally but traverses entire collection"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $largeCollection.distinct
note: |
  distinct traverses entire collection. If order doesn't matter: largeCollection.toSet.toList
  For ordered distinct: largeCollection.to(LazyList).distinct

---
id: scala-perf-sortby-expensive-key
language: scala
severity: warning
message: "Performance: sortBy calls key function O(n log n) times; use Schwartzian transform"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $collection.sortBy($EXPENSIVE_FUNC)
note: |
  Key function is called during each comparison. Use Schwartzian transform:
  collection.map(x => (x, expensiveKeyFunction(x))).sortBy(_._2).map(_._1)

---
id: scala-perf-flatten-after-map
language: scala
severity: info
message: "Performance: map then flatten creates intermediate collection; use flatMap"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $collection.map($FUNC).flatten
note: |
  Creates intermediate collection. Use flatMap instead:
  collection.flatMap(func)

---
id: scala-perf-zip-with-index-recreation
language: scala
severity: info
message: "Performance: Manually zipping with index is inefficient; use zipWithIndex"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $collection.zip(0 until $collection.length)
    $collection.zip($collection.indices)
note: |
  zipWithIndex is optimized for this pattern: collection.zipWithIndex

---
id: scala-perf-missing-view-for-chains
language: scala
severity: warning
message: "Performance: Chained operations create intermediate collections; use view for single-pass"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $collection.filter($P1).filter($P2).map($F1).map($F2)
note: |
  Each operation creates intermediate collection. Use view for lazy evaluation:
  collection.view.filter(p1).filter(p2).map(f1).map(f2).toList

---
id: scala-perf-view-not-materialized
language: scala
severity: info
message: "Performance: View recomputes on each access; materialize if reused"
tags:
  - performance
  - collections
rule:
  kind: assignment
  pattern: |
    val $result = $collection.view.map($F)
note: |
  Views recompute lazily on each access. If result is used multiple times, materialize:
  val result = collection.view.map(f).toVector

---
id: scala-perf-take-drop-before-expensive
language: scala
severity: warning
message: "Performance: Full collection processed before taking subset; use view or LazyList"
tags:
  - performance
  - collections
  - lazy-evaluation
rule:
  kind: method_invocation
  pattern: |
    $collection.map($EXPENSIVE).take(10)
    $collection.filter($EXPENSIVE).take(5)
note: |
  Full collection is processed before subset is taken. Use lazy evaluation:
  collection.view.map(expensive).take(10).toList or collection.to(LazyList).map(expensive).take(10).toList

---
id: scala-perf-find-after-filter
language: scala
severity: info
message: "Performance: filter processes entire collection when only first match needed; use find"
tags:
  - performance
  - collections
rule:
  kind: method_invocation
  pattern: |
    $collection.filter($PRED).headOption
    $collection.filter($PRED).head
note: |
  filter processes entire collection. Use find to stop at first match:
  collection.find(pred)

---
id: scala-perf-non-tail-recursive
language: scala
severity: error
message: "Performance: Non-tail recursive function can cause stack overflow on large inputs"
tags:
  - performance
  - recursion
rule:
  kind: function_definition
  pattern: |
    def $NAME($PARAMS) {
      if ($COND) $BASE
      else $REC * $FUNC($ARGS)
    }
note: |
  Non-tail recursive functions accumulate stack frames. Use tail recursion with accumulator:
  @tailrec def factorial(n: Int, acc: Int = 1): Int = {
    if (n <= 1) acc else factorial(n - 1, n * acc)
  }

---
id: scala-perf-missing-tailrec-annotation
language: scala
severity: info
message: "Performance: Intended tail recursive function should have @tailrec annotation"
tags:
  - performance
  - recursion
rule:
  kind: function_definition
  pattern: |
    def $NAME($PARAMS) = {
      $FUNC($ARGS)
    }
note: |
  Without @tailrec annotation, refactoring may accidentally break tail recursion.
  Use annotation so compiler enforces tail recursion: @tailrec def recursiveFunc(params): T

---
id: scala-perf-mutual-recursion
language: scala
severity: warning
message: "Performance: Mutual recursion cannot be optimized by @tailrec; can cause stack overflow"
tags:
  - performance
  - recursion
rule:
  kind: function_definition
  pattern: |
    def $FUNC1($P1) = if ($C) $B else $FUNC2($A)
    def $FUNC2($P2) = if ($C) $B else $FUNC1($A)
note: |
  @tailrec cannot optimize mutual recursion. Use trampolining:
  import scala.util.control.TailCalls._
  def isEven(n: Int): TailRec[Boolean] =
    if (n == 0) done(true) else tailcall(isOdd(n - 1))

---
id: scala-perf-expensive-default-parameter
language: scala
severity: warning
message: "Performance: Expensive default parameter computation should be lazy"
tags:
  - performance
  - parameters
rule:
  kind: parameter_definition
  pattern: |
    def $FUNC($PARAM: $TYPE = $EXPENSIVE_CALL)
note: |
  Default parameters are evaluated when method is called, even if argument provided.
  Use Option or by-name: def process(data: Option[List[Int]] = None): Result = {
    val actualData = data.getOrElse(expensiveComputation())
  }

---
id: scala-perf-byname-not-short-circuit
language: scala
severity: info
message: "Performance: Message parameter should be by-name to enable short-circuit evaluation"
tags:
  - performance
  - parameters
  - lazy-evaluation
rule:
  kind: parameter_definition
  pattern: |
    def logIfEnabled($ENABLED: Boolean, $MESSAGE: String)
note: |
  Message is evaluated even when logging disabled. Use by-name parameter:
  def logIfEnabled(enabled: Boolean, message: => String) = {
    if (enabled) log(message)
  }

---
id: scala-perf-repeated-byname-evaluation
language: scala
severity: warning
message: "Performance: By-name parameter evaluated multiple times; cache the value"
tags:
  - performance
  - parameters
rule:
  kind: parameter_usage
  pattern: |
    def compute($VALUE: => Int) = {
      $VALUE + $VALUE + $VALUE
    }
note: |
  By-name parameters are evaluated each time referenced. Cache once:
  def compute(value: => Int) = {
    val cached = value
    cached + cached + cached
  }

---
id: scala-perf-lazy-val-in-hot-path
language: scala
severity: warning
message: "Performance: lazy val in hot path has synchronization overhead; use simple val"
tags:
  - performance
  - lazy-initialization
rule:
  kind: lazy_variable
  pattern: |
    def hotMethod() = {
      lazy val $X = $COMPUTE
      $X + 1
    }
note: |
  lazy val uses double-checked locking with synchronization. If needed only once per call:
  def hotMethod() = { val x = computeX(); x + 1 }
  For shared access, use class-level lazy val.

---
id: scala-perf-lazy-val-many-in-object
language: scala
severity: info
message: "Performance: Many lazy vals add initialization overhead; group related values"
tags:
  - performance
  - lazy-initialization
rule:
  kind: lazy_variable
  pattern: |
    object $CONFIG {
      lazy val $V1 = $LOAD1
      lazy val $V2 = $LOAD2
    }
note: |
  Each lazy val adds synchronization overhead. Group related values in single lazy initialization:
  object Config {
    private lazy val config = loadAll()
    def value1 = config.value1
    def value2 = config.value2
  }

---
id: scala-perf-lazy-val-deadlock
language: scala
severity: error
message: "Performance: Circular lazy val dependencies can cause deadlock"
tags:
  - performance
  - lazy-initialization
  - concurrency
rule:
  kind: lazy_variable
  pattern: |
    lazy val $X = $OBJ.$Y + 1
note: |
  Circular lazy dependencies cause deadlock. Break circular dependency with explicit initialization:
  object Config {
    private var _x: Int = _
    private var _y: Int = _
    def initialize(): Unit = { _x = 1; _y = _x + 1 }
  }

---
id: scala-perf-match-on-string
language: scala
severity: info
message: "Performance: String matching; consider sealed traits or enums for better type safety"
tags:
  - performance
  - pattern-matching
rule:
  kind: pattern_match
  pattern: |
    $INPUT match {
      case "value1" => ...
      case "value2" => ...
    }
note: |
  String matching uses equals comparison. For better performance and type safety, use sealed traits:
  sealed trait InputType; case object Value1 extends InputType
  def parse(s: String): Option[InputType] = s match { case "value1" => Some(Value1) ... }

---
id: scala-perf-expensive-extractor
language: scala
severity: warning
message: "Performance: Custom extractor in hot path is called for each match attempt"
tags:
  - performance
  - pattern-matching
rule:
  kind: pattern_match
  pattern: |
    strings.collect {
      case $EXPENSIVE_EXTRACTOR($VALUE) => $VALUE
    }
note: |
  Extractors are invoked for each match attempt. Pre-compute or cache results:
  strings.flatMap(s => expensiveOperation(s))
  Or: val cache = mutable.Map.empty[String, Option[Int]];
      def cachedExtract(s) = cache.getOrElseUpdate(s, expensiveOperation(s))

---
id: scala-perf-pattern-guard-with-sideeffect
language: scala
severity: warning
message: "Performance: Guard in pattern match may have side effects evaluated multiple times"
tags:
  - performance
  - pattern-matching
rule:
  kind: pattern_match
  pattern: |
    $X match {
      case $Y if $SIDEEFFECT($Y) => ...
    }
note: |
  Guards may be evaluated multiple times. Move side effect outside match:
  val checkResult = sideEffectingCheck(x)
  x match { case y if checkResult => ... }

---
id: scala-perf-wildcard-before-specific
language: scala
severity: warning
message: "Performance: Wildcard pattern before specific cases makes latter unreachable"
tags:
  - performance
  - pattern-matching
rule:
  kind: pattern_match
  pattern: |
    $X match {
      case _ => $DEFAULT
      case $SPECIFIC => ...
    }
note: |
  Wildcard matches everything, making subsequent cases unreachable. Reorder:
  x match {
    case Specific(y) => ...
    case _ => default
  }

---
id: scala-perf-implicit-search-in-loop
language: scala
severity: warning
message: "Performance: Implicit search in loop has overhead; move outside loop"
tags:
  - performance
  - implicit-resolution
rule:
  kind: loop
  pattern: |
    for ($ITEM <- $ITEMS) {
      implicit val $CTX = createContext()
      process($ITEM)
    }
note: |
  Implicit resolution overhead occurs each iteration. Move outside:
  implicit val ctx = createContext()
  for (item <- items) { process(item) }

---
id: scala-perf-complex-implicit-chain
language: scala
severity: info
message: "Performance: Deep implicit conversion chain slows compilation and adds runtime overhead"
tags:
  - performance
  - implicit-resolution
rule:
  kind: implicit_definition
  pattern: |
    implicit def convert1[A](a: A): B = ...
    implicit def convert2[B](b: B): C = ...
    implicit def convert3[C](c: C): D = ...
note: |
  Deep chains slow compilation and add runtime cost. Use direct conversion:
  implicit def convertDirect[A](a: A): D = ... or explicit type classes

---
id: scala-perf-implicit-class-allocation
language: scala
severity: warning
message: "Performance: Implicit class creates wrapper object; use AnyVal to avoid allocation"
tags:
  - performance
  - implicit-resolution
rule:
  kind: class_definition
  pattern: |
    implicit class $RICH($VAL: $T) {
      def $METHOD = ...
    }
note: |
  Implicit class creates wrapper object for each operation in hot paths. Extend AnyVal:
  implicit class RichInt(val i: Int) extends AnyVal {
    def square = i * i
  }

---
id: scala-perf-blocking-in-future
language: scala
severity: error
message: "Performance: Blocking in Future exhausts thread pool; use flatMap composition instead"
tags:
  - performance
  - futures
  - concurrency
rule:
  kind: method_invocation
  pattern: |
    Future {
      Await.result($OTHER_FUTURE, Duration.Inf)
    }
    Thread.sleep($MS)
    synchronized { $BLOCK }
note: |
  Blocking exhausts thread pool. Use flatMap composition:
  future1.flatMap(r1 => future2.map(r2 => combine(r1, r2)))
  or for-comprehension: for { r1 <- future1; r2 <- future2 } yield combine(r1, r2)

---
id: scala-perf-global-execution-context-for-blocking
language: scala
severity: info
message: "Performance: Global ExecutionContext sized for CPU-bound work; use separate pool for blocking I/O"
tags:
  - performance
  - futures
  - concurrency
rule:
  kind: import_statement
  pattern: |
    import scala.concurrent.ExecutionContext.Implicits.global
note: |
  Global EC is sized for CPU-bound operations. For blocking I/O, use separate pool:
  implicit val blockingEC = ExecutionContext.fromExecutor(Executors.newCachedThreadPool())
  Future { blocking { /* I/O */ } }(blockingEC)

---
id: scala-perf-sequential-future-creation
language: scala
severity: warning
message: "Performance: Futures in for-comprehension execute sequentially; start all first"
tags:
  - performance
  - futures
rule:
  kind: for_comprehension
  pattern: |
    for {
      $A <- Future($COMPUTE_A)
      $B <- Future($COMPUTE_B)
      $C <- Future($COMPUTE_C)
    } yield ($A, $B, $C)
note: |
  Futures in for-comprehension are sequential (each waits for previous). Start all first:
  val futureA = Future(computeA()); val futureB = Future(computeB()); val futureC = Future(computeC())
  for { a <- futureA; b <- futureB; c <- futureC } yield (a, b, c)

---
id: scala-perf-future-sequence-large-collection
language: scala
severity: warning
message: "Performance: Future.sequence on large collection creates all futures immediately; use batching"
tags:
  - performance
  - futures
rule:
  kind: method_invocation
  pattern: |
    Future.sequence($LARGE_LIST.map($F))
note: |
  All futures created immediately, potentially overwhelming resources. Use batching:
  items.grouped(batchSize).foldLeft(Future.successful(List.empty[B])) { (acc, batch) =>
    for { results <- acc; batchResults <- Future.sequence(batch.map(f)) }
    yield results ++ batchResults
  }

---
id: scala-perf-await-result-in-production
language: scala
severity: error
message: "Performance: Await.result blocks thread, defeating async benefits; return Future up call stack"
tags:
  - performance
  - futures
rule:
  kind: method_invocation
  pattern: |
    Await.result($FUTURE, $DURATION)
    Await.ready($FUTURE, $DURATION)
note: |
  Await blocks the calling thread. Return Future up the call stack instead:
  def process(): Future[Result] = future.map(transform)
  Block only at application boundary in main().

---
id: scala-perf-no-recovery-on-future
language: scala
severity: info
message: "Performance: Futures without recovery propagate exceptions silently; add error handling"
tags:
  - performance
  - futures
rule:
  kind: method_invocation
  pattern: |
    Future { riskyOperation() }
note: |
  Failed futures without recovery may propagate exceptions silently. Add recovery:
  Future { riskyOperation() }.recover { case e: SpecificException => defaultValue }

---
id: scala-perf-blocking-in-actor-receive
language: scala
severity: error
message: "Performance: Blocking in actor receive blocks entire dispatcher thread; use pipe pattern"
tags:
  - performance
  - akka
  - concurrency
rule:
  kind: method_definition
  pattern: |
    def receive = {
      case $MSG =>
        Await.result($FUTURE, Duration.Inf)
        Thread.sleep($MS)
    }
note: |
  Blocking in receive blocks entire dispatcher thread. Use pipe pattern:
  import akka.pattern.pipe
  def receive = { case msg => future.pipeTo(sender()) }

---
id: scala-perf-large-actor-mailbox
language: scala
severity: warning
message: "Performance: Unbounded actor mailbox can cause OOM; use bounded mailbox or backpressure"
tags:
  - performance
  - akka
rule:
  kind: configuration
  pattern: |
    actor ! $MESSAGE
note: |
  Unbounded mailboxes can cause OOM under load. Use bounded mailbox:
  akka.actor.mailbox.bounded-mailbox { mailbox-type = "akka.dispatch.BoundedMailbox"; mailbox-capacity = 1000 }
  Or use Akka Streams: Source.queue[Message](100, OverflowStrategy.backpressure)

---
id: scala-perf-actor-per-request
language: scala
severity: warning
message: "Performance: Creating actor per request is expensive; use router or Akka Streams"
tags:
  - performance
  - akka
rule:
  kind: method_definition
  pattern: |
    def handleRequest($REQ: Request) = {
      context.actorOf(Props[RequestHandler])
    }
note: |
  Actors have creation overhead. Use router with fixed pool:
  val router = context.actorOf(FromConfig.props(Props[RequestHandler]), "requestRouter")
  Or use Akka Streams for request processing.

---
id: scala-perf-ask-pattern-timeout
language: scala
severity: warning
message: "Performance: Ask pattern creates temporary actor; missing timeout leaks resources"
tags:
  - performance
  - akka
rule:
  kind: method_invocation
  pattern: |
    import akka.pattern.ask
    $ACTOR ? $MESSAGE
note: |
  Ask creates temporary actor; without timeout, resources leak. Add explicit timeout:
  implicit val timeout: Timeout = 5.seconds
  (actor ? message).mapTo[Response]

---
id: scala-perf-stash-without-unstash
language: scala
severity: warning
message: "Performance: Stashed messages without unstashAll() cause memory leak"
tags:
  - performance
  - akka
rule:
  kind: method_definition
  pattern: |
    def receive = {
      case Initializing => stash()
    }
note: |
  Stashed messages never unstashed cause memory leak. Unstash when state changes:
  def receive = {
    case Initializing => stash()
    case Initialized => unstashAll(); context.become(ready)
  }

---
id: scala-perf-tuple-creation-in-hot-path
language: scala
severity: info
message: "Performance: Tuple allocation in tight loops can be significant; consider alternatives"
tags:
  - performance
  - memory-allocation
rule:
  kind: tuple_expression
  pattern: |
    for (i <- 0 until $N) {
      val $TUPLE = ($A, $B)
    }
note: |
  Tuple allocation can be significant in tight loops. Use value classes:
  case class IntPair(a: Int, b: Int) extends AnyVal
  Or avoid tuple entirely with local variables.

---
id: scala-perf-varargs-array-creation
language: scala
severity: info
message: "Performance: Varargs creates new array for each call; use overloaded methods in hot paths"
tags:
  - performance
  - memory-allocation
rule:
  kind: method_definition
  pattern: |
    def method($ARGS: Any*)
note: |
  Varargs creates array each call. For hot paths, use overloaded methods:
  def method(a: T): R; def method(a: T, b: T): R; def method(a: T, b: T, c: T): R
  def method(args: T*): R  // Fallback

---
id: scala-perf-string-interpolation-allocation
language: scala
severity: info
message: "Performance: String interpolation allocates even if unused; check log level first"
tags:
  - performance
  - memory-allocation
rule:
  kind: string_interpolation
  pattern: |
    for ($I <- 0 until $N) {
      s"Value: $I"
    }
note: |
  String interpolation allocates strings even if unused. Check level:
  if (logger.isDebugEnabled) { logger.debug(s"Value: $i") }
  Use StringBuilder for accumulation: val sb = new StringBuilder; sb.append(...).append(...).toString

---
id: scala-perf-closure-allocation-in-loop
language: scala
severity: warning
message: "Performance: Closure capturing loop variables allocates objects each iteration"
tags:
  - performance
  - memory-allocation
rule:
  kind: lambda_expression
  pattern: |
    for ($I <- 0 until $N) {
      $LIST.map(x => x + $I)
    }
note: |
  Closures capturing loop variables allocate objects. Hoist outside if possible:
  val f = (x: Int, offset: Int) => x + offset
  for (i <- 0 until n) { list.map(f(_, i)) }

---
id: scala-perf-option-allocation
language: scala
severity: info
message: "Performance: Some allocates an object; for primitive-heavy code consider specialized alternatives"
tags:
  - performance
  - memory-allocation
rule:
  kind: method_invocation
  pattern: |
    def maybeValue: Option[Int] = Some($VALUE)
note: |
  Some allocates object. For primitive-heavy code, consider:
  - Sentinel values (-1 for missing)
  - @specialized annotation
  - Specialized libraries like Spire's Opt

---
id: scala-perf-method-too-large-for-inlining
language: scala
severity: info
message: "Performance: Method too large for JIT inlining (>35 bytecode instructions); split into smaller methods"
tags:
  - performance
  - jvm-optimization
rule:
  kind: method_definition
  pattern: |
    def largeMethod() = {
      /* > 35 bytecode instructions */
    }
note: |
  JIT won't inline methods larger than threshold. Split into smaller methods:
  def largeMethod() = { part1(); part2(); part3() }

---
id: scala-perf-megamorphic-call-site
language: scala
severity: warning
message: "Performance: Call site with >2 types disables JIT optimizations; group by type"
tags:
  - performance
  - jvm-optimization
rule:
  kind: method_invocation
  pattern: |
    trait Animal { def speak(): String }
    $animals.foreach(_.speak())
note: |
  Call sites with >2 types become megamorphic, disabling JIT. Group by type:
  val dogs = animals.collect { case d: Dog => d }; val cats = animals.collect { case c: Cat => c }
  dogs.foreach(_.speak()); cats.foreach(_.speak())

---
id: scala-perf-synchronized-collection-access
language: scala
severity: warning
message: "Performance: Synchronized collections block all threads; use concurrent collections"
tags:
  - performance
  - concurrency
rule:
  kind: method_invocation
  pattern: |
    Collections.synchronizedList(new ArrayList[$T]())
    synchronized { $LIST.forEach(...) }
note: |
  Synchronized collections block all threads. Use concurrent collections:
  val map = new ConcurrentHashMap[K, V]() or val map = TrieMap.empty[K, V]

---
id: scala-perf-reflection-in-hot-path
language: scala
severity: error
message: "Performance: Reflection is slow and can't be JIT optimized; use type classes or interfaces"
tags:
  - performance
  - jvm-optimization
rule:
  kind: method_invocation
  pattern: |
    $OBJ.getClass.getMethod($NAME).invoke($OBJ)
note: |
  Reflection is slow and JIT can't optimize. Use type classes or interfaces:
  trait Processable { def process(): Unit }
  def process(obj: Processable) = obj.process()

---
id: scala-perf-string-concatenation-loop
language: scala
severity: error
message: "Performance: String concatenation in loop is O(n^2); use StringBuilder"
tags:
  - performance
  - string-operations
rule:
  kind: assignment
  pattern: |
    var $RESULT = ""
    for ($ITEM <- $ITEMS) {
      $RESULT += $ITEM.toString
    }
note: |
  Each concatenation creates new String. Use StringBuilder:
  val sb = new StringBuilder; for (item <- items) sb.append(item); sb.toString
  Or use mkString: items.mkString

---
id: scala-perf-regex-compilation-loop
language: scala
severity: error
message: "Performance: Regex compiled each loop iteration; compile once and reuse"
tags:
  - performance
  - string-operations
rule:
  kind: loop
  pattern: |
    for ($LINE <- $LINES) {
      $LINE.matches("pattern.*")
      "pattern".r.findFirstIn($LINE)
    }
note: |
  Regex compilation is expensive. Compile once:
  val pattern = "pattern.*".r
  for (line <- lines) { pattern.findFirstIn(line) }

---
id: scala-perf-split-with-regex
language: scala
severity: info
message: "Performance: split with regex is slower than literal split"
tags:
  - performance
  - string-operations
rule:
  kind: method_invocation
  pattern: |
    $STRING.split("\\s+")
note: |
  Regex split is slower. For simple delimiters use literal:
  string.split(" ")
  Or use StringTokenizer for complex cases.

---
id: scala-perf-format-vs-interpolation
language: scala
severity: info
message: "Performance: String.format is slower than interpolation"
tags:
  - performance
  - string-operations
rule:
  kind: method_invocation
  pattern: |
    String.format("Value: %d", $VALUE)
    "%s %s".format($A, $B)
note: |
  format is slower than interpolation. Use: s"Value: $value" or s"$a $b"

---
id: scala-perf-generic-collection-primitives
language: scala
severity: warning
message: "Performance: Generic collections box primitives, causing GC pressure; use Array or specialized types"
tags:
  - performance
  - boxing-unboxing
rule:
  kind: variable_declaration
  pattern: |
    val $LIST: List[Int] = List(1, 2, 3)
note: |
  Generic collections box primitives. Use Array:
  val array = Array(1, 2, 3)
  Or specialized collections like ArrayBuffer[Int] with @specialized.

---
id: scala-perf-pattern-match-primitive-box
language: scala
severity: info
message: "Performance: Matching Any unboxes primitives; use overloading or type classes"
tags:
  - performance
  - boxing-unboxing
rule:
  kind: pattern_match
  pattern: |
    def process($X: Any) = $X match {
      case $I: Int => $I + 1
    }
note: |
  Matching Any unboxes primitives. Use overloading:
  def process(i: Int): Int = i + 1
  def process(d: Double): Double = d + 1.0

---
id: scala-perf-autoboxing-java-interop
language: scala
severity: warning
message: "Performance: Java generics force boxing of primitives; use primitive arrays when possible"
tags:
  - performance
  - boxing-unboxing
rule:
  kind: method_invocation
  pattern: |
    javaMethod($SCALA_INT)
    val $RESULT: Int = javaMethod()
note: |
  Java generics force boxing. Use primitive arrays:
  val array = Array[Int](1, 2, 3)
  JavaClass.processInts(array)  // Pass array, not individual values

---
id: scala-perf-unbuffered-file-reading
language: scala
severity: warning
message: "Performance: Character-by-character file reading is slow; use line-based or NIO"
tags:
  - performance
  - io-operations
rule:
  kind: method_invocation
  pattern: |
    Source.fromFile("file.txt").foreach(char => ...)
note: |
  Character-by-character reading is slow. Use line-based:
  val source = Source.fromFile("file.txt")
  try { source.getLines().foreach(line => ...) } finally { source.close() }
  Or NIO: Files.readAllLines(Paths.get("file.txt"))

---
id: scala-perf-resource-not-closed
language: scala
severity: warning
message: "Performance: Unclosed resources leak file handles; use Using"
tags:
  - performance
  - io-operations
rule:
  kind: method_invocation
  pattern: |
    val $SOURCE = Source.fromFile("file.txt")
    $SOURCE.getLines().toList
note: |
  Unclosed resources leak file handles. Use Using:
  import scala.util.Using
  Using(Source.fromFile("file.txt")) { source => source.getLines().toList }

---
id: scala-perf-sync-io-in-async
language: scala
severity: error
message: "Performance: Blocking I/O in default ExecutionContext exhausts thread pool; use blocking marker or non-blocking I/O"
tags:
  - performance
  - io-operations
  - futures
rule:
  kind: method_invocation
  pattern: |
    Future {
      Files.readAllBytes($PATH)
    }
note: |
  Blocking I/O exhausts thread pool. Use blocking marker:
  import scala.concurrent.blocking
  Future { blocking { Files.readAllBytes(path) } }
  Or non-blocking I/O: AsynchronousFileChannel

---
id: scala-perf-database-connection-per-query
language: scala
severity: error
message: "Performance: Creating connection per query is expensive; use connection pooling"
tags:
  - performance
  - io-operations
rule:
  kind: method_invocation
  pattern: |
    val $CONN = DriverManager.getConnection($URL)
    $CONN.close()
note: |
  Creating connections is expensive. Use connection pool:
  import com.zaxxer.hikari.HikariDataSource
  val dataSource = new HikariDataSource(); dataSource.setJdbcUrl(url); dataSource.setMaximumPoolSize(10)
  val conn = dataSource.getConnection()

---
id: scala-perf-http-client-per-request
language: scala
severity: warning
message: "Performance: Creating HTTP client per request is expensive; reuse client instance"
tags:
  - performance
  - io-operations
rule:
  kind: method_definition
  pattern: |
    def fetch($URL: String) = {
      val $CLIENT = HttpClient.newHttpClient()
      $CLIENT.send($REQUEST, BodyHandlers.ofString())
    }
note: |
  Creating HTTP client per request is expensive. Reuse client:
  object HttpService {
    private val client = HttpClient.newBuilder().connectTimeout(Duration.ofSeconds(10)).build()
    def fetch(url: String) = client.send(request, BodyHandlers.ofString())
  }
---
id: ruby-perf-string-concat-loop
language: ruby
severity: warning
message: "Performance: String concatenation in loop creates O(n^2) memory allocations"
tags:
  - performance
  - strings
rule:
  kind: loop
  pattern: "$LOOP { $STR += $VALUE }"
note: |
  Each += creates a new string object, causing O(n^2) memory allocations in loops.
  Use StringIO or Array#join instead:
  - StringIO: io = StringIO.new; items.each { |item| io << item }; result = io.string
  - Array#join: result = items.map { |item| process(item) }.join
  Benchmark Impact: 10-100x improvement for large iterations

---
id: ruby-perf-interpolation-loop
language: ruby
severity: warning
message: "Performance: Repeated string interpolation in loop wastes CPU"
tags:
  - performance
  - strings
rule:
  kind: loop
  pattern: "$LOOP { $RESULT = \"#{$PREFIX}#{$VAR}#{$SUFFIX}\" }"
note: |
  While interpolation is fast, repeated interpolation with fixed parts wastes CPU cycles.
  Pre-compute fixed parts or use format strings outside the loop.
  Optimization: Move constant string parts to variables before loop iteration.

---
id: ruby-perf-gsub-simple-string
language: ruby
severity: info
message: "Performance: gsub with regex for literal string should use string form"
tags:
  - performance
  - strings
  - regex
rule:
  kind: method_call
  pattern: "$STR.gsub(/literal/, $REPLACEMENT)"
note: |
  For literal string replacement, gsub(string, replacement) is faster than regex.
  String form avoids regex compilation overhead.
  Fix: str.gsub("foo", "bar") instead of str.gsub(/foo/, "bar")
  Benchmark Impact: 2-5x improvement

---
id: ruby-perf-repeated-split
language: ruby
severity: warning
message: "Performance: Repeated String#split creates array when only partial access needed"
tags:
  - performance
  - strings
rule:
  kind: method_call
  pattern: "$STR.split($DELIM)[$INDEX]"
note: |
  Splitting creates a full array even when only one element is needed.
  Use limit parameter or alternative methods:
  - str.split(",", 2).first (only split once)
  - str[/^[^,]*/] (regex for first element)
  - str.partition(",").first (for first element)

---
id: ruby-perf-string-plus-vs-shovel
language: ruby
severity: info
message: "Performance: String + creates new objects; << modifies in place"
tags:
  - performance
  - strings
rule:
  kind: operator
  pattern: "$A + $B + $C"
note: |
  + operator creates new objects for each operation.
  << modifies the string in place (but be aware of mutation).
  Fix: Use << for building strings when appropriate:
  - result = "".dup; result << a << b << c
  Note: Be careful with frozen strings and mutation side effects.

---
id: ruby-perf-unnecessary-dup
language: ruby
severity: info
message: "Performance: Unnecessary String duplication wastes memory"
tags:
  - performance
  - strings
rule:
  kind: method_call
  pattern: "$STR.dup.freeze"
note: |
  Dup then freeze is wasteful.
  - str.dup.gsub!($A, $B) is unnecessary since gsub already returns new string
  - "literal".dup allocates when literal already creates new string
  Remove unnecessary dup calls to reduce object allocation.

---
id: ruby-perf-array-append-plus-equals
language: ruby
severity: warning
message: "Performance: Array append with += creates new array on each iteration"
tags:
  - performance
  - collections
rule:
  kind: loop
  pattern: "$LOOP { $ARR += [$ITEM] }"
note: |
  += creates a new array on each iteration, causing O(n^2) behavior.
  Use << or push for in-place modification:
  - arr << item (modifies in place)
  - arr.push(item) (same as <<)
  - arr.concat([item]) (for multiple items)
  Benchmark Impact: 100x+ improvement for large arrays

---
id: ruby-perf-each-build-vs-map
language: ruby
severity: warning
message: "Performance: Manual array building with each is slower than map"
tags:
  - performance
  - collections
  - functional
rule:
  kind: block
  pattern: "$RESULT = []; $COLLECTION.each { |$ITEM| $RESULT << $TRANSFORM }"
note: |
  Manual array building is slower and more verbose than map.
  Use map directly: result = collection.map { |item| transform(item) }
  Benchmark Impact: 10-20% improvement

---
id: ruby-perf-select-first-vs-find
language: ruby
severity: warning
message: "Performance: select + first iterates entire collection; find stops at first match"
tags:
  - performance
  - collections
  - enumerable
rule:
  kind: method_call
  pattern: "$COLLECTION.select { |$X| $CONDITION }.first"
note: |
  select iterates the entire collection; find stops at first match (O(1) vs O(n)).
  Use find (or detect alias) for single-match queries.
  Fix: collection.find { |x| condition } instead of select...first
  Benchmark Impact: O(n) to O(1) for early matches

---
id: ruby-perf-reject-empty-vs-any
language: ruby
severity: warning
message: "Performance: select/reject + empty? creates intermediate array"
tags:
  - performance
  - collections
  - enumerable
rule:
  kind: method_call
  pattern: "$COLLECTION.select { |$X| $COND }.empty?"
note: |
  Creates intermediate array to check emptiness.
  Use any? or none? for membership checks without allocation.
  Fix: collection.any? { |x| cond } or collection.none? { |x| cond }
  Benchmark Impact: Avoids full iteration and allocation

---
id: ruby-perf-map-compact-vs-filter-map
language: ruby
severity: warning
message: "Performance: map + compact uses two iterations and intermediate array"
tags:
  - performance
  - collections
  - enumerable
  - ruby27
rule:
  kind: method_call
  pattern: "$COLLECTION.map { |$X| $TRANSFORM }.compact"
note: |
  Two iterations and intermediate array allocation.
  Use filter_map (Ruby 2.7+) for single pass:
  Fix: collection.filter_map { |x| transform if condition }
  Benchmark Impact: 50% improvement

---
id: ruby-perf-flatten-depth
language: ruby
severity: info
message: "Performance: Unlimited flatten is expensive for deeply nested arrays"
tags:
  - performance
  - collections
rule:
  kind: method_call
  pattern: "$ARR.flatten"
note: |
  Unlimited flatten traverses entire nesting depth.
  Specify depth if known: arr.flatten(1) for one level only.
  Reduces unnecessary traversal of deeply nested structures.

---
id: ruby-perf-sort-first-vs-min
language: ruby
severity: warning
message: "Performance: sort + first/last is O(n log n); min/max is O(n)"
tags:
  - performance
  - collections
  - sorting
rule:
  kind: method_call
  pattern: "$COLLECTION.sort.first"
note: |
  Full sort is O(n log n); min/max is O(n).
  Use min/max and related methods for extremum queries:
  - collection.min, collection.max
  - collection.min_by { |x| key }, collection.max_by { |x| key }
  - collection.minmax (both at once)
  Benchmark Impact: 10-100x improvement for large collections

---
id: ruby-perf-reverse-each
language: ruby
severity: warning
message: "Performance: reverse + each creates reversed copy before iterating"
tags:
  - performance
  - collections
rule:
  kind: method_call
  pattern: "$COLLECTION.reverse.each { |$X| $BLOCK }"
note: |
  Creates reversed copy before iterating.
  Use reverse_each to iterate backward without allocation.
  Fix: collection.reverse_each { |x| block }
  Benchmark Impact: Avoids array allocation

---
id: ruby-perf-array-include-large
language: ruby
severity: warning
message: "Performance: Array#include? on large arrays is O(n); Set#include? is O(1)"
tags:
  - performance
  - collections
  - lookups
rule:
  kind: method_call
  pattern: "$LARGE_ARRAY.include?($VALUE)"
note: |
  Array#include? is O(n); Set#include? is O(1).
  Convert to Set for repeated lookups:
  - set = large_array.to_set; set.include?(value) (O(1))
  - ALLOWED = %w[a b c].to_set.freeze (frozen constant)
  Benchmark Impact: O(n) to O(1) per lookup

---
id: ruby-perf-hash-keys-include
language: ruby
severity: warning
message: "Performance: Hash#keys.include? creates intermediate array; use key?"
tags:
  - performance
  - collections
  - hashes
rule:
  kind: method_call
  pattern: "$HASH.keys.include?($KEY)"
note: |
  Creates intermediate array for simple membership check.
  Use direct hash methods:
  - hash.key?(key) or has_key?(key) (O(1))
  - hash.value?(value) or has_value?(value) (still O(n) but no allocation)
  Benchmark Impact: Avoids array allocation

---
id: ruby-perf-each-with-index
language: ruby
severity: info
message: "Performance: to_a.each_with_index forces array creation"
tags:
  - performance
  - collections
  - enumerable
rule:
  kind: method_call
  pattern: "$COLLECTION.to_a.each_with_index { |$X, $I| $BLOCK }"
note: |
  to_a forces array creation unnecessarily.
  Use each.with_index for enumerators or each_with_index for arrays:
  - collection.each.with_index { |x, i| block }
  - collection.each_with_index { |x, i| block } (for arrays)

---
id: ruby-perf-uniq-in-loop
language: ruby
severity: warning
message: "Performance: Calling uniq repeatedly in loop is O(n) each time"
tags:
  - performance
  - collections
rule:
  kind: loop
  pattern: "$LOOP { $ARR = $ARR.uniq }"
note: |
  Calling uniq repeatedly is O(n) each time.
  Use Set from the start for O(1) uniqueness:
  - set = Set.new; items.each { |item| set.add(item) }
  Avoids repeated scan of entire collection.

---
id: ruby-perf-repeated-method-condition
language: ruby
severity: warning
message: "Performance: Repeated method call in condition"
tags:
  - performance
  - method-calls
rule:
  kind: conditional
  pattern: "if $OBJ.$METHOD && $OBJ.$METHOD.$CHAIN"
note: |
  Same method called multiple times unnecessarily.
  Cache the result in condition:
  - if (result = obj.method) && result.chain { result.do_something }
  Reduces method lookup and execution overhead.

---
id: ruby-perf-block-to-proc
language: ruby
severity: info
message: "Performance: Simple method calls can use Symbol#to_proc"
tags:
  - performance
  - functional
rule:
  kind: block
  pattern: "$COLLECTION.map { |$X| $X.to_s }"
note: |
  Simple method calls can use Symbol#to_proc for slight performance gain.
  Alternatives:
  - collection.map(&:to_s)
  - collection.select(&:valid?)
  - collection.compact (instead of reject(&:nil?))
  Benchmark Impact: Slightly faster for simple transforms

---
id: ruby-perf-proc-allocation-loop
language: ruby
severity: warning
message: "Performance: Creating procs/lambdas in loop wastes allocations"
tags:
  - performance
  - blocks
  - allocation
rule:
  kind: loop
  pattern: "$LOOP { $PROC = Proc.new { $BLOCK } }"
note: |
  Creating procs in loops wastes allocations.
  Define outside loop and reuse:
  - processor = ->(x) { x * 2 }
  - items.each { |item| processor.call(item) }
  Reduces allocation overhead in tight loops.

---
id: ruby-perf-method-object-hot-path
language: ruby
severity: warning
message: "Performance: method(:name) creates Method object on each call"
tags:
  - performance
  - method-calls
rule:
  kind: loop
  pattern: "$LOOP { method(:$NAME).call($ARGS) }"
note: |
  method(:name) creates Method object on each call.
  Cache method reference or call directly:
  - processor = method(:process_item); items.each { |item| processor.call(item) }
  - items.each { |item| process_item(item) } (direct call)
  Reduces object allocation in hot paths.

---
id: ruby-perf-respond-to-hot-path
language: ruby
severity: info
message: "Performance: respond_to? in hot path has method lookup overhead"
tags:
  - performance
  - method-calls
  - introspection
rule:
  kind: loop
  pattern: "$LOOP { if $OBJ.respond_to?(:$METHOD) }"
note: |
  respond_to? has method lookup overhead in tight loops.
  Check once outside loop or use duck typing:
  - can_process = obj.respond_to?(:process)
  - items.each { |item| obj.process(item) if can_process }
  Or use try (Rails): obj.try(:method)

---
id: ruby-perf-dynamic-symbol-creation
language: ruby
severity: warning
message: "Performance: Dynamic symbol creation from user input causes memory leaks"
tags:
  - performance
  - memory
  - security
rule:
  kind: method_call
  pattern: "$USER_INPUT.to_sym"
note: |
  Symbols are never garbage collected (pre-Ruby 2.2). Even in modern Ruby, excessive
  symbol creation wastes memory and can be DOS vector.
  Use strings for dynamic keys:
  - hash[user_input] (string key, not symbol)
  - sym = ALLOWED_SYMBOLS.find { |s| s.to_s == input } (validate against whitelist)
  Critical for user-controlled data.

---
id: ruby-perf-range-in-loop
language: ruby
severity: info
message: "Performance: Range creation in loop allocates memory"
tags:
  - performance
  - allocation
rule:
  kind: loop
  pattern: "$LOOP { (0..10).each { |$I| $BLOCK } }"
note: |
  Range objects allocate on each iteration.
  Define as constant and reuse:
  - RANGE = (0..10).freeze
  - items.each { RANGE.each { |i| block } }
  Reduces allocation overhead in nested loops.

---
id: ruby-perf-mutable-default-argument
language: ruby
severity: warning
message: "Performance: Mutable default arguments allocate on each call"
tags:
  - performance
  - allocation
  - defaults
rule:
  kind: method_definition
  pattern: "def method(options = {})"
note: |
  Default mutable objects allocate on each call AND may share state (Ruby gotcha).
  Use nil default with ||= pattern:
  - def method(options = nil); options ||= {}; end
  Or use frozen constant with safety:
  - EMPTY_HASH = {}.freeze
  - def method(options = EMPTY_HASH); options = options.dup if options.equal?(EMPTY_HASH); end

---
id: ruby-perf-unnecessary-wrapping
language: ruby
severity: info
message: "Performance: Unnecessary type wrapping wastes cycles"
tags:
  - performance
  - type-coercion
rule:
  kind: method_call
  pattern: "Array($ALREADY_ARRAY)"
note: |
  Wrapping already-correct types wastes CPU cycles.
  Type check first or trust input:
  - items = input.is_a?(Array) ? input : [input]
  - Or use Kernel#Array which handles nil gracefully
  Avoids redundant wrapping overhead.

---
id: ruby-perf-object-allocation-loop
language: ruby
severity: warning
message: "Performance: Object.new in tight loop causes allocation and GC pressure"
tags:
  - performance
  - allocation
rule:
  kind: loop
  pattern: "$LOOP { $OBJ = $CLASS.new($ARGS) }"
note: |
  Object allocation and garbage collection pressure from temporary objects.
  Reuse objects or use struct:
  - result = Struct.new(:a, :b).new
  - items.each { |item| result.a = item.first; result.b = item.last; process(result) }
  Object pooling pattern reduces allocation overhead.

---
id: ruby-perf-regex-compile-loop
language: ruby
severity: warning
message: "Performance: Regex compilation in loop is expensive"
tags:
  - performance
  - regex
rule:
  kind: loop
  pattern: "$LOOP { $STR =~ /#{$PATTERN}/ }"
note: |
  Regex compilation is expensive operation in tight loops.
  Compile once as constant:
  - PATTERN = /pattern/i.freeze (constant)
  - @pattern_cache ||= {}; @pattern_cache[pattern] ||= Regexp.new(pattern) (cached)
  Benchmark Impact: 10-50x improvement

---
id: ruby-perf-match-vs-operator
language: ruby
severity: info
message: "Performance: match creates MatchData; match? returns boolean without allocation"
tags:
  - performance
  - regex
rule:
  kind: method_call
  pattern: "$STR.match($PATTERN)"
note: |
  match creates MatchData object; match? doesn't allocate (Ruby 2.4+).
  Use match? for boolean checks:
  - if str.match?(/pattern/) (no allocation)
  - if str =~ /(?<name>pattern)/; puts $~[:name] (for captures)
  Benchmark Impact: 2x improvement for boolean checks

---
id: ruby-perf-gsub-vs-sub
language: ruby
severity: info
message: "Performance: gsub scans entire string even with anchors; use sub for single replacement"
tags:
  - performance
  - regex
rule:
  kind: method_call
  pattern: "$STR.gsub(/^prefix/, $REPLACEMENT)"
note: |
  gsub scans entire string even with anchors that guarantee single match.
  Use sub for single replacement:
  - str.sub(/^prefix/, replacement)
  - str.sub(/suffix$/, replacement)
  Avoids full string scan for anchored patterns.

---
id: ruby-perf-scan-all-at-once
language: ruby
severity: warning
message: "Performance: scan creates array of all matches; use gsub block for streaming"
tags:
  - performance
  - regex
rule:
  kind: method_call
  pattern: "$STR.scan($PATTERN).each { |$M| $BLOCK }"
note: |
  scan creates array of all matches first before iteration.
  Use gsub with block for streaming:
  - str.gsub(pattern) { |match| process(match); match }
  Reduces memory allocation for large result sets.

---
id: ruby-perf-file-read-large
language: ruby
severity: warning
message: "Performance: File.read for large files loads entire file into memory"
tags:
  - performance
  - io
rule:
  kind: method_call
  pattern: "File.read($LARGE_FILE)"
note: |
  Reads entire file into memory at once.
  Stream processing for large files:
  - File.foreach(path) { |line| process(line) }
  - File.open(path).each_line.lazy.take(100) (lazy enumerator)
  Benchmark Impact: Constant memory usage regardless of file size

---
id: ruby-perf-puts-loop
language: ruby
severity: warning
message: "Performance: puts in tight loop flushes buffer repeatedly"
tags:
  - performance
  - io
rule:
  kind: loop
  pattern: "$LOOP { puts $OUTPUT }"
note: |
  Each puts may flush buffer, creating IO overhead.
  Buffer output in memory:
  - output = StringIO.new
  - items.each { |item| output.puts(process(item)) }
  - puts output.string
  Reduces IO operations and flush overhead.

---
id: ruby-perf-repeated-file-open
language: ruby
severity: warning
message: "Performance: Repeated File open/close in loop is expensive"
tags:
  - performance
  - io
rule:
  kind: loop
  pattern: "$LOOP { File.open($PATH) { |$F| $BLOCK } }"
note: |
  File handle creation and destruction is expensive in tight loops.
  Open file once and reuse:
  - File.open(path, 'r') { |file| items.each { |item| process(file.gets) } }
  Reduces IO system call overhead.

---
id: ruby-perf-json-parse-performance
language: ruby
severity: warning
message: "Performance: stdlib JSON is slower than alternatives like Oj"
tags:
  - performance
  - json
  - dependencies
rule:
  kind: method_call
  pattern: "JSON.parse($INPUT)"
note: |
  stdlib JSON is slower than C-extension alternatives.
  Use Oj gem for better performance:
  - require 'oj'
  - Oj.load(input, mode: :strict)
  - Oj.dump(object)
  Benchmark Impact: 2-5x improvement

---
id: ruby-perf-missing-memoization
language: ruby
severity: warning
message: "Performance: Missing memoization for expensive computation"
tags:
  - performance
  - caching
rule:
  kind: method_definition
  pattern: "def expensive_method; $EXPENSIVE_OPERATION; end"
note: |
  Complex computation without caching is recalculated on each call.
  Add memoization:
  - def expensive_method; @expensive_method ||= begin; ... ; end; end
  - With arguments: @cache ||= {}; @cache[key] ||= expensive_compute(key)
  Or use Memoist gem: extend Memoist; memoize :method

---
id: ruby-perf-memoization-falsy
language: ruby
severity: warning
message: "Performance: Memoization with ||= fails for falsy return values"
tags:
  - performance
  - caching
  - correctness
rule:
  kind: method_definition
  pattern: "def method; @result ||= compute; end"
note: |
  ||= recomputes if cached value is nil or false.
  Use defined? or explicit nil check:
  - def method; return @result if defined?(@result); @result = compute; end
  - Or use Memoizable gem: extend Memoist; memoize :method
  Critical for methods that legitimately return nil/false.

---
id: ruby-perf-ivar-check
language: ruby
severity: info
message: "Performance: Use defined? instead of instance_variable_defined?"
tags:
  - performance
  - introspection
rule:
  kind: conditional
  pattern: "instance_variable_defined?(:@var)"
note: |
  defined? is simpler and clearer for instance variable checks.
  Fix: defined?(@var) ? @var : (@var = default)
  Slightly more efficient than reflection-based approach.

---
id: ruby-perf-n-plus-one-query
language: ruby
severity: warning
message: "Performance: N+1 query - accessing association triggers separate query per record"
tags:
  - performance
  - database
  - critical
rule:
  kind: loop
  pattern: "$COLLECTION.each do |$ITEM| $ITEM.$ASSOCIATION.$METHOD; end"
note: |
  Triggers separate query for each association access.
  Use eager loading:
  - users.includes(:posts).each { |u| u.posts.size }
  - orders.preload(:customer).each { |o| o.customer.name }
  - users.eager_load(:profile) (for JOIN)
  Benchmark Impact: O(n) queries reduced to O(1)

---
id: ruby-perf-count-vs-size
language: ruby
severity: warning
message: "Performance: count always hits DB; use size for smart caching behavior"
tags:
  - performance
  - database
rule:
  kind: method_call
  pattern: "$RELATION.count"
note: |
  Different methods have different performance characteristics:
  - size: Uses cached count if loaded, else COUNT query
  - count: Always hits database with fresh COUNT query
  - length: Loads all records into memory
  Use size for optimal behavior based on loaded state.

---
id: ruby-perf-all-each-vs-find-each
language: ruby
severity: warning
message: "Performance: all.each loads entire table into memory; use find_each"
tags:
  - performance
  - database
rule:
  kind: method_call
  pattern: "$MODEL.all.each { |$RECORD| $BLOCK }"
note: |
  all.each loads entire table into memory.
  Use batched iteration:
  - Model.find_each(batch_size: 1000) { |record| process(record) }
  - Model.in_batches(of: 1000).each_record { |record| process(record) }
  Benchmark Impact: Constant memory usage

---
id: ruby-perf-pluck-vs-map
language: ruby
severity: warning
message: "Performance: map instantiates ActiveRecord objects; pluck returns raw values"
tags:
  - performance
  - database
rule:
  kind: method_call
  pattern: "$MODEL.all.map(&:$ATTRIBUTE)"
note: |
  map instantiates full ActiveRecord objects for each row.
  Use pluck for raw value extraction:
  - Model.pluck(:column) (single column)
  - Model.pluck(:id, :name) (multiple columns)
  Benchmark Impact: 5-10x improvement

---
id: ruby-perf-exists-vs-present
language: ruby
severity: warning
message: "Performance: present? loads records; exists? uses efficient EXISTS query"
tags:
  - performance
  - database
rule:
  kind: method_call
  pattern: "$MODEL.where($COND).present?"
note: |
  present? loads records to check; exists? uses efficient EXISTS query.
  Fix: Model.exists?(conditions) or Model.where(conditions).exists?
  Benchmark Impact: Much faster for large tables

---
id: ruby-perf-update-all-vs-individual
language: ruby
severity: warning
message: "Performance: Individual updates = O(n) queries; use update_all"
tags:
  - performance
  - database
rule:
  kind: loop
  pattern: "$MODEL.where($COND).each { |$R| $R.update($ATTRS) }"
note: |
  Each update generates separate query.
  Use batch update:
  - Model.where(conditions).update_all(column: value)
  Benchmark Impact: Single query vs O(n) queries

---
id: ruby-perf-select-ids-vs-pluck
language: ruby
severity: warning
message: "Performance: select(:id).map(&:id) is less efficient than pluck(:id)"
tags:
  - performance
  - database
rule:
  kind: method_call
  pattern: "$MODEL.select(:id).map(&:id)"
note: |
  Creates ActiveRecord objects just to extract ID.
  Use pluck or ids helper:
  - Model.where(conditions).pluck(:id)
  - Model.ids (shorthand for pluck(:id))
  Avoids unnecessary object instantiation.

---
id: ruby-perf-where-not-vs-excluding
language: ruby
severity: info
message: "Performance: Large NOT IN clauses are slow; consider subquery or restructure"
tags:
  - performance
  - database
rule:
  kind: method_call
  pattern: "$MODEL.where.not(id: $LARGE_IDS)"
note: |
  Large NOT IN clauses degrade performance.
  Use subquery approach:
  - Model.where.not(id: excluded_scope)
  - Model.excluding(records) (Rails 7+)
  Database-level optimization preferred for large exclusions.

---
id: ruby-perf-repeated-association
language: ruby
severity: warning
message: "Performance: Repeated association queries cause multiple DB hits"
tags:
  - performance
  - database
rule:
  kind: method_call
  pattern: "$MODEL.$ASSOC.where($COND1); $MODEL.$ASSOC.where($COND2)"
note: |
  Multiple queries on same association.
  Cache association and filter in memory:
  - assoc = model.association.to_a
  - filtered1 = assoc.select { |r| cond1 }
  - filtered2 = assoc.select { |r| cond2 }
  Reduces database round-trips.

---
id: ruby-perf-missing-lazy
language: ruby
severity: warning
message: "Performance: Missing lazy for chain operations creates intermediate arrays"
tags:
  - performance
  - enumerable
rule:
  kind: method_call
  pattern: "$LARGE_COLLECTION.select { $COND }.map { $TRANSFORM }.first(10)"
note: |
  Creates intermediate arrays for each operation.
  Use lazy for chained transformations:
  - large_collection.lazy.select { cond }.map { transform }.first(10)
  - enum.lazy.map { a }.select { b }.take(n).force
  Benchmark Impact: Avoids intermediate arrays

---
id: ruby-perf-unnecessary-force
language: ruby
severity: info
message: "Performance: force on lazy negates benefits before iteration"
tags:
  - performance
  - enumerable
rule:
  kind: method_call
  pattern: "$ENUM.lazy.$CHAIN.force.each { $BLOCK }"
note: |
  force realizes entire lazy chain before iteration.
  Keep lazy and iterate directly:
  - enum.lazy.chain.each { block } (stay lazy)
  Preserves lazy evaluation benefits.

---
id: ruby-perf-render-loop
language: ruby
severity: warning
message: "Performance: render in loop has per-item lookup and compilation overhead"
tags:
  - performance
  - rails
  - views
rule:
  kind: loop
  pattern: "$COLLECTION.each { |$ITEM| render partial: $PARTIAL, locals: { $VAR: $ITEM } }"
note: |
  Partial lookup and rendering overhead multiplied by collection size.
  Use Rails collection rendering:
  - render partial: 'item', collection: @items, as: :item
  - render partial: 'item', collection: @items, cached: true (with caching)
  Benchmark Impact: 2-10x improvement

---
id: ruby-perf-helper-in-loop
language: ruby
severity: info
message: "Performance: Helper method overhead in view loops"
tags:
  - performance
  - rails
  - views
rule:
  kind: erb_template
  pattern: "<% @items.each do |item| %><%= number_to_currency(item.price) %><% end %>"
note: |
  Helper method invocation overhead in tight template loops.
  Cache formatted values in presenter:
  - class ItemPresenter; def formatted_price; @formatted_price ||= number_to_currency(item.price); end; end
  Or format in controller before passing to view.

---
id: ruby-perf-asset-compile-development
language: ruby
severity: warning
message: "Performance: On-the-fly asset compilation in development is slow"
tags:
  - performance
  - rails
  - assets
rule:
  kind: configuration
  pattern: "config.assets.compile = true"
note: |
  On-the-fly compilation adds latency to every request in development.
  Precompile assets or use modern tooling:
  - Use Webpacker or Vite for hot reloading
  - Or precompile in development: rails assets:precompile
  Significantly improves development experience.

---
id: ruby-perf-missing-index
language: ruby
severity: warning
message: "Performance: Missing database indexes on foreign keys causes full table scans"
tags:
  - performance
  - database
  - critical
rule:
  kind: migration
  pattern: "add_column :table, :foreign_key_id, :integer"
note: |
  Foreign keys and frequently-queried columns need indexes.
  Add indexes in migration:
  - add_index :table, :foreign_key_id
  - add_index :table, [:composite, :key] (for multi-column queries)
  Benchmark Impact: Dramatic query improvement

---
id: ruby-perf-unnecessary-includes
language: ruby
severity: warning
message: "Performance: Eager loading unused associations wastes memory"
tags:
  - performance
  - database
rule:
  kind: method_call
  pattern: "$MODEL.includes(:$ASSOC).where($COND)"
note: |
  Including unused associations wastes memory.
  Only include associations actually used in result processing:
  - Remove .includes(:unused_assoc)
  - Keep only necessary eager loads
  Reduces memory overhead.

---
id: ruby-perf-thread-without-pool
language: ruby
severity: warning
message: "Performance: Unbounded thread creation causes overhead and potential issues"
tags:
  - performance
  - concurrency
rule:
  kind: loop
  pattern: "$LOOP { Thread.new { $BLOCK } }"
note: |
  Unbounded thread creation causes context switch and memory overhead.
  Use thread pool:
  - require 'concurrent'
  - pool = Concurrent::FixedThreadPool.new(10)
  - items.each { |item| pool.post { process(item) } }
  - pool.shutdown; pool.wait_for_termination
  Bounded concurrency with connection pooling.

---
id: ruby-perf-mutex-hot-path
language: ruby
severity: warning
message: "Performance: Mutex in hot path causes lock contention"
tags:
  - performance
  - concurrency
rule:
  kind: loop
  pattern: "$LOOP { @mutex.synchronize { $BLOCK } }"
note: |
  Lock contention slows concurrent code.
  Minimize critical section:
  - result = process(item) (outside mutex)
  - @mutex.synchronize { @results << result } (quick update)
  Reduce lock hold time to improve throughput.

---
id: ruby-perf-missing-fragment-cache
language: ruby
severity: warning
message: "Performance: Expensive view fragment without cache repeats work"
tags:
  - performance
  - rails
  - caching
rule:
  kind: erb_template
  pattern: "<% @items.each do |item| %><%= render_complex_item(item) %><% end %>"
note: |
  Expensive rendering repeated on each request.
  Add fragment caching:
  - <% cache item do %><%= render_complex_item(item) %><% end %>
  Rails automatically invalidates cache on model updates.

---
id: ruby-perf-cache-key-version
language: ruby
severity: warning
message: "Performance: Static cache keys cause stale cache"
tags:
  - performance
  - caching
  - correctness
rule:
  kind: method_call
  pattern: "Rails.cache.fetch(\"static_key\") { $BLOCK }"
note: |
  Static keys don't invalidate on model updates.
  Use model-aware cache keys:
  - Rails.cache.fetch([@item, 'complex_calc']) { block }
  - cache(@item) { block } (uses updated_at automatically)
  Ensures cache invalidation on updates.

---
id: ruby-perf-large-object-memory
language: ruby
severity: warning
message: "Performance: Large objects in memory prevent GC and bloat memory"
tags:
  - performance
  - memory
rule:
  kind: assignment
  pattern: "@all_records = $MODEL.all.to_a"
note: |
  Holding large objects in instance variables prevents garbage collection.
  Stream processing or pagination:
  - Model.find_each { |record| process(record) }
  - Use pagination for collections that must be loaded
  Reduces memory footprint and GC pressure.

---
id: ruby-perf-ivar-accumulation
language: ruby
severity: warning
message: "Performance: Instance variable accumulation prevents garbage collection"
tags:
  - performance
  - memory
rule:
  kind: method_call
  pattern: "@processed ||= []; @processed << item"
note: |
  Accumulating data in instance variables prevents GC.
  Use local variables with proper scope:
  - def process_batch(items)
  - results = items.map { |item| process(item) }
  - save_results(results)
  - (results goes out of scope for GC)
  Allows garbage collection of intermediate data.

---
id: ruby-perf-nested-loop-quadratic
language: ruby
severity: warning
message: "Performance: Nested loop with equality check is O(n^2)"
tags:
  - performance
  - algorithm-complexity
rule:
  kind: loop
  pattern: "$OUTER.each { |$A| $INNER.each { |$B| if $A.$PROP == $B.$PROP } }"
note: |
  Nested loops with equality check are O(n^2) complexity.
  Use hash for O(n) lookup:
  - lookup = inner.group_by(&:prop)
  - outer.each { |a| lookup[a.prop]&.each { |b| block } }
  Benchmark Impact: O(n^2) to O(n)

---
id: ruby-perf-repeated-linear-search
language: ruby
severity: warning
message: "Performance: Repeated linear search is O(n) per iteration"
tags:
  - performance
  - algorithm-complexity
rule:
  kind: loop
  pattern: "$LOOP { $ARR.find { |$X| $X.$KEY == $VALUE } }"
note: |
  Linear search in loop causes O(n^2) behavior.
  Build index for O(1) lookups:
  - index = arr.index_by(&:key)
  - loop { result = index[value] }
  Or use each_with_object for Rails-style indexing.

---
id: ruby-perf-tr-vs-gsub
language: ruby
severity: info
message: "Performance: tr is 3-5x faster than gsub for character replacement"
tags:
  - performance
  - strings
rule:
  kind: method_call
  pattern: "$STR.gsub(/[abc]/, 'x')"
note: |
  tr is much faster for character-to-character translation.
  Fix: str.tr('abc', 'xxx') or str.tr('a', 'x')
  Benchmark Impact: 3-5x improvement

---
id: ruby-perf-format-vs-interpolation
language: ruby
severity: info
message: "Performance: Interpolation is faster than format for simple cases"
tags:
  - performance
  - strings
rule:
  kind: method_call
  pattern: "sprintf(\"%s: %d\", $A, $B)"
note: |
  String interpolation is faster for simple formatting:
  - "#{a}: #{b}" (faster)
  - format("%.2f", number) (use for complex formatting only)
  Use format only when needed for specific formatting options.
---
id: clojure-perf-concat-quadratic
language: clojure
severity: warning
message: "Performance: concat in loop creates O(n^2) complexity"
tags:
  - performance
  - collections
  - lazy-sequences
rule:
  kind: loop
  pattern: "(loop ... (concat ...))"
note: |
  concat creates lazy sequences. Repeated concat in loops leads to deeply nested
  lazy sequences with O(n^2) realization cost.
  Fix: Use into with transducers or conj with vectors.

---
id: clojure-perf-conj-list
language: clojure
severity: warning
message: "Performance: conj on list instead of vector for append"
tags:
  - performance
  - collections
rule:
  kind: function_call
  pattern: "(conj list-var item)"
note: |
  conj on lists prepends (O(1)), on vectors appends (O(1)). Using lists when
  order matters for appending is wrong.
  Fix: Use vectors when you need to append or access by index.

---
id: clojure-perf-nth-list
language: clojure
severity: error
message: "Performance: nth on list is O(n) instead of O(log n) on vector"
tags:
  - performance
  - collections
rule:
  kind: function_call
  pattern: "(nth list-var n)"
note: |
  nth on lists is O(n). On vectors it's O(log32 n) - effectively O(1).
  Fix: Use vectors for indexed access.

---
id: clojure-perf-count-lazy
language: clojure
severity: warning
message: "Performance: count on lazy sequence realizes entire sequence"
tags:
  - performance
  - lazy-sequences
rule:
  kind: function_call
  pattern: "(count lazy-seq)"
note: |
  count realizes entire lazy sequence. Can cause memory issues for infinite sequences.
  Fix: Use bounded operations like take first, or use counted? to check.

---
id: clojure-perf-assoc-loop
language: clojure
severity: error
message: "Performance: assoc in loop instead of transient creates O(n) allocations"
tags:
  - performance
  - collections
rule:
  kind: loop
  pattern: "(loop ... (assoc map ...))"
note: |
  Each assoc creates new persistent map. For bulk operations, use transients.
  Fix: Use transient, assoc!, and persistent! for bulk updates.

---
id: clojure-perf-into-no-transducer
language: clojure
severity: warning
message: "Performance: into without transducer creates intermediate sequences"
tags:
  - performance
  - transducers
rule:
  kind: function_call
  pattern: "(into [] (map f (filter g coll)))"
note: |
  Chained sequence operations create intermediate sequences. Transducers eliminate them.
  Fix: Compose transducers with comp and pass to into.
  Example: (into [] (comp (filter even?) (map inc)) (range 1000))

---
id: clojure-perf-repeated-get
language: clojure
severity: info
message: "Performance: repeated get on same map key"
tags:
  - performance
  - collections
rule:
  kind: function_call
  pattern: "(get m k)"
note: |
  Multiple get calls with same key traverse map structure. Cache results if used multiple times.
  Fix: Use let to bind result, or use destructuring.

---
id: clojure-perf-keys-vals-iteration
language: clojure
severity: info
message: "Performance: keys/vals creates intermediate sequences instead of direct iteration"
tags:
  - performance
  - collections
rule:
  kind: function_call
  pattern: "(map f (keys m))||(map f (vals m))"
note: |
  keys/vals create intermediate sequences. Direct map iteration is more efficient.
  Fix: Iterate over map entries directly with (map (fn [[_ v]] (process v)) m)

---
id: clojure-perf-set-membership
language: clojure
severity: warning
message: "Performance: membership testing on non-set is O(n) instead of O(log n)"
tags:
  - performance
  - collections
rule:
  kind: function_call
  pattern: "(some #{x} coll)"
note: |
  Membership testing on vectors/lists is O(n). Sets are O(log32 n).
  Fix: Convert to set if testing membership multiple times.

---
id: clojure-perf-sorted-map-overhead
language: clojure
severity: info
message: "Performance: sorted-map has O(log n) operations vs O(1) for hash map"
tags:
  - performance
  - collections
rule:
  kind: function_call
  pattern: "(sorted-map ...)"
note: |
  Sorted maps have O(log n) operations vs O(1) amortized for hash maps.
  Fix: Use regular {} or hash-map unless sorting is required.

---
id: clojure-perf-nested-merge
language: clojure
severity: info
message: "Performance: nested merge operations create intermediate maps"
tags:
  - performance
  - collections
rule:
  kind: function_call
  pattern: "(merge m1 (merge m2 m3))"
note: |
  Each merge creates a new map. Merge accepts varargs.
  Fix: Use single merge with all maps.

---
id: clojure-perf-update-in-single-level
language: clojure
severity: info
message: "Performance: update-in overhead for single level update"
tags:
  - performance
  - collections
rule:
  kind: function_call
  pattern: "(update-in m [:key] f)"
note: |
  update-in has overhead for path traversal. Use update for single level.
  Fix: Use update for single-level updates.

---
id: clojure-perf-assoc-in-single-level
language: clojure
severity: info
message: "Performance: assoc-in overhead for single level assignment"
tags:
  - performance
  - collections
rule:
  kind: function_call
  pattern: "(assoc-in m [:key] v)"
note: |
  assoc-in has overhead for path traversal.
  Fix: Use assoc for single-level assignments.

---
id: clojure-perf-lazy-head-retention
language: clojure
severity: error
message: "Performance: lazy sequence at top level retains head and prevents GC"
tags:
  - performance
  - lazy-sequences
  - memory
rule:
  kind: def
  pattern: "(def x (map f large-coll))"
note: |
  Lazy sequences at top level retain head, preventing garbage collection.
  Fix: Force realization with doall or use vec.

---
id: clojure-perf-lazy-side-effects
language: clojure
severity: error
message: "Performance: lazy sequence in loop with side effects may not execute"
tags:
  - performance
  - lazy-sequences
rule:
  kind: function_call
  pattern: "(doseq ... (map ...))"
note: |
  map is lazy. Side effects may not execute, or execute unexpectedly.
  Fix: Use run!, doseq, or doall for side effects.

---
id: clojure-perf-repeated-lazy-realization
language: clojure
severity: warning
message: "Performance: repeated iteration over unrealized lazy sequence recomputes"
tags:
  - performance
  - lazy-sequences
rule:
  kind: let
  pattern: "(let [expensive (map expensive-fn data)] ...)"
note: |
  Each iteration re-computes the lazy sequence.
  Fix: Realize once with vec or doall if iterating multiple times.

---
id: clojure-perf-chunked-sequence-overhead
language: clojure
severity: info
message: "Performance: chunked sequences process 32 items at a time"
tags:
  - performance
  - lazy-sequences
rule:
  kind: function_call
  pattern: "(first (filter pred large-coll))"
note: |
  Chunked sequences process 32 items at a time. May compute more than necessary.
  Fix: Use some for finding first match, or use reducers.

---
id: clojure-perf-count-vs-bounded
language: clojure
severity: warning
message: "Performance: count on large collection realizes entire sequence"
tags:
  - performance
  - lazy-sequences
rule:
  kind: function_call
  pattern: "(= (count coll) n)"
note: |
  count realizes entire sequence. Use bounded checks when possible.
  Fix: Use bounded-count or early termination.

---
id: clojure-perf-lazy-concat-stacking
language: clojure
severity: error
message: "Performance: lazy concat stacking creates O(n^2) complexity"
tags:
  - performance
  - lazy-sequences
  - collections
rule:
  kind: function_call
  pattern: "(reduce concat colls)"
note: |
  Creates deeply nested lazy sequences. O(n^2) when realized.
  Fix: Use apply concat, mapcat, or into with cat.

---
id: clojure-perf-doall-large-sequences
language: clojure
severity: warning
message: "Performance: doall on very large sequences retains head"
tags:
  - performance
  - lazy-sequences
  - memory
rule:
  kind: function_call
  pattern: "(doall very-large-sequence)"
note: |
  doall retains head. Can cause OutOfMemoryError for very large sequences.
  Fix: Use run! for side effects, or process in batches.

---
id: clojure-perf-missing-type-hints
language: clojure
severity: error
message: "Performance: missing type hints on Java interop causes reflection"
tags:
  - performance
  - java-interop
rule:
  kind: function_call
  pattern: "(.method obj)"
note: |
  Without type hints, Clojure uses reflection for method calls. 10-100x slower.
  Fix: Add type hints on parameters or expressions.
  Example: (defn get-length [^String s] (.length s))

---
id: clojure-perf-reflection-warnings
language: clojure
severity: warning
message: "Performance: file without warn-on-reflection enabled"
tags:
  - performance
  - java-interop
rule:
  kind: namespace
  pattern: "(ns myapp.core)"
note: |
  Reflection warnings help identify performance issues.
  Fix: Enable at namespace level for performance-critical code:
  (set! *warn-on-reflection* true)

---
id: clojure-perf-type-hint-location
language: clojure
severity: warning
message: "Performance: type hint on wrong location affects wrong code path"
tags:
  - performance
  - java-interop
rule:
  kind: function_def
  pattern: "(defn ^String process [s] ...)"
note: |
  Hints on def affect return type. Parameter hints affect method lookup.
  Fix: Place hints on function parameters for method calls.
  Correct: (defn process [^String s] (.toUpperCase s))

---
id: clojure-perf-boxed-math
language: clojure
severity: warning
message: "Performance: math on unchecked numbers uses boxed operations"
tags:
  - performance
  - primitives
rule:
  kind: function_def
  pattern: "(defn sum [numbers] (reduce + numbers))"
note: |
  Clojure boxes numbers by default. Use primitive type hints for math.
  Fix: Use ^long or ^double hints, or unchecked-* operations.

---
id: clojure-perf-array-access-hints
language: clojure
severity: error
message: "Performance: array access without type hints uses reflection"
tags:
  - performance
  - primitives
rule:
  kind: function_call
  pattern: "(aget arr idx)"
note: |
  Array access without hints uses reflection.
  Fix: Type hint arrays with ^longs, ^doubles, ^objects, etc.

---
id: clojure-perf-inefficient-array-creation
language: clojure
severity: warning
message: "Performance: into-array creates Object array instead of primitive"
tags:
  - performance
  - primitives
rule:
  kind: function_call
  pattern: "(into-array (map ...))"
note: |
  into-array creates Object arrays. Use typed array constructors.
  Fix: Use long-array, double-array, etc.

---
id: clojure-perf-varargs-hot-path
language: clojure
severity: warning
message: "Performance: variadic functions in hot path create arrays"
tags:
  - performance
  - functions
rule:
  kind: function_def
  pattern: "(defn add [& nums] ...)"
note: |
  Varargs create arrays and seqs. Fixed arity is faster.
  Fix: Provide fixed-arity overloads for common cases.

---
id: clojure-perf-non-tail-recursion
language: clojure
severity: error
message: "Performance: non-tail recursion without recur causes stack overflow"
tags:
  - performance
  - recursion
rule:
  kind: function_call
  pattern: "(defn factorial [n] ... (factorial (dec n)) ...)"
note: |
  Without recur, each call uses stack space. Deep recursion causes StackOverflowError.
  Fix: Use recur for tail recursion, or use trampoline / loop.

---
id: clojure-perf-loop-recur-vs-reduce
language: clojure
severity: info
message: "Performance: loop/recur is less idiomatic than reduce for simple reductions"
tags:
  - performance
  - functions
rule:
  kind: loop
  pattern: "(loop [sum 0 items (seq coll)] ...)"
note: |
  reduce is often clearer and equally performant. Use reduce when appropriate.
  Fix: Prefer reduce for simple accumulation patterns.

---
id: clojure-perf-nested-lambdas
language: clojure
severity: info
message: "Performance: nested anonymous functions create multiple classes"
tags:
  - performance
  - functions
rule:
  kind: function_call
  pattern: "#(... #(...) ...)"
note: |
  Each lambda creates a new class. Nested lambdas have overhead.
  Fix: Extract to named functions or use fn with destructuring.

---
id: clojure-perf-partial-overhead
language: clojure
severity: info
message: "Performance: partial in hot loop creates wrapper functions"
tags:
  - performance
  - functions
rule:
  kind: function_call
  pattern: "(map (partial + 10) numbers)"
note: |
  partial creates wrapper functions. Direct closures are slightly faster.
  Fix: Use anonymous function instead of partial in performance-critical code.

---
id: clojure-perf-apply-small-args
language: clojure
severity: info
message: "Performance: apply with small argument lists has overhead"
tags:
  - performance
  - functions
rule:
  kind: function_call
  pattern: "(apply f [a b c])"
note: |
  apply has overhead for destructuring args. Direct calls are faster.
  Fix: Call function directly when args are known: (f a b c)

---
id: clojure-perf-comp-hot-path
language: clojure
severity: info
message: "Performance: comp creates wrapper, direct composition can be inlined"
tags:
  - performance
  - functions
rule:
  kind: def
  pattern: "(def process (comp f g h))"
note: |
  comp creates wrapper. Direct composition can be inlined.
  Fix: For performance-critical code, compose manually: (defn process [x] (f (g (h x))))

---
id: clojure-perf-multimethod-hot
language: clojure
severity: warning
message: "Performance: multimethod dispatch in hot path slower than protocols"
tags:
  - performance
  - multimethods
rule:
  kind: function_def
  pattern: "(defmulti process type)"
note: |
  Multimethods use hash lookup for dispatch. Protocols use vtable (faster).
  Fix: Use protocols for type-based dispatch in performance-critical code.

---
id: clojure-perf-extend-protocol-style
language: clojure
severity: info
message: "Performance: multiple extend-type calls less efficient than extend-protocol"
tags:
  - performance
  - protocols
rule:
  kind: function_call
  pattern: "(extend-type String ...)(extend-type Long ...)"
note: |
  extend-protocol is clearer and slightly more efficient.
  Fix: Use extend-protocol when extending same protocol to multiple types.

---
id: clojure-perf-protocol-object-default
language: clojure
severity: info
message: "Performance: protocol extended to Object as fallback is slowest path"
tags:
  - performance
  - protocols
rule:
  kind: function_call
  pattern: "(extend-protocol Prot Object ...)"
note: |
  Object extension works but is slowest path.
  Fix: Extend to specific types when possible.

---
id: clojure-perf-multimethod-dispatch-multi
language: clojure
severity: warning
message: "Performance: multimethod dispatch on multiple values creates vectors"
tags:
  - performance
  - multimethods
rule:
  kind: function_def
  pattern: "(defmulti f (juxt type type))"
note: |
  Multi-value dispatch creates vectors and does hash lookup.
  Fix: Consider protocols or type-specific overloads.

---
id: clojure-perf-string-concat-loop
language: clojure
severity: error
message: "Performance: string concatenation in loop creates O(n^2) complexity"
tags:
  - performance
  - strings
rule:
  kind: loop
  pattern: "(loop ... (str result new-part) ...)"
note: |
  String concatenation creates new strings. O(n^2) for repeated concat.
  Fix: Use StringBuilder or clojure.string/join.

---
id: clojure-perf-format-simple-concat
language: clojure
severity: info
message: "Performance: format parses format string, str is faster for simple cases"
tags:
  - performance
  - strings
rule:
  kind: function_call
  pattern: "(format \"%s%s\" a b)"
note: |
  format parses format string. str is faster for simple concatenation.
  Fix: Use str for simple concatenation.

---
id: clojure-perf-regex-compilation
language: clojure
severity: warning
message: "Performance: regex compilation in loop with constant pattern"
tags:
  - performance
  - strings
rule:
  kind: function_call
  pattern: "(re-find #\"...\" s)"
note: |
  Regex patterns are compiled each time they appear in code path.
  Fix: Compile pattern once with re-pattern and reuse.

---
id: clojure-perf-subs-overhead
language: clojure
severity: info
message: "Performance: substring operations create new strings"
tags:
  - performance
  - strings
rule:
  kind: function_call
  pattern: "(subs s start end)"
note: |
  subs and .substring are equivalent. Both create new strings.
  Fix: Consider using StringView patterns for heavy substring work.

---
id: clojure-perf-split-no-limit
language: clojure
severity: info
message: "Performance: split without limit can create many small strings"
tags:
  - performance
  - strings
rule:
  kind: function_call
  pattern: "(clojure.string/split s #\"...\")"
note: |
  Unlimited split can create many small strings.
  Fix: Use limit parameter when you know the expected number of parts.

---
id: clojure-perf-head-retention
language: clojure
severity: error
message: "Performance: holding head of large sequence prevents garbage collection"
tags:
  - performance
  - memory
  - lazy-sequences
rule:
  kind: let
  pattern: "(let [large-seq (range 10000000)] ...)"
note: |
  Binding head prevents GC of processed elements.
  Fix: Avoid binding head. Process in streaming fashion.

---
id: clojure-perf-large-intermediate
language: clojure
severity: warning
message: "Performance: large intermediate collections allocate at each stage"
tags:
  - performance
  - memory
  - transducers
rule:
  kind: function_call
  pattern: "(->> coll (filter) (map) (filter) (take n))"
note: |
  Lazy sequences avoid full realization, but each stage allocates.
  Fix: Use transducers to eliminate intermediate allocations.

---
id: clojure-perf-unnecessary-object-creation
language: clojure
severity: info
message: "Performance: creating maps/vectors for temporary use adds GC pressure"
tags:
  - performance
  - memory
rule:
  kind: let
  pattern: "(let [pair {:a x :b y}] ...)"
note: |
  Short-lived objects add GC pressure.
  Fix: Reuse objects when possible. Use destructuring instead of temporary maps.

---
id: clojure-perf-memoize-unbounded
language: clojure
severity: warning
message: "Performance: memoize without limit causes memory leak"
tags:
  - performance
  - memory
  - caching
rule:
  kind: def
  pattern: "(def cached-fn (memoize expensive-fn))"
note: |
  Unbounded memoization can cause memory leaks.
  Fix: Use cache libraries with size limits (core.memoize).

---
id: clojure-perf-object-pooling
language: clojure
severity: warning
message: "Performance: repeated creation of expensive objects"
tags:
  - performance
  - memory
rule:
  kind: loop
  pattern: ""
note: |
  Some objects are expensive to create (connections, parsers, etc.).
  Fix: Use object pools for reusable expensive resources.

---
id: clojure-perf-atom-high-contention
language: clojure
severity: warning
message: "Performance: atom in high-contention scenario uses CAS loop with retries"
tags:
  - performance
  - concurrency
rule:
  kind: function_call
  pattern: "(swap! counter inc)"
note: |
  Atoms use CAS loop. High contention causes retries.
  Fix: Use java.util.concurrent.atomic.AtomicLong for counters in high contention.

---
id: clojure-perf-nested-swap
language: clojure
severity: warning
message: "Performance: multiple swap! in sequence multiply retry conflicts"
tags:
  - performance
  - concurrency
rule:
  kind: function_call
  pattern: "(swap! state assoc :a 1)(swap! state assoc :b 2)"
note: |
  Each swap! can conflict. Multiple swaps multiply retries.
  Fix: Combine into single swap! when possible.

---
id: clojure-perf-ref-single-value
language: clojure
severity: info
message: "Performance: ref for single value uses STM overhead"
tags:
  - performance
  - concurrency
rule:
  kind: def
  pattern: "(def state (ref {:count 0}))"
note: |
  Refs use STM, which has overhead. Atoms are simpler for single values.
  Fix: Use atoms for single values. Refs are for coordinated updates.

---
id: clojure-perf-pmap-small
language: clojure
severity: warning
message: "Performance: pmap on small collection has thread coordination overhead"
tags:
  - performance
  - concurrency
rule:
  kind: function_call
  pattern: "(pmap f small-coll)"
note: |
  pmap has thread coordination overhead. Not worth it for small collections.
  Fix: Use regular map for small collections or cheap operations.

---
id: clojure-perf-unbounded-futures
language: clojure
severity: error
message: "Performance: unbounded future creation exhausts thread pool"
tags:
  - performance
  - concurrency
rule:
  kind: loop
  pattern: "(doseq [item items] (future (process item)))"
note: |
  Each future uses a thread. Unbounded creation exhausts thread pool.
  Fix: Use bounded thread pools or core.async channels.

---
id: clojure-perf-agent-send-blocking
language: clojure
severity: warning
message: "Performance: send for blocking operations blocks fixed thread pool"
tags:
  - performance
  - concurrency
rule:
  kind: function_call
  pattern: "(send agent (fn [_] (blocking-io-operation)))"
note: |
  send uses fixed thread pool. Blocking exhausts it.
  Fix: Use send-off for blocking I/O operations.

---
id: clojure-perf-unbuffered-io
language: clojure
severity: warning
message: "Performance: unbuffered I/O has high syscall overhead"
tags:
  - performance
  - io
rule:
  kind: function_call
  pattern: "(with-open [r (io/reader file)] ...)"
note: |
  Unbuffered I/O has high syscall overhead.
  Fix: Use buffered readers/writers.

---
id: clojure-perf-slurp-large-file
language: clojure
severity: error
message: "Performance: slurp reads entire file into memory"
tags:
  - performance
  - io
rule:
  kind: def
  pattern: "(def contents (slurp \"huge-file.txt\"))"
note: |
  slurp reads entire file into memory. Fails for large files.
  Fix: Use line-seq or BufferedReader for streaming processing.

---
id: clojure-perf-json-no-streaming
language: clojure
severity: error
message: "Performance: JSON parsing without streaming doubles memory usage"
tags:
  - performance
  - io
rule:
  kind: function_call
  pattern: "(json/parse-string (slurp large-file))"
note: |
  Loading entire JSON into memory before parsing doubles memory usage.
  Fix: Use streaming JSON parser.

---
id: clojure-perf-repeated-file-opens
language: clojure
severity: warning
message: "Performance: repeated file opens in loop has syscall overhead"
tags:
  - performance
  - io
rule:
  kind: loop
  pattern: "(doseq [line lines] (spit file line :append true))"
note: |
  Opening files has overhead. Open once for multiple operations.
  Fix: Open once, perform all operations, then close.

---
id: clojure-perf-heavy-computation-load
language: clojure
severity: warning
message: "Performance: heavy computation at namespace load time slows startup"
tags:
  - performance
  - compilation
rule:
  kind: def
  pattern: "(def expensive-data (compute-expensive-thing))"
note: |
  Top-level computations run at load time, slowing startup.
  Fix: Use delay or memoize for lazy initialization.

---
id: clojure-perf-excessive-requires
language: clojure
severity: info
message: "Performance: unnecessary requires slow startup"
tags:
  - performance
  - compilation
rule:
  kind: namespace
  pattern: "(:require [lib :refer :all])"
note: |
  Loading unnecessary namespaces slows startup.
  Fix: Only require what's needed. Use specific :refer.

---
id: clojure-perf-no-aot
language: clojure
severity: warning
message: "Performance: runtime compilation of performance-critical code adds latency"
tags:
  - performance
  - compilation
rule:
  kind: build_config
  pattern: ""
note: |
  JIT compilation during startup adds latency.
  Fix: Use AOT compilation for faster startup.

---
id: clojure-perf-definline
language: clojure
severity: info
message: "Performance: small frequently-called functions can use definline"
tags:
  - performance
  - compilation
rule:
  kind: function_def
  pattern: ""
note: |
  Function call overhead matters for very small functions.
  Fix: Use definline for trivial functions in hot paths.

---
id: clojure-perf-simple-timing
language: clojure
severity: warning
message: "Performance: simple timing benchmark doesn't account for JIT/GC"
tags:
  - performance
  - testing
rule:
  kind: function_call
  pattern: "(time (dotimes [_ 1000] (my-fn)))"
note: |
  Simple timing doesn't account for JIT warmup, GC, etc.
  Fix: Use Criterium for accurate benchmarks.

---
id: clojure-perf-optimize-without-profile
language: clojure
severity: warning
message: "Performance: micro-optimizing without profiling wastes effort"
tags:
  - performance
  - testing
rule:
  kind: pattern
  pattern: ""
note: |
  Optimizing wrong code wastes effort. Profile first.
  Fix: Use profilers (VisualVM, YourKit, clj-async-profiler).

---
id: clojure-perf-deep-destructuring
language: clojure
severity: info
message: "Performance: deep destructuring in hot path creates intermediate structures"
tags:
  - performance
  - functions
rule:
  kind: function_def
  pattern: "(defn process [[[a b] c] d] ...)"
note: |
  Deep destructuring creates intermediate structures.
  Fix: Use direct access for performance-critical code.

---
id: clojure-perf-map-destruct-with-defaults
language: clojure
severity: info
message: "Performance: map destructuring with defaults creates map in hot path"
tags:
  - performance
  - functions
rule:
  kind: function_def
  pattern: "(defn process [{:keys [a b] :or {a 0 b 0}}] ...)"
note: |
  :or creates map and does merge-like operation.
  Fix: Use get with default for single keys in hot path.
---
id: lua-perf-global-variable-access-in-hot-path
language: lua
severity: warning
message: "Performance: Global variable access in hot path requires hash table lookup; cache as local"
tags:
  - performance
  - variable-scope
  - hot-path
rule:
  kind: identifier
  pattern: "[a-zA-Z_][a-zA-Z0-9_]*"
note: |
  Global variable lookup through _G is much slower than local access (10-30% slower).
  Cache globals as locals before loops or frequently-called functions.
  Example: local print = print before loop.

---
id: lua-perf-missing-local-declaration
language: lua
severity: warning
message: "Performance: Variable assigned without 'local' keyword becomes global"
tags:
  - performance
  - variable-scope
  - globals
rule:
  kind: assignment
  pattern: "(?<!local\\s)\\b[a-zA-Z_][a-zA-Z0-9_]*\\s*="
note: |
  Undeclared variables become globals, causing slower access (20-40% per access) and polluting global namespace.
  Always use 'local' for function-scoped variables.
  Example: local result = 0 instead of result = 0

---
id: lua-perf-uncached-module-functions
language: lua
severity: warning
message: "Performance: Module function called repeatedly without local caching"
tags:
  - performance
  - module-calls
  - caching
rule:
  kind: call
  pattern: "\\b[a-zA-Z_][a-zA-Z0-9_]*\\.[a-zA-Z_][a-zA-Z0-9_]*\\s*\\("
note: |
  Module table lookup + function lookup on each call. Causes 15-25% overhead per call.
  Cache module functions locally: local sin = math.sin before using in loops.
  Two table lookups per call become one local lookup.

---
id: lua-perf-repeated-require-calls
language: lua
severity: info
message: "Performance: 'require()' called inside function instead of at module level"
tags:
  - performance
  - module-loading
rule:
  kind: call
  pattern: "\\brequire\\s*\\("
note: |
  Repeated require() has overhead even with caching. Require at module level once.
  Minor but adds up in hot paths. Cache result in local variable once.

---
id: lua-perf-variable-declared-in-loop
language: lua
severity: info
message: "Performance: Variable declared in loop body with constant value"
tags:
  - performance
  - loop-optimization
rule:
  kind: statement
  pattern: "for\\s+.*\\s+do.*local\\s+[a-zA-Z_][a-zA-Z0-9_]*\\s*="
note: |
  Unnecessary allocation in each iteration when value doesn't depend on loop variable.
  Hoist constant declarations outside loop to avoid recreation every iteration.
  Minor per iteration but accumulates.

---
id: lua-perf-table-pre-allocation-missing
language: lua
severity: warning
message: "Performance: Large table built incrementally without pre-allocation"
tags:
  - performance
  - table-optimization
  - memory
rule:
  kind: statement
  pattern: "local\\s+\\w+\\s*=\\s*\\{\\}.*\\[.*\\]\\s*="
note: |
  Tables grow dynamically causing multiple reallocations and rehashing (2-5x slower).
  Pre-allocate using table.new (LuaJIT) or table constructor for large tables.
  For LuaJIT: local t = table.new(size, 0)

---
id: lua-perf-table-insert-in-hot-path
language: lua
severity: warning
message: "Performance: table.insert() called inside performance-critical loop"
tags:
  - performance
  - table-optimization
  - function-calls
rule:
  kind: call
  pattern: "table\\.insert\\s*\\("
note: |
  table.insert is function call with length calculation overhead (2-3x slower).
  Use direct indexing with counter instead: n = n + 1; results[n] = value
  Saves function call overhead and length recalculation.

---
id: lua-perf-unnecessary-table-copy
language: lua
severity: warning
message: "Performance: Table copied when reference would suffice"
tags:
  - performance
  - table-optimization
  - memory
rule:
  kind: statement
  pattern: "local\\s+\\w+\\s*=\\s*\\{\\}.*for.*in\\s+pairs"
note: |
  Deep or shallow copy is expensive. Pass by reference when modification isn't needed (O(n) saved).
  Only copy if function modifies the table. Otherwise pass original.

---
id: lua-perf-pairs-for-array-iteration
language: lua
severity: warning
message: "Performance: pairs() used for sequential array iteration"
tags:
  - performance
  - table-iteration
rule:
  kind: statement
  pattern: "for\\s+.*\\s+in\\s+pairs\\s*\\("
note: |
  pairs() uses hash table iteration; ipairs() or numeric for is faster for arrays (2-4x improvement).
  Use numeric for loop: for i = 1, #array do instead.
  Enables JIT optimization and avoids hash iteration overhead.

---
id: lua-perf-ipairs-vs-numeric-for
language: lua
severity: info
message: "Performance: ipairs() used for simple array iteration"
tags:
  - performance
  - table-iteration
  - optimization
rule:
  kind: statement
  pattern: "for\\s+.*\\s+in\\s+ipairs\\s*\\("
note: |
  ipairs() creates iterator; numeric for is slightly faster (10-20% improvement).
  Use: for i = 1, #array do; local v = array[i]
  Direct indexing is optimal and traces well in JIT.

---
id: lua-perf-sparse-array-length-operator
language: lua
severity: warning
message: "Performance: Length operator # used on sparse array"
tags:
  - performance
  - table-optimization
  - correctness
rule:
  kind: operator
  pattern: "#"
note: |
  Length operator undefined for sparse arrays; may return incorrect value.
  Track length explicitly with variable or use table.maxn (deprecated).
  This is correctness issue more than performance issue.

---
id: lua-perf-table-creation-in-loop
language: lua
severity: warning
message: "Performance: Table literal {} created inside loop"
tags:
  - performance
  - table-optimization
  - memory
rule:
  kind: statement
  pattern: "for\\s+.*\\s+do.*\\{.*\\}"
note: |
  Each iteration allocates new table, increasing GC pressure significantly.
  Reuse table if possible or use separate arrays.
  Major GC reduction by moving table allocation outside loop.

---
id: lua-perf-next-for-table-emptiness-check
language: lua
severity: info
message: "Performance: Iterating table to check if empty instead of using next()"
tags:
  - performance
  - table-operations
rule:
  kind: statement
  pattern: "for\\s+.*\\s+in\\s+pairs\\s*\\(\\s*\\w+\\s*\\)\\s+do\\s+return"
note: |
  next(t) == nil is O(1) check; iteration is O(n) worst case.
  Use: if next(t) == nil then for empty check.
  Eliminates iteration overhead completely.

---
id: lua-perf-table-remove-from-beginning
language: lua
severity: warning
message: "Performance: table.remove() used to remove first element (queue operation)"
tags:
  - performance
  - table-optimization
  - algorithm
rule:
  kind: call
  pattern: "table\\.remove\\s*\\(\\s*\\w+\\s*,\\s*1\\s*\\)"
note: |
  Removing first element requires shifting all elements (O(n) per remove).
  Use circular buffer with head pointer: head = 1; while head <= #queue
  Converts O(n) to O(1) per dequeue operation.

---
id: lua-perf-unpack-large-tables
language: lua
severity: warning
message: "Performance: unpack() or table.unpack() on large arrays"
tags:
  - performance
  - table-operations
  - stack-safety
rule:
  kind: call
  pattern: "(table\\.)?unpack\\s*\\("
note: |
  Unpacking pushes all values onto stack; limited by stack size (~8000 elements).
  Pass table directly or iterate instead.
  Prevents stack overflow and improves performance.

---
id: lua-perf-string-concatenation-in-loop
language: lua
severity: critical
message: "Performance: String concatenation with .. in loop (O(n²) complexity)"
tags:
  - performance
  - string-optimization
  - critical
rule:
  kind: operator
  pattern: "\\.\\.\\s*(?!\".*\\1)"
note: |
  String immutability means each concatenation creates new string (O(n²) complexity).
  100x-1000x improvement: Use table.concat() instead.
  Collect strings in table, then: table.concat(parts)
  This is mandatory optimization for any string loop.

---
id: lua-perf-multiple-concatenation-operators
language: lua
severity: warning
message: "Performance: Multiple concatenation operators in one expression"
tags:
  - performance
  - string-optimization
rule:
  kind: expression
  pattern: "\".*\"\\s*\\.\\.*\\s*\".*\"\\s*\\.\\.*"
note: |
  Creates intermediate strings for each .. operator.
  Use string.format() for multiple values: string.format("User %s at %s", name, time)
  Single allocation instead of multiple intermediate strings.

---
id: lua-perf-repeated-string-pattern-compilation
language: lua
severity: warning
message: "Performance: Same string pattern used in multiple matching calls"
tags:
  - performance
  - string-optimization
  - pattern-matching
rule:
  kind: call
  pattern: "string\\.(match|find|gsub)\\s*\\(.*,\\s*['\"].*['\"]"
note: |
  Pattern recompiled each call (depends on implementation).
  Most Lua caches patterns. For complex patterns in LuaJIT, consider lpeg.
  Can precompile patterns if same pattern used repeatedly.

---
id: lua-perf-string-sub-vs-pattern-match
language: lua
severity: info
message: "Performance: string.match() used for fixed prefix/position check"
tags:
  - performance
  - string-optimization
rule:
  kind: call
  pattern: "string\\.match\\s*\\(.*,\\s*['\"]\\^"
note: |
  string.sub is faster for fixed-position extractions.
  Use string.sub(s, 1, 3) instead of string.match(s, "^...")
  Minor per call but adds up with repeated use.

---
id: lua-perf-tostring-in-concatenation
language: lua
severity: info
message: "Performance: Explicit tostring() when .. operator handles conversion"
tags:
  - performance
  - string-optimization
rule:
  kind: call
  pattern: "tostring\\s*\\("
note: |
  .. operator automatically converts numbers; explicit tostring() is redundant.
  Use: "Count: " .. count instead of "Count: " .. tostring(count)
  Minor overhead, cleaner code.

---
id: lua-perf-string-gmatch-creating-closure
language: lua
severity: info
message: "Performance: string.gmatch in tight loop creates closures"
tags:
  - performance
  - string-optimization
  - closures
rule:
  kind: call
  pattern: "string\\.gmatch\\s*\\("
note: |
  Each gmatch creates iterator closure.
  Consider pre-splitting if text is constant.
  Collect results once: words[#words+1] = word, then reuse.

---
id: lua-perf-string-length-in-loop-condition
language: lua
severity: info
message: "Performance: String length or string:len() in loop condition"
tags:
  - performance
  - loop-optimization
rule:
  kind: statement
  pattern: "for\\s+.*\\s+do\\s+.*#\\w+"
note: |
  Lua strings store length (O(1)). For userdata/metatabled objects, cache length.
  Cache if length computation is expensive: local len = #custom_object
  Generally not issue for native strings.

---
id: lua-perf-closure-creation-in-loop
language: lua
severity: warning
message: "Performance: Function definition inside loop creating closures"
tags:
  - performance
  - closures
  - memory
rule:
  kind: statement
  pattern: "for\\s+.*\\s+do.*function\\s*\\(|local\\s+\\w+\\s*=\\s*function"
note: |
  Each iteration creates new closure, increasing memory and GC pressure significantly.
  Define outside loop or use factory function.
  Major for high iteration counts; reduces allocations.

---
id: lua-perf-tail-call-not-used
language: lua
severity: warning
message: "Performance: Return statement not tail call (stack grows)"
tags:
  - performance
  - recursion
  - optimization
rule:
  kind: statement
  pattern: "return\\s+\\w+\\s*[*+/-]|return\\s+\\d+\\s*[*]"
note: |
  Lua optimizes tail calls to avoid stack growth. Non-tail returns miss this (O(n) stack).
  Proper tail call: return factorial(n-1, n*acc)
  Not tail call: return 1 * factorial(n-1, acc)

---
id: lua-perf-vararg-overhead
language: lua
severity: info
message: "Performance: Varargs (...) with select() overhead in hot path"
tags:
  - performance
  - function-parameters
rule:
  kind: parameter
  pattern: "\\.\\.\\."
note: |
  Vararg handling has overhead; fixed parameters are faster (minor per call).
  Use table if vararg overhead matters for performance-critical code.
  Generally acceptable for non-hot paths.

---
id: lua-perf-method-call-vs-function-call
language: lua
severity: info
message: "Performance: Method call obj:method() vs function obj.method(obj)"
tags:
  - performance
  - function-calls
rule:
  kind: call
  pattern: "\\w+:\\w+\\s*\\("
note: |
  Colon syntax has implicit self lookup; slightly slower but more readable.
  Optimize only if proven bottleneck.
  Negligible performance impact; prefer readability.

---
id: lua-perf-inline-simple-functions
language: lua
severity: info
message: "Performance: Simple function calls in tight loop that could be inlined"
tags:
  - performance
  - function-calls
  - micro-optimization
rule:
  kind: call
  pattern: "\\w+\\s*\\([^)]*\\)"
note: |
  Function call overhead for trivial operations like square(x) = x*x.
  Inline trivial operations in critical paths: result + i*i instead of square(i)
  Eliminates function call overhead.

---
id: lua-perf-anonymous-function-creation
language: lua
severity: warning
message: "Performance: Anonymous function passed repeatedly to higher-order functions"
tags:
  - performance
  - closures
  - function-calls
rule:
  kind: statement
  pattern: "for\\s+.*\\s+do.*function\\s*\\(.*\\)\\s*return|\\bfunction\\s*\\([^)]*\\)\\s*return"
note: |
  Creates new closure each call, increasing allocations.
  Define comparator/function once outside loop.
  Reuse closure: local by_score = function(a,b)... for i in loop do... end

---
id: lua-perf-loop-invariant-code-motion
language: lua
severity: warning
message: "Performance: Loop-invariant computation inside loop"
tags:
  - performance
  - loop-optimization
rule:
  kind: statement
  pattern: "for\\s+.*\\s+do.*[a-zA-Z_]\\w*\\s*=.*\\(.*\\)"
note: |
  Computation inside loop that doesn't depend on loop variable wastes cycles.
  Hoist invariant computations: local factor = math.sqrt(config.base) * 2
  Proportional improvement to loop iterations.

---
id: lua-perf-nested-loop-optimization
language: lua
severity: warning
message: "Performance: Nested loop that could use hash table (O(n*m) instead of O(n+m))"
tags:
  - performance
  - algorithm-optimization
  - loop-optimization
rule:
  kind: statement
  pattern: "for\\s+.*\\s+do.*for\\s+.*\\s+do"
note: |
  O(n*m) when O(n+m) possible with hash table lookup.
  Algorithmic improvement: build lookup table first, then single loop.
  Major performance improvement for large datasets.

---
id: lua-perf-break-early-from-loop
language: lua
severity: info
message: "Performance: Loop continues after result is found"
tags:
  - performance
  - loop-optimization
rule:
  kind: statement
  pattern: "for\\s+.*\\s+do.*if.*\\bthen\\s*$|for\\s+.*\\s+do.*if.*==.*\\bthen\\s*$"
note: |
  Unnecessary iterations continue after result found.
  Add break statement: if items[i].id == target then found = true; break end
  Reduces iterations; readability improvement.

---
id: lua-perf-while-true-loop-without-optimization
language: lua
severity: info
message: "Performance: while true loop with condition check inside"
tags:
  - performance
  - loop-optimization
  - readability
rule:
  kind: statement
  pattern: "while\\s+true\\s+do.*if.*\\bbreak"
note: |
  Less clear than while condition loop. For clarity, use while item ~= nil instead.
  Performance equivalent, better readability.

---
id: lua-perf-repeat-until-vs-while
language: lua
severity: info
message: "Performance: repeat-until vs while loop choice"
tags:
  - performance
  - loop-optimization
  - correctness
rule:
  kind: statement
  pattern: "repeat\\s+.*\\s+until|while.*\\s+do"
note: |
  repeat-until executes at least once; while may not execute.
  Choose loop construct matching semantics and control flow.
  Correctness more than performance issue.

---
id: lua-perf-avoid-unnecessary-object-creation
language: lua
severity: warning
message: "Performance: Creating temporary objects that could be reused"
tags:
  - performance
  - memory
  - object-pooling
rule:
  kind: statement
  pattern: "function\\s+\\w+\\s*\\(.*\\).*local\\s+\\w+\\s*=\\s*\\{"
note: |
  Object creation triggers allocations and eventual GC. Major GC reduction by reuse.
  Move table creation outside function/loop and reuse: local velocity = {...}
  Especially in game update loops (each frame).

---
id: lua-perf-object-pool-pattern
language: lua
severity: warning
message: "Performance: Frequent allocation/deallocation of similar objects"
tags:
  - performance
  - object-pooling
  - memory
rule:
  kind: statement
  pattern: "return\\s*\\{.*\\}|table\\.remove"
note: |
  Pooling eliminates allocation overhead and GC pauses.
  Implement object pool: reuse = pool[#pool] or new_object; pool[#pool]=nil
  Major improvement for frequently-created objects.

---
id: lua-perf-incremental-gc-tuning
language: lua
severity: warning
message: "Performance: Default GC settings in real-time applications"
tags:
  - performance
  - garbage-collection
  - real-time
rule:
  kind: statement
  pattern: "collectgarbage"
note: |
  Default GC can cause pauses; tuning improves consistency.
  collectgarbage("setpause", 100); collectgarbage("setstepmul", 200)
  Smoother frame times in games/real-time apps.

---
id: lua-perf-explicit-nil-to-release-references
language: lua
severity: info
message: "Performance: Large objects not nil'd when no longer needed"
tags:
  - performance
  - memory
  - garbage-collection
rule:
  kind: statement
  pattern: "local\\s+\\w+\\s*=.*\\n\\n.*\\n"
note: |
  Objects remain in memory until variable goes out of scope.
  Explicitly nil large objects: huge_data = nil; collectgarbage("step")
  Reduces peak memory usage.

---
id: lua-perf-weak-tables-for-caches
language: lua
severity: warning
message: "Performance: Cache tables that can grow unbounded (memory leak)"
tags:
  - performance
  - memory-leak
  - caching
rule:
  kind: statement
  pattern: "local\\s+\\w+\\s*=\\s*\\{\\}.*function.*\\[.*\\]\\s*="
note: |
  Strong references prevent GC from collecting cached items.
  Use weak tables: setmetatable({}, {__mode = "v"})
  Prevents memory leaks; allows GC to collect unused cache entries.

---
id: lua-perf-luajit-ffi-struct-vs-table
language: lua
severity: warning
message: "Performance: Using tables for performance-critical data in LuaJIT"
tags:
  - performance
  - luajit
  - ffi
rule:
  kind: statement
  pattern: "local\\s+\\w+\\s*=\\s*\\{\\s*x.*y.*z"
note: |
  FFI cdata is more memory-efficient and faster for numeric data.
  Use FFI structs: ffi.new("Vec3", 1.0, 2.0, 3.0) vs {x=1, y=2, z=3}
  Major improvement for numeric-heavy code in LuaJIT.

---
id: lua-perf-avoid-string-keys-for-hot-data
language: lua
severity: warning
message: "Performance: String key lookup in performance-critical code"
tags:
  - performance
  - table-optimization
  - luajit
rule:
  kind: access
  pattern: "\\w+\\.[a-zA-Z_][a-zA-Z0-9_]*"
note: |
  String keys require hash computation; integer keys faster.
  Use array indices with constants: local HP=1; entity[HP] = entity[HP] - 10
  Less readable but faster; optimize only if bottleneck proven.

---
id: lua-perf-upvalue-count
language: lua
severity: info
message: "Performance: Closures with many upvalues"
tags:
  - performance
  - closures
rule:
  kind: statement
  pattern: "local\\s+[a-zA-Z_]\\w*,\\s*[a-zA-Z_]\\w*.*function\\s*\\(.*\\)"
note: |
  Each upvalue adds overhead to closure creation.
  Consider grouping upvalues in table if many (>5): local state = {...}
  Depends on closure creation frequency.

---
id: lua-perf-upvalue-vs-local-access
language: lua
severity: info
message: "Performance: Accessing upvalue repeatedly in tight loop"
tags:
  - performance
  - closures
  - optimization
rule:
  kind: statement
  pattern: "for\\s+.*\\s+do.*\\w+.*-- .*[Uu]pvalue"
note: |
  Upvalue access slightly slower than local (minor improvement).
  Cache upvalue in local for tight loops: local c = constant
  Minor improvement; readability trade-off.

---
id: lua-perf-coroutine-creation-in-loop
language: lua
severity: warning
message: "Performance: coroutine.create() inside loop"
tags:
  - performance
  - coroutines
rule:
  kind: call
  pattern: "coroutine\\.create\\s*\\("
note: |
  Creating coroutines is expensive; reuse if possible.
  Use generator pattern or iterators outside loop.
  Reduced allocations by creating coroutine once.

---
id: lua-perf-coroutine-vs-callback
language: lua
severity: info
message: "Performance: Coroutines for simple async operations"
tags:
  - performance
  - coroutines
  - async
rule:
  kind: statement
  pattern: "coroutine\\.yield|coroutine\\.resume"
note: |
  Coroutines have overhead; callbacks may be faster for simple cases.
  Use callbacks for simple async patterns.
  Depends on complexity and use case.

---
id: lua-perf-coroutine-wrap-vs-create
language: lua
severity: info
message: "Performance: Using coroutine.create when wrap would suffice"
tags:
  - performance
  - coroutines
  - api
rule:
  kind: call
  pattern: "coroutine\\.create\\s*\\(|coroutine\\.resume\\s*\\("
note: |
  coroutine.wrap returns function directly; simpler for iteration.
  Use wrap: local next = coroutine.wrap(generator)
  Readability improvement; performance equivalent.

---
id: lua-perf-trace-abort-from-nyi
language: lua
severity: warning
message: "Performance: Using features not implemented in LuaJIT traces"
tags:
  - performance
  - luajit
  - nyi
rule:
  kind: statement
  pattern: "for\\s+.*\\s+in\\s+next"
note: |
  NYI (Not Yet Implemented) features abort trace compilation.
  Order of magnitude difference: use pairs() or numeric for instead.
  Check LuaJIT NYI list for problematic patterns.

---
id: lua-perf-avoid-pairs-on-array-luajit
language: lua
severity: warning
message: "Performance: Using pairs() on sequential arrays in LuaJIT"
tags:
  - performance
  - luajit
  - table-iteration
rule:
  kind: statement
  pattern: "for\\s+.*\\s+in\\s+pairs\\s*\\(\\s*\\w+\\s*\\)"
note: |
  LuaJIT optimizes ipairs() and numeric for better than pairs() for arrays (2-5x).
  Use numeric for loop: for i = 1, #array do
  Enables better JIT optimization and array tracing.

---
id: lua-perf-ffi-array-vs-lua-table
language: lua
severity: warning
message: "Performance: Lua tables for large numeric arrays in LuaJIT"
tags:
  - performance
  - luajit
  - ffi
  - memory
rule:
  kind: statement
  pattern: "local\\s+\\w+\\s*=\\s*\\{\\}.*for\\s+.*=.*do\\s+\\w+\\[.*\\]\\s*=.*\\d"
note: |
  FFI arrays are contiguous memory; much faster for numerics.
  Use: local data = ffi.new("double[?]", 1000000)
  Major improvement for numeric operations and memory efficiency.

---
id: lua-perf-jit-compilation-hints
language: lua
severity: info
message: "Performance: Hot functions not being JIT compiled"
tags:
  - performance
  - luajit
rule:
  kind: statement
  pattern: "function\\s+\\w+\\s*\\(.*\\)"
note: |
  LuaJIT may miss optimization opportunities if function called once.
  Warm up: for i = 1, 100 do hot_function() end to force compilation.
  Ensures JIT trace compilation for hot paths.

---
id: lua-perf-avoid-side-effect-in-loop-conditions
language: lua
severity: warning
message: "Performance: Function call with side effects in loop condition"
tags:
  - performance
  - luajit
  - loop-optimization
rule:
  kind: statement
  pattern: "while\\s+\\w+:\\w+\\s*\\(|while\\s+\\w+\\.\\w+\\s*\\("
note: |
  Method call in condition may abort traces in LuaJIT.
  Cache in local: local has_next = iterator.has_next; while has_next(iterator)
  Improves tracing and performance.

---
id: lua-perf-buffered-vs-unbuffered-io
language: lua
severity: warning
message: "Performance: Single-character or small reads/writes (unbuffered I/O)"
tags:
  - performance
  - io
rule:
  kind: call
  pattern: "file:write\\s*\\(|io\\.write\\s*\\("
note: |
  Each I/O call has syscall overhead (10-100x improvement possible).
  Buffer output: collect in table, write in larger chunks with table.concat()
  Major improvement for many small writes.

---
id: lua-perf-read-entire-file-at-once
language: lua
severity: warning
message: "Performance: Line-by-line reading when bulk read possible"
tags:
  - performance
  - io
rule:
  kind: call
  pattern: "file:lines\\s*\\(|io\\.lines\\s*\\("
note: |
  Multiple syscalls vs one: *a reads entire file in one syscall.
  Use: local content = file:read("*a"); then parse with gmatch/gsub()
  Significant improvement for many files.

---
id: lua-perf-io-open-caching
language: lua
severity: info
message: "Performance: Repeatedly opening same file"
tags:
  - performance
  - io
  - caching
rule:
  kind: call
  pattern: "io\\.open\\s*\\(.*['\"][^'\"]*['\"]"
note: |
  Opening file has overhead; cache handle if reading multiple times.
  Open once, read once, cache content for reuse.
  Major if repeated; minor for single access.

---
id: lua-perf-greedy-vs-non-greedy-patterns
language: lua
severity: info
message: "Performance: Greedy .* pattern when non-greedy .­- might be better"
tags:
  - performance
  - pattern-matching
rule:
  kind: pattern
  pattern: "\\.\\*"
note: |
  Greedy matching backtracks; can be slow on long strings.
  Use non-greedy when appropriate: string.match(s, 'start(.-)end')
  Depends on string and pattern; minor per match.

---
id: lua-perf-anchor-patterns-when-possible
language: lua
severity: info
message: "Performance: Pattern without ^ or $ when position is known"
tags:
  - performance
  - pattern-matching
rule:
  kind: pattern
  pattern: "string\\.(match|find)\\s*\\(.*,['\"](?!\\^|\\$)"
note: |
  Anchored patterns are faster when position known.
  Use anchors: string.match(line, "^ERROR") instead of "ERROR"
  Minor per match but helps with optimization.

---
id: lua-perf-pre-validate-before-pattern-match
language: lua
severity: info
message: "Performance: Complex pattern match without preliminary checks"
tags:
  - performance
  - pattern-matching
  - optimization
rule:
  kind: statement
  pattern: "if\\s+string\\.match"
note: |
  Simple checks eliminate need for expensive pattern match.
  Quick rejection first: if line:find("function", 1, true) then complex_match()
  Reduces pattern matches; short-circuit optimization.

---
id: lua-perf-integer-vs-float
language: lua
severity: info
message: "Performance: Float operations when integers would suffice (Lua 5.3+)"
tags:
  - performance
  - numeric-optimization
rule:
  kind: operator
  pattern: "[0-9]+\\s*/\\s*[0-9]+"
note: |
  Lua 5.3+ has native integers; faster for integer math.
  Use // for integer division: local result = n // 2
  Minor; integer ops faster but difference small.

---
id: lua-perf-avoid-math-floor-ceil-simple-cases
language: lua
severity: info
message: "Performance: math.floor/ceil for simple rounding (Lua 5.3+)"
tags:
  - performance
  - numeric-optimization
  - function-calls
rule:
  kind: call
  pattern: "math\\.floor\\s*\\(.*\\+.*0\\.5"
note: |
  Function call overhead for simple rounding.
  Use integer conversion (Lua 5.3+): local rounded = (x + 0.5) // 1
  Minor per call; cleaner in modern Lua.

---
id: lua-perf-lookup-table-vs-computation
language: lua
severity: warning
message: "Performance: Repeated complex computation of same values"
tags:
  - performance
  - lookup-tables
  - optimization
rule:
  kind: statement
  pattern: "math\\.(sin|cos|sqrt|pow)\\s*\\("
note: |
  Lookup is O(1); computation may be O(n) or worse.
  Precompute lookup table if range limited: sin_lookup[angle] = math.sin(angle)
  Significant for transcendentals; trade memory for speed.

---
id: lua-perf-strength-reduction
language: lua
severity: info
message: "Performance: Expensive operations that could be simpler"
tags:
  - performance
  - optimization
rule:
  kind: operator
  pattern: "\\^\\s*2|\\s*/\\s*2"
note: |
  Replace expensive operations with equivalent cheaper ones.
  Use: x * x instead of x ^ 2; x * 0.5 instead of x / 2
  Minor per operation; adds up in loops.

---
id: lua-perf-lazy-module-loading
language: lua
severity: info
message: "Performance: Loading all modules at startup"
tags:
  - performance
  - module-loading
rule:
  kind: call
  pattern: "^\\s*local\\s+\\w+\\s*=\\s*require\\s*\\("
note: |
  Modules loaded only when needed reduce startup time.
  Lazy load: json = json or require("json") on first use.
  Faster startup; trade for first-use latency.

---
id: lua-perf-module-return-pattern
language: lua
severity: info
message: "Performance: Using deprecated module() function (Lua 5.1)"
tags:
  - performance
  - module-loading
rule:
  kind: call
  pattern: "^\\s*module\\s*\\("
note: |
  Deprecated module() has overhead and side effects.
  Use return table pattern: local M = {}; ... return M
  Cleaner, faster, no global pollution.
---
id: docker-perf-no-multistage
language: dockerfile
severity: warning
message: "Performance: Multi-stage build not used - unnecessary build tools in final image"
tags:
  - performance
  - docker
  - image-size
rule:
  kind: Dockerfile
  pattern: "FROM .* RUN apt-get install .*(build-essential|gcc|g\\+\\+|make) .* COPY . . RUN .*(npm|pip|go) .* CMD"
note: |
  Build tools and dependencies bloat the final image, increasing pull time, storage, and attack surface.
  Use multi-stage builds: compile in a builder stage, copy only artifacts to production stage.
  Example: FROM node:20 AS builder → FROM node:20-slim → COPY --from=builder

---
id: docker-perf-layer-caching
language: dockerfile
severity: warning
message: "Performance: Layer caching not optimized - COPY before package install"
tags:
  - performance
  - docker
  - caching
rule:
  kind: Dockerfile
  pattern: "COPY . . RUN (npm|pip|apt-get)"
note: |
  Copying source code before dependency installation invalidates the dependency cache layer on every change.
  Move COPY of package files (package.json, requirements.txt) BEFORE RUN install commands.
  Reorder: COPY package.json → RUN npm install → COPY source code.

---
id: docker-perf-no-dockerignore
language: dockerfile
severity: info
message: "Performance: Missing .dockerignore file"
tags:
  - performance
  - docker
  - build-context
rule:
  kind: Dockerfile
  pattern: "^$"
note: |
  Without .dockerignore, the entire directory is sent as build context, including node_modules, .git, and temporary files.
  Create .dockerignore to exclude: .git, node_modules, .env*, *.md, __pycache__, .pytest_cache, dist, build, coverage.
  Reduces build context size and speeds up image builds.

---
id: docker-perf-large-base
language: dockerfile
severity: warning
message: "Performance: Large base image used instead of slim/alpine variant"
tags:
  - performance
  - docker
  - image-size
rule:
  kind: Dockerfile
  pattern: "FROM (ubuntu|python|node|openjdk):[0-9\\.]+(\\s|$|\\n)"
note: |
  Full OS images are significantly larger (ubuntu:22.04 ~77MB, python:3.11 ~1GB) compared to slim/alpine.
  Use slim variants: python:3.11-slim (~150MB), node:20-alpine (~180MB), eclipse-temurin:17-jre-alpine (~180MB).
  Alpine images provide 5-10x size reduction with minimal functionality loss.

---
id: docker-perf-add-vs-copy
language: dockerfile
severity: info
message: "Performance: ADD used for file copying instead of COPY"
tags:
  - performance
  - docker
  - best-practices
rule:
  kind: Dockerfile
  pattern: "ADD \\./[^\\s]+ "
note: |
  ADD has extra features (URL fetching, auto-extraction) that add overhead when not needed.
  Use COPY for regular file operations; ADD only for tar/gzip extraction.
  COPY is simpler, more predictable, and performs better.

---
id: docker-perf-run-consolidation
language: dockerfile
severity: warning
message: "Performance: Multiple consecutive RUN commands should be consolidated"
tags:
  - performance
  - docker
  - layers
rule:
  kind: Dockerfile
  pattern: "RUN .* \\n RUN .* \\n RUN"
note: |
  Each RUN command creates a separate layer, increasing image size and complexity.
  Consolidate related commands using && and line continuations.
  Example: RUN apt-get update && apt-get install -y package && apt-get clean && rm -rf /var/lib/apt/lists/*.

---
id: docker-perf-cache-cleanup
language: dockerfile
severity: warning
message: "Performance: Package manager cache not cleaned after installation"
tags:
  - performance
  - docker
  - image-size
rule:
  kind: Dockerfile
  pattern: "RUN (apt-get install|yum install|apk add)([^&]*?)(?!.*rm -rf|.*apk --no-cache)"
note: |
  Package manager caches add significant size without runtime value.
  Always clean after installation: apt: rm -rf /var/lib/apt/lists/*, yum: yum clean all, apk: use --no-cache.
  Reduces layer size by 50-90% for dependency layers.

---
id: docker-perf-unnecessary-packages
language: dockerfile
severity: info
message: "Performance: Unnecessary recommended packages or dev dependencies installed"
tags:
  - performance
  - docker
  - dependencies
rule:
  kind: Dockerfile
  pattern: "RUN (apt-get install|npm install|pip install)([^-]*?)(?!.*--no-install-recommends|.*--only=production|.*--no-cache-dir)"
note: |
  Package managers install recommended packages by default, and npm/pip include dev dependencies.
  Use flags: apt: --no-install-recommends, npm: --only=production, pip: --no-cache-dir.
  Reduces image size and decreases attack surface.

---
id: docker-perf-no-buildkit
language: dockerfile
severity: info
message: "Performance: BuildKit not enabled (missing syntax directive)"
tags:
  - performance
  - docker
  - buildkit
rule:
  kind: Dockerfile
  pattern: "^(FROM|RUN|COPY)"
note: |
  BuildKit provides faster builds, improved caching, native secrets support, and cache mount features.
  Add as first line: # syntax=docker/dockerfile:1
  Enable via DOCKER_BUILDKIT=1 or default in Docker 25.0+.

---
id: docker-perf-unpinned-deps
language: dockerfile
severity: warning
message: "Performance: Package versions not pinned - reduces reproducibility and cache efficiency"
tags:
  - performance
  - docker
  - reproducibility
rule:
  kind: Dockerfile
  pattern: "RUN (pip install|npm install|apt-get install) [a-zA-Z0-9_-]+(?![=:@~])"
note: |
  Unpinned versions break build reproducibility and invalidate caches on version changes.
  Use lockfiles: pip (requirements.txt with versions), npm (package-lock.json, npm ci).
  Or pin versions: pip install flask==2.3.0, apt-get install nginx=1.24.0-1.

---
id: docker-perf-shell-form
language: dockerfile
severity: info
message: "Performance: ENTRYPOINT/CMD using shell form (inefficient)"
tags:
  - performance
  - docker
  - signals
rule:
  kind: Dockerfile
  pattern: "(ENTRYPOINT|CMD) [a-z]+ "
note: |
  Shell form (ENTRYPOINT python app.py) runs via /bin/sh -c, adding overhead and preventing signal propagation.
  Use exec form: ENTRYPOINT ["python", "app.py"] or CMD ["node", "server.js"].
  Enables proper Ctrl+C handling and faster startup.

---
id: docker-perf-large-context
language: dockerfile
severity: warning
message: "Performance: Large build context slows down builds"
tags:
  - performance
  - docker
  - build-context
rule:
  kind: Dockerfile
  pattern: "^$"
note: |
  Build context > 100MB is slow, especially for remote Docker daemons.
  Create comprehensive .dockerignore to exclude: .git, node_modules, coverage, test files, .env.
  Monitor context size: docker build --progress=plain shows context compression size.

---
id: docker-perf-workdir
language: dockerfile
severity: info
message: "Performance: Using cd in RUN instead of WORKDIR for persistent directory"
tags:
  - performance
  - docker
  - best-practices
rule:
  kind: Dockerfile
  pattern: "RUN cd [^ ]+ && "
note: |
  WORKDIR sets the directory persistently for all subsequent commands.
  Using cd in RUN only affects that single RUN layer, requiring re-entry in subsequent commands.
  Replace: RUN cd /app && command → WORKDIR /app followed by RUN command.

---
id: docker-perf-apt-versions
language: dockerfile
severity: info
message: "Performance: apt-get packages without version pinning"
tags:
  - performance
  - docker
  - reproducibility
rule:
  kind: Dockerfile
  pattern: "RUN apt-get install [^=]*[a-z](?![=:])"
note: |
  Unpinned apt packages reduce reproducibility and cause cache invalidation.
  Pin versions: RUN apt-get install nginx=1.24.0-1ubuntu1 libssl-dev=3.0.
  Ensures deterministic builds across environments.

---
id: docker-perf-cache-mounts
language: dockerfile
severity: info
message: "Performance: BuildKit cache mounts not used for package managers"
tags:
  - performance
  - docker
  - buildkit
rule:
  kind: Dockerfile
  pattern: "RUN (pip install|npm ci) "
note: |
  BuildKit cache mounts persist package manager caches between builds, speeding up dependencies.
  Enable with: # syntax=docker/dockerfile:1
  Use: RUN --mount=type=cache,target=/root/.cache/pip pip install -r requirements.txt
  Or: RUN --mount=type=cache,target=/root/.npm npm ci

---
id: docker-perf-individual-copy
language: dockerfile
severity: info
message: "Performance: Copying files individually instead of using globs"
tags:
  - performance
  - docker
  - layers
rule:
  kind: Dockerfile
  pattern: "COPY [^ ]+ [^ ]+\\nCOPY [^ ]+ [^ ]+"
note: |
  Multiple COPY commands create multiple layers.
  Combine related files: COPY file1.js file2.js file3.js /app/
  Or use directories: COPY src/ /app/src/

---
id: docker-perf-no-parallel
language: dockerfile
severity: info
message: "Performance: Multi-stage build not leveraging parallel execution"
tags:
  - performance
  - docker
  - buildkit
rule:
  kind: Dockerfile
  pattern: "FROM .* AS stage1"
note: |
  BuildKit can run independent stages in parallel.
  Structure stages so they don't depend on each other when possible.
  Example: builder and test stages can run simultaneously before final stage.

---
id: docker-perf-large-files
language: dockerfile
severity: warning
message: "Performance: Large files added then removed in different layers (bloats image)"
tags:
  - performance
  - docker
  - image-size
rule:
  kind: Dockerfile
  pattern: "(COPY|ADD) .* [^ ]+\\n.*RUN .* rm -rf "
note: |
  Files copied in one layer and removed in another still exist in earlier layers.
  Combine operations: RUN curl -O installer.run && ./installer.run && rm installer.run.
  Keeps layer size minimal.

---
id: tf-perf-count-vs-foreach
language: terraform
severity: warning
message: "Performance: Using count when for_each is more appropriate (causes unnecessary recreation)"
tags:
  - performance
  - terraform
  - resource-management
rule:
  kind: TerraformResource
  pattern: "count = length\\(var\\."
note: |
  count uses numeric indices; removing/reordering items causes all subsequent resources to be recreated.
  for_each uses stable keys, allowing safe modifications without cascading recreations.
  Migration: change count = length(var.list) to for_each = toset(var.list) and update references from count.index to each.key.

---
id: tf-perf-depends-on-overuse
language: terraform
severity: info
message: "Performance: Explicit depends_on blocking parallelism (implicit dependencies preferred)"
tags:
  - performance
  - terraform
  - dependencies
rule:
  kind: TerraformResource
  pattern: "depends_on = \\["
note: |
  Explicit depends_on constraints reduce parallelism during planning and application.
  Terraform automatically infers dependencies from resource references (implicit).
  Remove unnecessary depends_on; keep only for non-reference dependencies.

---
id: tf-perf-no-data-sources
language: terraform
severity: info
message: "Performance: Hardcoding values that should use data sources"
tags:
  - performance
  - terraform
  - maintainability
rule:
  kind: TerraformResource
  pattern: "(ami|subnet_id|security_group_id|vpc_id) = \"(ami|subnet|sg|vpc)-[a-z0-9]+\""
note: |
  Hardcoded resource IDs reduce flexibility and maintainability.
  Use data sources to dynamically look up values: data "aws_ami" "ubuntu" { ... }
  Enables code reuse across environments and reduces manual updates.

---
id: tf-perf-large-state
language: terraform
severity: warning
message: "Performance: Single state file managing too many resources (> 500) slows planning"
tags:
  - performance
  - terraform
  - state-management
rule:
  kind: TerraformBackend
  pattern: "^$"
note: |
  Large state files (>10MB or >500 resources) increase plan/apply time and lock contention.
  Split into logical workspaces or use separate state files for different environments/components.
  Use remote state with locking to enable team collaboration safely.

---
id: tf-perf-no-modules
language: terraform
severity: info
message: "Performance: Repeated resource patterns without modules"
tags:
  - performance
  - terraform
  - code-reuse
rule:
  kind: TerraformResource
  pattern: "resource \"(aws_instance|aws_security_group|aws_subnet)\" \"[^\"]+\" \\{[^}]+\\}.*resource \"(aws_instance|aws_security_group|aws_subnet)\" \"[^\"]+\" \\{[^}]+\\}"
note: |
  Repeating resource patterns without modules increases maintenance burden and reduces consistency.
  Extract patterns into reusable modules: module "web_server" { source = "./modules/web-server" for_each = ... }
  Modules enable consistent, DRY infrastructure definitions.

---
id: tf-perf-unpinned-provider
language: terraform
severity: warning
message: "Performance: Provider version not pinned - causes inconsistent plans"
tags:
  - performance
  - terraform
  - reproducibility
rule:
  kind: TerraformBlock
  pattern: "required_providers \\{[^}]*source = \"[^\"]+\"[^}]*\\}"
note: |
  Unpinned providers allow automatic updates that may contain breaking changes.
  Pin versions: version = "~> 5.0" (allows patch updates) or "= 5.27.0" (exact).
  Commit .terraform.lock.hcl to ensure consistent versions across team.

---
id: tf-perf-local-state
language: terraform
severity: warning
message: "Performance: Using local state for team projects (no locking, no collaboration)"
tags:
  - performance
  - terraform
  - state-management
rule:
  kind: TerraformBackend
  pattern: "^$"
note: |
  Local state files don't support team collaboration, locking, or proper CI/CD integration.
  Configure remote backend: S3 (with DynamoDB lock), Terraform Cloud, Consul, or Azure Storage.
  Example: backend "s3" { bucket = "..." key = "..." region = "..." dynamodb_table = "..." encrypt = true }

---
id: tf-perf-refresh-on-plan
language: terraform
severity: info
message: "Performance: Unnecessary refresh during plan queries all resources (slow)"
tags:
  - performance
  - terraform
  - planning
rule:
  kind: TerraformCommand
  pattern: "terraform plan"
note: |
  terraform plan by default queries all resources, causing I/O delays for large infrastructures.
  Use -refresh=false to skip queries, -target for specific resources during iteration.
  Enable fast iteration during development with -target=module.component.resource.

---
id: tf-perf-low-parallelism
language: terraform
severity: info
message: "Performance: Default parallelism (10) may be suboptimal for large deployments"
tags:
  - performance
  - terraform
  - concurrency
rule:
  kind: TerraformCommand
  pattern: "terraform (apply|plan|destroy)"
note: |
  Terraform parallelism default (10) is conservative; independent resources can apply faster with higher parallelism.
  Increase for large deployments: terraform apply -parallelism=30
  Tune based on API rate limits and infrastructure scale.

---
id: tf-perf-nested-modules
language: terraform
severity: info
message: "Performance: Deeply nested modules (>3 levels) increase complexity"
tags:
  - performance
  - terraform
  - architecture
rule:
  kind: TerraformModule
  pattern: "module \"[^\"]+\" \\{ source = \"\\./modules/[^/]+/[^/]+/[^/]+/[^\"]+\""
note: |
  Nesting modules more than 3 levels deep increases plan time and code comprehension difficulty.
  Flatten module structure; use composition over inheritance.
  Max recommended depth: root → environment → component → module → resource.

---
id: tf-perf-repeated-data
language: terraform
severity: info
message: "Performance: Repeated data source lookups (same query multiple times)"
tags:
  - performance
  - terraform
  - api-calls
rule:
  kind: TerraformData
  pattern: "data \"(aws_caller_identity|aws_ami|aws_availability_zones)\" \"[^\"]+\" \\{.*\\}.*data \"(aws_caller_identity|aws_ami|aws_availability_zones)\" \"[^\"]+\" \\{"
note: |
  Calling the same data source multiple times wastes API calls and plan time.
  Define shared data sources at root level, pass references to modules.
  Example: data "aws_caller_identity" "current" { } → use data.aws_caller_identity.current.account_id in modules.

---
id: tf-perf-no-target
language: terraform
severity: info
message: "Performance: Full applies during development (slower iteration)"
tags:
  - performance
  - terraform
  - development
rule:
  kind: TerraformCommand
  pattern: "terraform apply"
note: |
  Full applies test all resources, causing slow iteration cycles during development.
  Use -target for single resource changes: terraform apply -target=module.web.aws_instance.server
  Faster feedback loop for development and testing.

---
id: tf-perf-large-files
language: terraform
severity: warning
message: "Performance: Large files embedded in Terraform state (>100MB)"
tags:
  - performance
  - terraform
  - state-size
rule:
  kind: TerraformResource
  pattern: "filename = \"[^\"]+\\.(zip|tar\\.gz|bin)\" #"
note: |
  Large files in state (deployment packages, binaries) bloat state file and slow operations.
  Store in S3 or artifact registry, reference via bucket/key in Terraform.
  Example: s3_bucket = aws_s3_bucket.lambda.bucket, s3_key = "functions/package.zip"

---
id: tf-perf-no-lockfile
language: terraform
severity: warning
message: "Performance: Missing .terraform.lock.hcl (inconsistent provider versions)"
tags:
  - performance
  - terraform
  - reproducibility
rule:
  kind: TerraformFile
  pattern: "^$"
note: |
  Without lock file, team members and CI/CD may use different provider versions, causing plan inconsistencies.
  Commit .terraform.lock.hcl to version control after terraform init.
  Ensures deterministic planning and prevents version-related surprises.

---
id: tf-perf-slow-provisioners
language: terraform
severity: warning
message: "Performance: Heavy use of local-exec/remote-exec provisioners (slow and unreliable)"
tags:
  - performance
  - terraform
  - best-practices
rule:
  kind: TerraformProvisioner
  pattern: "provisioner \"(local-exec|remote-exec)\" \\{"
note: |
  Provisioners run serially and outside Terraform's dependency graph, causing unpredictable failures.
  Prefer: pre-baked AMIs, user_data scripts, cloud-init, or configuration management tools.
  Use provisioners only as last resort, wrapped in on_failure = continue for best-effort.

---
id: tf-perf-no-moved
language: terraform
severity: info
message: "Performance: Resource renames cause destroy/recreate (use moved blocks)"
tags:
  - performance
  - terraform
  - refactoring
rule:
  kind: TerraformMove
  pattern: "^$"
note: |
  Renaming resources (aws_instance.old → aws_instance.new) causes destruction and recreation without moved blocks.
  Add moved block to preserve state: moved { from = aws_instance.old_name, to = aws_instance.new_name }
  Prevents unnecessary downtime and state loss during refactoring.

---
id: k8s-perf-no-requests
language: yaml
severity: warning
message: "Performance: Pod missing resource requests (scheduler can't place optimally)"
tags:
  - performance
  - kubernetes
  - resource-management
rule:
  kind: PodSpec
  pattern: "containers:.*resources:.*limits:"
note: |
  Without resource requests, Kubernetes scheduler can't make optimal placement decisions.
  Always define requests: requests: { cpu: "100m", memory: "128Mi" }
  Requests enable proper bin-packing and prevent overcommitment.

---
id: k8s-perf-no-limits
language: yaml
severity: warning
message: "Performance: Pod missing resource limits (can consume unbounded resources)"
tags:
  - performance
  - kubernetes
  - resource-management
rule:
  kind: PodSpec
  pattern: "containers:.*resources:.*requests:.*(?!limits)"
note: |
  Containers without limits can consume all node resources, affecting other pods.
  Set limits: limits: { cpu: "500m", memory: "512Mi" }
  Limits protect node stability and enable QoS guarantees.

---
id: k8s-perf-hpa-no-metrics
language: yaml
severity: warning
message: "Performance: HPA configured but target pods lack resource requests"
tags:
  - performance
  - kubernetes
  - autoscaling
rule:
  kind: HorizontalPodAutoscaler
  pattern: "metrics:.*type: Resource.*(?!.*requests)"
note: |
  HPA percentage-based scaling requires resource requests to calculate utilization percentages.
  Ensure target deployment has: resources: { requests: { cpu: "100m", memory: "128Mi" } }
  Without requests, HPA metrics are meaningless (dividing by zero).

---
id: k8s-perf-no-pdb
language: yaml
severity: warning
message: "Performance: Production deployment lacks PodDisruptionBudget (risk during upgrades)"
tags:
  - performance
  - kubernetes
  - availability
rule:
  kind: Deployment
  pattern: "replicas: [0-9]"
note: |
  High-replica deployments without PDB can become unavailable during voluntary disruptions (upgrades, drains).
  Add PDB: minAvailable: 2 or maxUnavailable: 1 matching deployment labels.
  Ensures minimum availability during cluster maintenance.

---
id: k8s-perf-bad-affinity
language: yaml
severity: info
message: "Performance: Overly restrictive node affinity reduces scheduling flexibility"
tags:
  - performance
  - kubernetes
  - scheduling
rule:
  kind: PodSpec
  pattern: "nodeAffinity:.*requiredDuringSchedulingIgnoredDuringExecution:"
note: |
  Required node affinity can cause pods to remain pending if matching nodes become unavailable.
  Prefer preferredDuringSchedulingIgnoredDuringExecution for flexibility.
  Use required only for critical constraints (specific hardware, compliance zones).

---
id: k8s-perf-no-topology-spread
language: yaml
severity: info
message: "Performance: Multi-replica deployment without topology spread constraints"
tags:
  - performance
  - kubernetes
  - scheduling
rule:
  kind: Deployment
  pattern: "replicas: [2-9]"
note: |
  Without topology spread, replicas may concentrate on single node or zone, reducing HA.
  Add: topologySpreadConstraints: [{ maxSkew: 1, topologyKey: topology.kubernetes.io/zone, ... }]
  Ensures even distribution across zones and nodes.

---
id: k8s-perf-no-antiaffinity
language: yaml
severity: info
message: "Performance: Stateful/multi-replica workload without pod anti-affinity"
tags:
  - performance
  - kubernetes
  - scheduling
rule:
  kind: StatefulSet
  pattern: "replicas: [0-9]"
note: |
  Without anti-affinity, replicas may schedule on same node, losing HA benefits.
  Add: podAntiAffinity: { preferredDuringSchedulingIgnoredDuringExecution: [{ ... }] }
  Use preferred for workloads that can survive colocation (cost vs. HA trade-off).

---
id: k8s-perf-probe-timeout
language: yaml
severity: info
message: "Performance: Liveness probe with aggressive timeouts (causes unnecessary restarts)"
tags:
  - performance
  - kubernetes
  - probes
rule:
  kind: Probe
  pattern: "livenessProbe:.*timeoutSeconds: [0-3].*failureThreshold: 1"
note: |
  Aggressive probes (timeoutSeconds=1, failureThreshold=1) restart containers during load spikes.
  Use: initialDelaySeconds=30, timeoutSeconds=5, periodSeconds=10, failureThreshold=3
  Tune based on application startup time and health check complexity.

---
id: k8s-perf-no-readiness
language: yaml
severity: warning
message: "Performance: Service-backed pod lacks readiness probe (traffic to unready pods)"
tags:
  - performance
  - kubernetes
  - probes
rule:
  kind: PodSpec
  pattern: "containers:.*livenessProbe:.*(?!.*readinessProbe)"
note: |
  Without readiness probes, traffic is sent to pods not yet ready to serve, causing errors.
  Add readiness probe with appropriate health check for application startup.
  Separate from liveness: readiness for startup readiness, liveness for crash detection.

---
id: k8s-perf-no-startup-probe
language: yaml
severity: info
message: "Performance: Slow-starting application without startup probe (high initialDelaySeconds)"
tags:
  - performance
  - kubernetes
  - probes
rule:
  kind: Probe
  pattern: "livenessProbe:.*initialDelaySeconds: [3-9][0-9]"
note: |
  High initialDelaySeconds (>30s) for slow-starting apps extends deployment time.
  Add startup probe: failureThreshold=30, periodSeconds=10 allows up to 5 minutes without affecting liveness.
  Startup probe doesn't apply after pod is running; liveness probe takes over.

---
id: k8s-perf-emptydir-no-limit
language: yaml
severity: info
message: "Performance: emptyDir volume without size limit (unbounded disk consumption)"
tags:
  - performance
  - kubernetes
  - storage
rule:
  kind: Volume
  pattern: "emptyDir: \\{\\s*\\}"
note: |
  Unbounded emptyDir can consume all node disk, causing node pressure and pod evictions.
  Add size limit: emptyDir: { sizeLimit: "1Gi" }
  Prevents runaway temporary file accumulation.

---
id: k8s-perf-emptydir-disk
language: yaml
severity: info
message: "Performance: Using disk-backed emptyDir for temporary files (memory option faster)"
tags:
  - performance
  - kubernetes
  - storage
rule:
  kind: Volume
  pattern: "emptyDir: \\{\\s*\\}"
note: |
  Disk-backed emptyDir for temporary files is slower than memory-backed.
  Use: emptyDir: { medium: Memory, sizeLimit: "100Mi" } for high-throughput temp storage.
  Trade memory for latency; suitable for caching, temp processing.

---
id: k8s-perf-always-pull
language: yaml
severity: info
message: "Performance: imagePullPolicy Always with immutable tags (wastes bandwidth)"
tags:
  - performance
  - kubernetes
  - images
rule:
  kind: Container
  pattern: "image: .+:(\\d+\\.\\d+\\.\\d+|[a-zA-Z0-9]+).*imagePullPolicy: Always"
note: |
  Always pulling immutable tags (version numbers) wastes bandwidth and slows startup.
  Use IfNotPresent for immutable tags, Always only for 'latest' (moving tags).
  Reduces startup time and registry load.

---
id: k8s-perf-large-configmap
language: yaml
severity: warning
message: "Performance: ConfigMap or Secret exceeds 1MB (slows kubelet, API server)"
tags:
  - performance
  - kubernetes
  - configuration
rule:
  kind: ConfigMap
  pattern: "data:"
note: |
  ConfigMaps/Secrets >1MB approach Kubernetes limits and slow kubelet operations.
  Split into multiple smaller ConfigMaps, or use external storage for large files.
  Keep per-container data < 10MB; reference external storage for large configs.

---
id: k8s-perf-no-local-volume
language: yaml
severity: info
message: "Performance: Data-intensive workload using network storage (local volumes faster)"
tags:
  - performance
  - kubernetes
  - storage
rule:
  kind: Pod
  pattern: "volumeMounts:.*persistentVolumeClaim"
note: |
  Network storage (EBS, GCE persistent disks) provides lower IOPS than local SSDs.
  Use local volumes for data-intensive workloads: volumeSource: { local: { path: "/mnt/data" } }
  Add topology constraints to ensure pod-to-volume affinity.

---
id: k8s-perf-no-priority
language: yaml
severity: info
message: "Performance: Critical pods lack priorityClassName (can't preempt less important work)"
tags:
  - performance
  - kubernetes
  - scheduling
rule:
  kind: Pod
  pattern: "^$"
note: |
  Critical pods without priority may be evicted when less important pods are running.
  Set: spec: { priorityClassName: "high-priority" }
  Define priority classes: high-priority (1000), medium (500), low (0).

---
id: k8s-perf-no-vpa
language: yaml
severity: info
message: "Performance: Long-running workload with suboptimal resource settings"
tags:
  - performance
  - kubernetes
  - autoscaling
rule:
  kind: Deployment
  pattern: "replicas: [0-9]"
note: |
  Fixed resource requests may not match actual usage, leading to over/under provisioning.
  Install Vertical Pod Autoscaler to auto-adjust requests based on observed usage.
  VPA continuously optimizes resource allocation, reducing waste and costs.

---
id: k8s-perf-no-surge
language: yaml
severity: info
message: "Performance: RollingUpdate with maxSurge: 0 (slower updates)"
tags:
  - performance
  - kubernetes
  - deployments
rule:
  kind: RollingUpdateDeploymentStrategy
  pattern: "maxSurge: 0"
note: |
  maxSurge: 0 means new pods wait for old pods to terminate, slowing updates.
  Use: maxSurge: 25% (or 1 pod) and maxUnavailable: 25% for faster rolling updates.
  Trade temporary overcapacity for faster, safer deployments.
---
id: bash-perf-external-command-in-loop
language: bash
severity: warning
message: "Performance: Avoid calling external commands (grep, sed, awk, cut, tr, head, tail, wc, sort, uniq, cat, basename, dirname, date, expr) in loops - creates new process each iteration causing significant overhead."
tags:
  - performance
  - fork-overhead
  - loop-optimization
rule:
  kind: loop
  pattern: "(for|while|until).*do.*\\n.*(grep|sed|awk|cut|tr|head|tail|wc|sort|uniq|cat|basename|dirname|date|expr)\\b"
note: |
  Fork overhead: Each external command spawns a new process, causing context switches and memory allocation.
  Benchmark: 1000 files - external commands: ~5s, built-in: ~0.5s (10x improvement)
  Solution: Use bash parameter expansion (${var##*/}, ${var%/*}, etc.) or batch process with single command.

---
id: bash-perf-date-command-in-loop
language: bash
severity: warning
message: "Performance: Avoid calling `date` in loops - extremely expensive; use printf with %(%s)T or capture once outside loop."
tags:
  - performance
  - fork-overhead
  - loop-optimization
rule:
  kind: loop
  pattern: "(for|while).*\\n.*\\$\\(date"
note: |
  The date command is expensive to fork repeatedly.
  Solution 1: Use printf for current time (Bash 4.2+): printf '%(%s)T: event %s\n' -1 "$i"
  Solution 2: Capture once if same timestamp acceptable: timestamp=$(date +%s) outside loop.
  Performance gain: ~100x faster in loops with many iterations.

---
id: bash-perf-expr-for-arithmetic
language: bash
severity: warning
message: "Performance: Use $(( )) for arithmetic instead of `expr` - expr forks a process unnecessarily."
tags:
  - performance
  - fork-overhead
  - arithmetic
rule:
  kind: command_substitution
  pattern: "\\$\\(expr\\s|`expr\\s|expr\\s+[0-9]+\\s*[+\\-\\*/]"
note: |
  The expr command creates a new process for simple arithmetic.
  Benchmark: 10000 iterations - expr: ~3s, arithmetic expansion: ~0.05s (60x improvement)
  Solution: Use $((a + b)) or ((count++)) for in-place operations.
  Also avoids quoting issues and is more portable.

---
id: bash-perf-seq-in-for-loop
language: bash
severity: warning
message: "Performance: Use brace expansion {1..N} instead of `seq` in for loops - avoids fork."
tags:
  - performance
  - fork-overhead
  - loop-optimization
rule:
  kind: loop
  pattern: "for.*in\\s+\\$\\(seq\\s|for.*`seq\\s"
note: |
  The seq command forks a process just to generate a sequence of numbers.
  Solution 1: Use brace expansion: for i in {1..1000}; do ... done
  Solution 2: Use C-style loop: for ((i=1; i<=1000; i++)); do ... done
  Both are built-in and avoid subprocess creation entirely.

---
id: bash-perf-basename-dirname-command
language: bash
severity: warning
message: "Performance: Use parameter expansion instead of basename/dirname commands - avoids fork."
tags:
  - performance
  - fork-overhead
  - parameter-expansion
rule:
  kind: command_substitution
  pattern: "\\$\\(basename\\s|\\$\\(dirname\\s|`basename\\s|`dirname\\s"
note: |
  basename and dirname commands fork processes when parameter expansion works just as well.
  Parameter expansion solutions:
    - filename=${path##*/}          (remove longest prefix up to /)
    - directory=${path%/*}          (remove shortest suffix from /)
    - extension=${file##*.}         (remove longest prefix up to .)
    - basename_no_ext=${file%.*}    (remove shortest suffix)
  Performance: ~5x faster than command substitution.

---
id: bash-perf-external-test-command
language: bash
severity: info
message: "Performance: Use built-in [ or [[ ]] instead of /usr/bin/test to avoid fork."
tags:
  - performance
  - fork-overhead
rule:
  kind: command
  pattern: "/usr/bin/test\\s|/bin/test\\s|command\\s+test\\s"
note: |
  External test command /usr/bin/test forks a process.
  Solution: Use [ -f "$file" ] (POSIX) or [[ -f "$file" ]] (Bash).
  [[ ]] is generally preferred for Bash as it supports regex matching and doesn't expand globs.
  Performance gain: Eliminates fork overhead, especially noticeable in tight loops.

---
id: bash-perf-useless-use-of-cat
language: bash
severity: warning
message: "Performance: Don't pipe cat to other commands - use input redirection or command flags instead."
tags:
  - performance
  - useless-commands
rule:
  kind: pipeline
  pattern: "cat\\s+[^|]+\\|\\s*(grep|sed|awk|sort|head|tail|wc|tr)|cat\\s+-\\s*\\|"
note: |
  This is the famous UUOC (Useless Use of Cat) award pattern.
  cat file.txt | grep pattern  ❌ Creates two processes
  grep pattern file.txt         ✓ Single process
  cat file.txt | wc -l          ❌ Two processes
  wc -l < file.txt              ✓ Single process via input redirection
  Many tools support reading files directly, making cat in pipes unnecessary.

---
id: bash-perf-useless-use-of-echo-with-pipe
language: bash
severity: info
message: "Performance: Use here-strings (<<<) instead of echo piping to commands."
tags:
  - performance
  - useless-commands
rule:
  kind: pipeline
  pattern: "echo\\s+[\"']?[^|]+[\"']?\\s*\\|\\s*(grep|sed|awk|cut|tr)"
note: |
  Piping echo to other commands is less efficient than using here-strings.
  echo "$text" | grep pattern           ❌ Slower
  grep pattern <<< "$text"              ✓ Faster, uses here-string
  Alternative: Use bash pattern matching directly:
  [[ "$text" =~ pattern ]] && echo "matched"  ✓ No subprocess needed
  Here-strings avoid subprocess and are more direct.

---
id: bash-perf-useless-use-of-grep-with-wc
language: bash
severity: info
message: "Performance: Use grep -c instead of piping grep to wc -l."
tags:
  - performance
  - useless-commands
rule:
  kind: pipeline
  pattern: "grep\\s+[^|]+\\|\\s*wc\\s+-l"
note: |
  The grep command has a built-in count flag that's more efficient.
  count=$(grep pattern file.txt | wc -l)  ❌ Two processes
  count=$(grep -c pattern file.txt)         ✓ Single process
  The -c flag makes grep skip output generation and just count matches.
  Performance gain: Eliminates pipe and wc subprocess entirely.

---
id: bash-perf-useless-use-of-awk-for-simple-fields
language: bash
severity: info
message: "Performance: Use cut command instead of awk for simple field extraction."
tags:
  - performance
  - useless-commands
rule:
  kind: command
  pattern: "awk\\s+['\"]?\\{?\\s*print\\s+\\$[0-9]+\\s*\\}?['\"]?\\s+"
note: |
  awk is powerful but slow for simple field extraction; cut is faster.
  awk '{print $1}' file.txt                ❌ Slower, general-purpose interpreter
  cut -d' ' -f1 file.txt                   ✓ Faster, specialized tool
  cut -d: -f1 /etc/passwd                  ✓ Good for delimiter-separated files
  For line-by-line parsing with IFS:
  while IFS=: read -r user rest; do echo "$user"; done < /etc/passwd  ✓ Very fast
  Performance: cut ~2-3x faster than awk for this use case.

---
id: bash-perf-useless-use-of-ls
language: bash
severity: warning
message: "Performance: Don't parse ls output in loops - use bash globs or find instead."
tags:
  - performance
  - useless-commands
rule:
  kind: loop
  pattern: "for.*in\\s+\\$\\(ls\\s|ls\\s+[^|]+\\|\\s*(while|for)"
note: |
  Parsing ls output is unreliable and slow; glob patterns are better.
  for file in $(ls *.txt); do echo "$file"; done    ❌ Unreliable, breaks on spaces
  for file in *.txt; do [ -e "$file" ] || continue; echo "$file"; done  ✓ Better
  ls breaks on:
    - Filenames with spaces
    - Filenames with newlines
    - Hidden files (depending on flags)
  Glob expansion handles these correctly and is faster (no process).
  Performance: Eliminates ls subprocess + safer filename handling.

---
id: bash-perf-useless-use-of-head-tail
language: bash
severity: info
message: "Performance: Use sed for line ranges instead of piping head/tail together."
tags:
  - performance
  - useless-commands
rule:
  kind: pipeline
  pattern: "head\\s+-[0-9]+.*\\|\\s*tail\\s+-[0-9]+|tail\\s+-[0-9]+.*\\|\\s*head\\s+-[0-9]+"
note: |
  Combining head and tail requires two processes reading the same file.
  head -20 file.txt | tail -10              ❌ Two processes (head and tail)
  sed -n '11,20p' file.txt                  ✓ Single process
  For single line:
  sed -n '15p' file.txt                     ✓ Faster than head -15 | tail -1
  sed can specify exact line ranges efficiently.
  Performance: 2x faster by eliminating pipeline overhead.

---
id: bash-perf-external-command-for-string-length
language: bash
severity: warning
message: "Performance: Use ${#var} for string length instead of wc or expr."
tags:
  - performance
  - parameter-expansion
rule:
  kind: command_substitution
  pattern: "echo.*\\|\\s*wc\\s+-c|expr\\s+length\\s+"
note: |
  String length can be obtained via parameter expansion without subprocess.
  len=$(echo "$string" | wc -c)   ❌ Fork + pipeline
  len=$(expr length "$string")     ❌ Fork
  len=${#string}                   ✓ Built-in expansion, instant
  The parameter expansion ${#var} is built into bash and returns character count.
  Performance: ~100x faster than subprocess alternatives.

---
id: bash-perf-external-command-for-substring
language: bash
severity: warning
message: "Performance: Use substring expansion ${var:offset:length} instead of cut or sed."
tags:
  - performance
  - parameter-expansion
rule:
  kind: command_substitution
  pattern: "echo.*\\|\\s*cut\\s+-c|echo.*\\|\\s*sed\\s+['\"]s/\\.\\{[0-9]+\\}"
note: |
  Bash supports substring extraction via parameter expansion.
  substr=$(echo "$string" | cut -c1-5)   ❌ Fork + subprocess
  substr=${string:0:5}                   ✓ Built-in expansion
  first_char=${string:0:1}               ✓ Built-in expansion
  last_chars=${string: -3}               ✓ Negative offset from end
  Parameter expansion is instant and requires no subprocess.
  Performance: ~50x faster than cut/sed alternatives.

---
id: bash-perf-external-command-for-case-conversion
language: bash
severity: warning
message: "Performance: Use parameter expansion ${var^^} and ${var,,} for case conversion instead of tr."
tags:
  - performance
  - parameter-expansion
rule:
  kind: command_substitution
  pattern: "echo.*\\|\\s*tr\\s+['\"][A-Za-z]+['\"]"
note: |
  Bash 4.0+ supports case modification via parameter expansion.
  upper=$(echo "$string" | tr '[:lower:]' '[:upper:]')  ❌ Fork
  upper=${string^^}                                      ✓ Bash 4.0+
  lower=${string,,}                                      ✓ Bash 4.0+
  first_upper=${string^}                                 ✓ First char only
  first_lower=${string,}                                 ✓ First char only
  These operators are built-in and require no process.
  Performance: ~30x faster than tr alternative.

---
id: bash-perf-external-command-for-substitution
language: bash
severity: warning
message: "Performance: Use parameter expansion ${var/old/new} instead of sed for simple substitutions."
tags:
  - performance
  - parameter-expansion
rule:
  kind: command_substitution
  pattern: "echo.*\\|\\s*sed\\s+['\"]s/[^/]+/[^/]*/['\"]"
note: |
  Bash supports substitution via parameter expansion.
  result=$(echo "$string" | sed 's/old/new/')     ❌ Fork
  result=${string/old/new}                        ✓ First occurrence only
  result=${string//old/new}                       ✓ All occurrences
  The double slash // replaces all matches, single slash / replaces first.
  Parameter expansion is instant and built-in.
  Performance: ~40x faster than sed alternative.

---
id: bash-perf-external-command-for-prefix-suffix-removal
language: bash
severity: warning
message: "Performance: Use parameter expansion for prefix/suffix removal instead of sed."
tags:
  - performance
  - parameter-expansion
rule:
  kind: command_substitution
  pattern: "echo.*\\|\\s*sed\\s+['\"]s/\\^|echo.*\\|\\s*sed\\s+['\"]s/.*\\$/['\"]"
note: |
  Bash has operators for removing prefixes and suffixes.
  name=$(echo "$file" | sed 's/\\.txt$//')       ❌ Fork
  name=${file%.txt}                              ✓ Remove shortest suffix
  name=${file%%.txt}                             ❌ Note: %% is same as % for this
  Remove longest prefix:
  name=${path##*/}                               ✓ Remove longest prefix up to /
  name=${path#*/}                                ✓ Remove shortest prefix up to /
  This is pure parameter expansion, no subprocess.
  Performance: ~20x faster than sed alternative.

---
id: bash-perf-external-command-for-default-values
language: bash
severity: info
message: "Performance: Use parameter expansion for default values instead of if/test."
tags:
  - performance
  - parameter-expansion
rule:
  kind: conditional
  pattern: "if\\s+\\[\\s+-z\\s+.*\\].*then.*=.*fi|\\[\\s+-z\\s+.*\\]\\s*&&.*="
note: |
  Default value assignment has built-in parameter expansion.
  if [ -z "$var" ]; then var="default"; fi    ❌ Verbose
  var=${var:-default}                         ✓ Use default if unset or null
  var=${var:=default}                         ✓ Set and use default if unset
  : ${var:=default}                           ✓ Just set, don't use result
  var=${var-default}                          ✓ Use default if unset (not null)
  Expansion operators are built-in and instant.
  Performance: ~2x faster, more readable.

---
id: bash-perf-read-line-without-ifs-reset
language: bash
severity: info
message: "Performance: Always use IFS= with read to prevent field splitting."
tags:
  - performance
  - loop-optimization
  - string-handling
rule:
  kind: loop
  pattern: "while\\s+read\\s+(?!.*IFS)"
note: |
  The read command splits input by IFS (Internal Field Separator) by default.
  while read line; do echo "$line"; done < file.txt       ❌ May split fields
  while IFS= read -r line; do echo "$line"; done < file.txt  ✓ Preserves line intact
  IFS= prevents field splitting, -r prevents backslash interpretation.
  This ensures each line is read as a single variable.
  Performance: Small gain (~5%) but more importantly, correctness.

---
id: bash-perf-cat-in-while-loop
language: bash
severity: warning
message: "Performance: Use input redirection < instead of piping cat to while."
tags:
  - performance
  - loop-optimization
  - useless-commands
rule:
  kind: loop
  pattern: "cat\\s+.*\\|\\s*while\\s+read"
note: |
  Piping cat to a loop creates an unnecessary process.
  cat file.txt | while read line; do echo "$line"; done    ❌ Extra process
  while IFS= read -r line; do echo "$line"; done < file.txt  ✓ Direct redirection
  Also note: piping to while creates a subshell, so variable changes are lost.
  Input redirection avoids both the cat process and subshell issues.
  Performance: ~2x faster, fixes variable scope issues.

---
id: bash-perf-command-substitution-in-loop
language: bash
severity: warning
message: "Performance: Avoid running command substitution for each loop iteration when possible."
tags:
  - performance
  - loop-optimization
rule:
  kind: loop
  pattern: "while.*do.*\\n.*\\$\\(.*\\).*\\n.*done"
note: |
  Running command substitution inside loops can fork many times.
  while IFS= read -r file; do          ❌ High overhead
    size=$(stat -c %s "$file")
    owner=$(stat -c %U "$file")
    echo "$file: $size $owner"
  done < <(find . -type f)

  Better: Batch the stat calls:
  while IFS= read -r file; do
    read size owner < <(stat -c '%s %U' "$file")
    echo "$file: $size $owner"
  done < <(find . -type f)

  Or use find directly with printf:
  find . -type f -printf '%p: %s %u\n'

  This avoids repeated stat calls per file.

---
id: bash-perf-string-concatenation-in-loop
language: bash
severity: warning
message: "Performance: Avoid string concatenation in loops - creates new string each iteration. Use arrays or printf."
tags:
  - performance
  - loop-optimization
  - string-handling
rule:
  kind: loop
  pattern: "(for|while).*do.*\\n.*[a-zA-Z_][a-zA-Z0-9_]*=.*\\$[a-zA-Z_]"
note: |
  String concatenation (+=) in loops is O(n²) due to copying.
  result=""                              ❌ O(n²) - each += copies entire string
  for word in $words; do
    result="$result $word"
  done

  Better: Use array and join at end (O(n)):
  declare -a arr
  for word in $words; do
    arr+=("$word")
  done
  result="${arr[*]}"

  Or use printf:
  result=$(printf '%s ' $words)

  Benchmark: 1000 iterations - string concat: ~100ms, array: ~5ms (20x improvement).

---
id: bash-perf-array-append-with-length
language: bash
severity: warning
message: "Performance: Use arr+=() instead of arr[${#arr[@]}]= for appending to arrays."
tags:
  - performance
  - array-operations
  - loop-optimization
rule:
  kind: array_operation
  pattern: "[a-zA-Z_]+\\[\\$\\{#[a-zA-Z_]+\\[@\\]\\}\\]="
note: |
  Calculating array index with ${#arr[@]} is slower than using +=.
  arr[${#arr[@]}]="item$i"   ❌ Slower - must get length each time
  arr+=("item$i")             ✓ Faster - += handles index internally
  The += operator is optimized for appending and is noticeably faster.
  Performance: ~2x faster for large arrays.

---
id: bash-perf-let-for-counter-increment
language: bash
severity: info
message: "Performance: Use (( )) instead of let for arithmetic operations."
tags:
  - performance
  - loop-optimization
  - arithmetic
rule:
  kind: arithmetic
  pattern: "let\\s+[a-zA-Z_]+=|let\\s+[a-zA-Z_]+\\+\\+"
note: |
  The let builtin is older and slightly slower than (( )).
  let count++                  ❌ Older syntax
  ((count++))                  ✓ Preferred modern syntax
  Both are builtins, but (( )) is more efficient.
  Performance: ~5% faster with (( )).

---
id: bash-perf-unnecessary-subshell
language: bash
severity: warning
message: "Performance: Use { } instead of ( ) for command grouping when you don't need a subshell."
tags:
  - performance
  - subshell-overhead
rule:
  kind: subshell
  pattern: "\\(\\s*(cd|export|local|set|unset)"
note: |
  Subshells ( ) create new shell process; braces { } don't.
  (                              ❌ Subshell - slow
    cd /some/dir
    process files
  )

  {                              ✓ Group - no fork
    cd /some/dir
    process files
  }

  Note: Use subshell when you want isolation of variables.
  For temporary cd, consider:
  pushd /some/dir >/dev/null
  process files
  popd >/dev/null

  Performance: Subshells fork, braces don't - significant overhead elimination.

---
id: bash-perf-backtick-command-substitution
language: bash
severity: info
message: "Performance: Use $( ) instead of backticks for command substitution."
tags:
  - performance
  - subshell-overhead
rule:
  kind: command_substitution
  pattern: "`[^`]+`"
note: |
  Backticks are older syntax, harder to nest, and slightly less efficient.
  result=`command`               ❌ Backticks - harder to read/nest
  nested=`echo \`date\``         ❌ Ugly escaping required
  result=$(command)              ✓ Modern syntax - cleaner
  nested=$(echo $(date))         ✓ Natural nesting
  $( ) is preferred in modern bash and easier to parse.
  Performance: ~2% faster, much better readability.

---
id: bash-perf-subshell-in-pipeline-loop
language: bash
severity: warning
message: "Performance: Avoid piping to while - creates subshell. Use input redirection or process substitution."
tags:
  - performance
  - subshell-overhead
  - loop-optimization
rule:
  kind: loop
  pattern: "\\|\\s*while\\s+.*do.*done"
note: |
  Piping to while creates a subshell where variable changes are lost.
  count=0
  cat file.txt | while read line; do        ❌ while in subshell
    ((count++))
  done
  echo "$count"  # Always 0!

  Solution 1: Input redirection (preferred):
  count=0
  while read line; do
    ((count++))
  done < file.txt
  echo "$count"  # Correct!

  Solution 2: Process substitution:
  count=0
  while read line; do
    ((count++))
  done < <(some_command)

  Input redirection avoids subshell entirely.
  Performance: Eliminates fork, fixes variable scope.

---
id: bash-perf-multiple-subshells-for-variables
language: bash
severity: warning
message: "Performance: Use single read with multiple variables instead of multiple command substitutions."
tags:
  - performance
  - subshell-overhead
rule:
  kind: assignment
  pattern: "[a-zA-Z_]+=\\$\\(.*\\).*\\n[a-zA-Z_]+=\\$\\(.*\\)"
note: |
  Multiple command substitutions create multiple subshells.
  user=$(stat -c %U file)        ❌ Three subshells
  group=$(stat -c %G file)
  size=$(stat -c %s file)

  Solution 1: Single read from one substitution:
  read user group size < <(stat -c '%U %G %s' file)

  Solution 2: Using IFS:
  IFS=' ' read -r user group size <<< "$(stat -c '%U %G %s' file)"

  This batches queries into single command call.
  Performance: 3x faster (only one fork instead of three).

---
id: bash-perf-subshell-for-temporary-variable
language: bash
severity: info
message: "Performance: Use command prefix for temporary variable change instead of subshell."
tags:
  - performance
  - subshell-overhead
rule:
  kind: subshell
  pattern: "\\(\\s*[A-Z_]+="
note: |
  Temporary variable changes don't need a subshell.
  (                              ❌ Subshell - forks
    PATH=/custom/path:$PATH
    run_command
  )

  PATH=/custom/path:$PATH run_command   ✓ Prefix assignment - no fork
  # Variable change is local to command only

  This syntax sets the variable for just that command.
  Much more efficient than forking a subshell.
  Performance: Eliminates fork entirely.

---
id: bash-perf-fork-for-simple-test
language: bash
severity: warning
message: "Performance: Use bash pattern matching [[ ]] instead of forking grep in conditionals."
tags:
  - performance
  - subshell-overhead
  - testing
rule:
  kind: conditional
  pattern: "if\\s+\\[\\s*\\$\\(|if\\s+`"
note: |
  Forking just to test a condition wastes resources.
  if [ $(echo "$var" | grep -c pattern) -gt 0 ]; then   ❌ Forks + processes

  if [[ "$var" == *pattern* ]]; then                    ✓ Bash pattern
  if [[ "$var" =~ pattern ]]; then                      ✓ Bash regex

  Bash has native pattern matching operators:
  == for glob patterns
  =~ for extended regex

  Performance: ~50x faster - no forks, direct matching.

---
id: bash-perf-multiple-redirections-to-same-file
language: bash
severity: warning
message: "Performance: Group commands with >> to same file instead of multiple redirections."
tags:
  - performance
  - io-optimization
rule:
  kind: redirection
  pattern: ">>\\s*[a-zA-Z0-9_./]+\\s*\\n.*>>\\s*[a-zA-Z0-9_./]+"
note: |
  Multiple redirections open the file repeatedly.
  echo "line1" >> output.log    ❌ Opens file three times
  echo "line2" >> output.log
  echo "line3" >> output.log

  Solution 1: Group output:
  {
    echo "line1"
    echo "line2"
    echo "line3"
  } >> output.log

  Solution 2: Use file descriptor:
  exec 3>>output.log
  echo "line1" >&3
  echo "line2" >&3
  echo "line3" >&3
  exec 3>&-

  Grouping opens file once.
  Performance: ~3x faster for many writes.

---
id: bash-perf-sync-write-on-each-line
language: bash
severity: info
message: "Performance: Redirect output once instead of appending each line in loop."
tags:
  - performance
  - io-optimization
rule:
  kind: loop
  pattern: "while.*do.*\\n.*>>\\s*|while.*do.*\\n.*echo.*>>\\s*"
note: |
  Writing each line separately prevents buffering.
  while read line; do           ❌ Inefficient I/O buffering
    echo "$line" >> output.txt
  done < input.txt

  while read line; do           ✓ Better buffering
    echo "$line"
  done < input.txt > output.txt

  This batches all writes together, allowing OS to buffer efficiently.
  The shell can buffer much more than individual redirections.
  Performance: ~5x faster I/O, depends on file size.

---
id: bash-perf-reading-file-multiple-times
language: bash
severity: warning
message: "Performance: Read file once for multiple operations instead of reading multiple times."
tags:
  - performance
  - io-optimization
rule:
  kind: command
  pattern: "(cat|<)\\s*([a-zA-Z0-9_./]+).*\\n.*(cat|<)\\s*\\2"
note: |
  Reading the same file multiple times wastes I/O.
  line_count=$(wc -l < file.txt)   ❌ Three file reads
  first_line=$(head -1 file.txt)
  last_line=$(tail -1 file.txt)

  Solution 1: Read once into variables:
  {
    read first_line
    while read line; do
      last_line=$line
      ((line_count++))
    done
  } < file.txt

  Solution 2: Use awk for complex cases:
  read first_line last_line line_count < <(awk 'NR==1{first=$0} {last=$0; count++} END{print first, last, count}' file.txt)

  Single pass is much faster.
  Performance: ~3x faster for files with many operations.

---
id: bash-perf-temporary-file-instead-of-pipe
language: bash
severity: info
message: "Performance: Use pipes or process substitution instead of temporary files for IPC."
tags:
  - performance
  - io-optimization
rule:
  kind: redirection
  pattern: ">\\s*/tmp/.*\\n.*<\\s*/tmp/"
note: |
  Temporary files require disk I/O; pipes operate in memory.
  command1 > /tmp/temp.txt   ❌ Disk I/O
  command2 < /tmp/temp.txt
  rm /tmp/temp.txt

  Solution 1: Pipe directly (no random access):
  command1 | command2

  Solution 2: Process substitution (allows random access):
  command2 < <(command1)

  Pipes keep data in memory (faster), temporary files hit disk.
  Performance: ~10x faster for small data, memory usage depends on data size.

---
id: bash-perf-polling-file-for-changes
language: bash
severity: warning
message: "Performance: Use inotifywait instead of busy-waiting with sleep for file changes."
tags:
  - performance
  - io-optimization
rule:
  kind: loop
  pattern: "while.*true.*do.*\\n.*sleep.*\\n.*done|while.*:.*do.*\\n.*sleep"
note: |
  Polling with sleep is inefficient and wastes CPU.
  while true; do             ❌ Busy-waiting, wakes every second
    if [ file.txt -nt "$last_check" ]; then
      process file.txt
      last_check=$(date +%s)
    fi
    sleep 1
  done

  Better: Use inotify (instant notification):
  inotifywait -m -e modify file.txt | while read; do
    process file.txt
  done

  inotify is event-driven - responds instantly when file changes.
  Performance: Eliminates polling overhead, responds immediately.
  Note: inotifywait is Linux-only (inotify API).

---
id: bash-perf-heredoc-vs-herestring
language: bash
severity: info
message: "Performance: Use here-strings (<<<) instead of here-documents for simple variable expansion."
tags:
  - performance
  - io-optimization
rule:
  kind: redirection
  pattern: "<<\\s*['\"]?EOF.*\\n\\$[a-zA-Z_]+\\nEOF"
note: |
  Here-documents have more overhead than here-strings.
  grep pattern <<EOF           ❌ Here-document (more overhead)
  $variable
  EOF

  grep pattern <<< "$variable" ✓ Here-string (simpler)

  Here-strings are syntactically simpler and faster.
  Both work similarly, but <<< is more direct.
  Performance: ~2% faster, cleaner syntax.

---
id: bash-perf-array-search-with-loop
language: bash
severity: warning
message: "Performance: Use associative arrays for lookup instead of linear search in loop."
tags:
  - performance
  - array-operations
rule:
  kind: loop
  pattern: "for.*in.*\\$\\{.*\\[@\\]\\}.*do.*\\n.*if.*==.*then.*break"
note: |
  Linear search through arrays is O(n).
  arr=(apple banana cherry)        ❌ O(n) linear search
  found=false
  for item in "${arr[@]}"; do
    if [ "$item" = "banana" ]; then
      found=true
      break
    fi
  done

  Better: Use associative array (O(1) lookup):
  declare -A lookup
  for item in apple banana cherry; do
    lookup[$item]=1
  done
  if [ "${lookup[banana]}" ]; then
    found=true
  fi

  Or for small arrays, pattern matching:
  if [[ " ${arr[*]} " == *" banana "* ]]; then
    found=true
  fi

  Associative arrays provide constant-time lookup.
  Performance: For large arrays, O(1) vs O(n) - huge difference.

---
id: bash-perf-copying-array-element-by-element
language: bash
severity: warning
message: "Performance: Copy arrays using slice syntax instead of element-by-element loop."
tags:
  - performance
  - array-operations
rule:
  kind: loop
  pattern: "for.*in.*\\$\\{.*\\[@\\]\\}.*do.*\\n.*+="
note: |
  Element-by-element copying is slow; slice syntax is instant.
  arr1=(1 2 3 4 5)             ❌ Slow loop
  arr2=()
  for item in "${arr1[@]}"; do
    arr2+=("$item")
  done

  arr2=("${arr1[@]}")           ✓ Instant copy
  arr2=("${arr1[@]:2:3}")       ✓ Slice copy (elements 2,3,4)

  The expansion syntax copies the entire array (or slice) directly.
  Performance: ~10x faster for large arrays.

---
id: bash-perf-counting-array-elements-with-loop
language: bash
severity: info
message: "Performance: Use ${#arr[@]} instead of looping to count array elements."
tags:
  - performance
  - array-operations
rule:
  kind: loop
  pattern: "count=0.*\\n.*for.*in.*\\[@\\].*\\n.*\\+\\+"
note: |
  Counting array elements with a loop is unnecessary.
  count=0                       ❌ Loop unnecessary
  for item in "${arr[@]}"; do
    ((count++))
  done

  count=${#arr[@]}              ✓ Instant count

  The ${#arr[@]} expansion returns the count directly.
  This is instant and doesn't require iteration.
  Performance: ~100x faster for large arrays.

---
id: bash-perf-joining-array-with-loop
language: bash
severity: warning
message: "Performance: Use IFS trick to join arrays instead of loop concatenation."
tags:
  - performance
  - array-operations
rule:
  kind: loop
  pattern: "for.*in.*\\[@\\].*do.*\\n.*result=.*\\$result"
note: |
  Loop concatenation is O(n²) due to string copying.
  arr=(one two three)          ❌ O(n²) - inefficient
  result=""
  for item in "${arr[@]}"; do
    result="$result,$item"
  done
  result=${result:1}

  Solution 1: IFS trick (fastest):
  arr=(one two three)
  IFS=,
  result="${arr[*]}"
  unset IFS

  Solution 2: IFS in subshell:
  result=$(IFS=,; echo "${arr[*]}")

  The IFS trick joins array in single operation.
  Performance: ~20x faster than loop concatenation.

---
id: bash-perf-string-comparison-with-grep
language: bash
severity: warning
message: "Performance: Use pattern matching [[ ]] instead of grep for string comparison."
tags:
  - performance
  - string-operations
rule:
  kind: conditional
  pattern: "echo.*\\|\\s*grep\\s+-q|echo.*\\|\\s*grep.*>/dev/null"
note: |
  Piping to grep for string matching is wasteful.
  if echo "$string" | grep -q "pattern"; then   ❌ Forks process

  if [[ "$string" == *"pattern"* ]]; then       ✓ Bash pattern
  if [[ "$string" =~ pattern ]]; then           ✓ Bash regex
  if [ "$string" = "value" ]; then              ✓ Exact match

  Bash has built-in pattern matching operators that are instant.
  == for glob patterns, =~ for extended regex.
  Performance: ~50x faster - no fork, direct matching.

---
id: bash-perf-repeated-string-operations
language: bash
severity: info
message: "Performance: Chain parameter expansions carefully or use external tools for complex replacements."
tags:
  - performance
  - string-operations
rule:
  kind: assignment
  pattern: "[a-zA-Z_]+=\\$\\{[a-zA-Z_]+#.*\\}\\n[a-zA-Z_]+=\\$\\{[a-zA-Z_]+%"
note: |
  Multiple parameter expansions are generally fine, but can be optimized.
  path="/path/to/file.txt"     ✓ Generally acceptable
  dir=${path%/*}               ✓ Get directory
  name=${path##*/}             ✓ Get filename
  base=${name%.*}              ✓ Remove extension
  ext=${name##*.}              ✓ Get extension

  Each expansion is fast (no subprocesses).
  For complex operations, consider awk/sed.

  Bash parameter expansion is efficient and readable.
  Only optimize if profiling shows bottleneck.

---
id: bash-perf-printf-for-simple-echo
language: bash
severity: info
message: "Performance: Use echo for simple output (slightly faster), printf for safety with variables."
tags:
  - performance
  - string-operations
rule:
  kind: command
  pattern: "printf\\s+['\"]%s\\\\n['\"]"
note: |
  printf has slight overhead due to format parsing.
  printf '%s\n' "Hello, World"   ❌ Slower than echo (format parsing)
  echo "Hello, World"            ✓ Slightly faster

  However, printf is safer with user input:
  echo "Hello, $user_input"      ❌ Unsafe - interprets special chars
  printf '%s\n' "$user_input"    ✓ Safe - literal output

  Trade-off: echo is 2-3% faster, but printf is safer.
  Use echo for literals, printf for variables.
  Performance: Negligible difference in practice.

---
id: bash-perf-multiple-parameter-expansions
language: bash
severity: info
message: "Performance: Chain parameter expansions vs single sed - depends on complexity."
tags:
  - performance
  - string-operations
rule:
  kind: assignment
  pattern: "\\$\\{.*\\{.*\\{"
note: |
  Nested parameter expansions are fine for simple cases.
  result=${var//pattern1/repl1}  ✓ Good for simple case

  But chaining gets unreadable:
  result=${var//pattern1/repl1}  ❌ Gets complex fast
  result=${result//pattern2/repl2}
  result=${result//pattern3/repl3}

  Better with external tools:
  result=$(sed 's/pattern1/repl1/g; s/pattern2/repl2/g; s/pattern3/repl3/g' <<< "$var")

  sed is cleaner for multiple replacements.
  Performance: sed is ~5x faster for 3+ replacements.
  Readability: sed is clearer for complex patterns.

---
id: bash-perf-glob-pattern-vs-find
language: bash
severity: warning
message: "Performance: Use glob patterns instead of find for simple directory listing."
tags:
  - performance
  - pattern-matching
rule:
  kind: loop
  pattern: "for.*in\\s+\\$\\(find\\s+\\.\\s+-maxdepth\\s+1"
note: |
  find is powerful but slower than simple globs for basic matching.
  for file in $(find . -maxdepth 1 -name "*.txt"); do   ❌ Fork
    echo "$file"
  done

  for file in ./*.txt; do                                ✓ Glob (no fork)
    [ -e "$file" ] || continue
    echo "$file"
  done

  Glob patterns are instant (no subprocess).
  The [ -e ] check handles case when no matches.
  Performance: ~2x faster, no subprocess overhead.

---
id: bash-perf-extended-glob-not-enabled
language: bash
severity: info
message: "Performance: Enable extglob (shopt -s extglob) to simplify complex glob patterns."
tags:
  - performance
  - pattern-matching
rule:
  kind: glob_pattern
  pattern: "# Check for complex patterns without shopt -s extglob"
note: |
  Extended glob patterns can replace complex loops.
  Without extglob (complex):
  for file in *.txt *.log *.dat; do  ❌ Repetitive
    process "$file"
  done

  With extglob (simple):
  shopt -s extglob
  for file in *.@(txt|log|dat); do   ✓ Cleaner
    process "$file"
  done

  Extglob provides ?(pat), *(pat), +(pat), @(pat), !(pat) operators.
  These allow complex matching patterns without loops.
  Performance: No performance difference, but cleaner code.

---
id: bash-perf-case-vs-if-chain
language: bash
severity: info
message: "Performance: Use case statement instead of if/elif chain for pattern matching."
tags:
  - performance
  - pattern-matching
rule:
  kind: conditional
  pattern: "if\\s+\\[\\[?.*==.*\\]\\]?.*then.*\\n.*elif\\s+\\[\\[?.*=="
note: |
  case statement is more readable and potentially faster.
  if [[ "$input" == "start" ]]; then    ❌ Slow chain
    action1
  elif [[ "$input" == "stop" ]]; then
    action2
  elif [[ "$input" == "restart" ]]; then
    action3
  fi

  case "$input" in                      ✓ Faster
    start)   action1 ;;
    stop)    action2 ;;
    restart) action3 ;;
  esac

  case is optimized for pattern matching.
  Performance: ~10-20% faster for 3+ conditions.
  Readability: Much cleaner and more maintainable.

---
id: bash-perf-regex-match-in-loop
language: bash
severity: warning
message: "Performance: Store regex pattern in variable before loop to avoid recompiling."
tags:
  - performance
  - pattern-matching
rule:
  kind: loop
  pattern: "(for|while).*do.*\\n.*=~"
note: |
  Regex compilation happens each iteration if pattern is literal.
  for line in "${lines[@]}"; do        ❌ Recompiles regex each iteration
    if [[ "$line" =~ ^[0-9]+$ ]]; then
      echo "number: $line"
    fi
  done

  pattern='^[0-9]+$'                   ✓ Store pattern
  for line in "${lines[@]}"; do
    if [[ "$line" =~ $pattern ]]; then
      echo "number: $line"
    fi
  done

  Storing pattern in variable uses cached compiled regex.
  Performance: ~30% faster for loops with many iterations.

---
id: bash-perf-wait-for-all-background-jobs
language: bash
severity: info
message: "Performance: Use wait without arguments to wait for all background jobs."
tags:
  - performance
  - process-management
rule:
  kind: process_management
  pattern: "&\\s*\\n.*wait\\s+\\$!.*\\n.*&\\s*\\n.*wait"
note: |
  Waiting for individual jobs is verbose; wait alone waits for all.
  job1 &                        ❌ Verbose - waiting individually
  pid1=$!
  job2 &
  pid2=$!
  wait $pid1
  wait $pid2

  job1 &                        ✓ Wait for all at once
  job2 &
  wait

  Plain wait waits for all background jobs to complete.
  More concise and equally efficient.
  Performance: No difference in speed, but cleaner code.

---
id: bash-perf-process-substitution-vs-temp-file
language: bash
severity: warning
message: "Performance: Use process substitution < <() instead of temporary files for IPC."
tags:
  - performance
  - process-management
rule:
  kind: redirection
  pattern: ">\\s*/tmp/.*\\$\\$.*\\n.*<\\s*/tmp/.*\\$\\$"
note: |
  Process substitution keeps data in memory instead of disk.
  sort file1.txt > /tmp/sorted1.$$     ❌ Disk I/O
  sort file2.txt > /tmp/sorted2.$$
  diff /tmp/sorted1.$$ /tmp/sorted2.$$
  rm /tmp/sorted1.$$ /tmp/sorted2.$$

  diff <(sort file1.txt) <(sort file2.txt)  ✓ Memory-based

  Process substitution creates temporary named pipes (in /dev/fd/).
  Data stays in memory, no disk I/O needed.
  Performance: ~5x faster, avoids disk writes entirely.

---
id: bash-perf-repeated-command-execution
language: bash
severity: warning
message: "Performance: Cache command results in variables instead of running repeatedly."
tags:
  - performance
  - caching
rule:
  kind: command_substitution
  pattern: "\\$\\(command.*\\).*\\n.*\\$\\(command.*\\)"
note: |
  Running the same command multiple times wastes resources.
  if [ "$(uname)" = "Linux" ]; then    ❌ Runs uname twice
    echo "Linux specific"
  fi
  if [ "$(uname)" = "Linux" ]; then
    echo "More Linux stuff"
  fi

  os=$(uname)                         ✓ Cache once
  if [ "$os" = "Linux" ]; then
    echo "Linux specific"
  fi
  if [ "$os" = "Linux" ]; then
    echo "More Linux stuff"
  fi

  Caching avoids repeated execution of expensive commands.
  Performance: Depends on command cost - could be 100x faster for expensive ops.

---
id: bash-perf-directory-listing-cache
language: bash
severity: warning
message: "Performance: Cache glob results in array instead of globbing multiple times."
tags:
  - performance
  - caching
rule:
  kind: loop
  pattern: "for.*in\\s+/path/\\*.*\\n.*for.*in\\s+/path/\\*"
note: |
  Globbing the same directory multiple times is wasteful.
  for file in /data/*.csv; do         ❌ Globs twice
    process1 "$file"
  done
  for file in /data/*.csv; do
    process2 "$file"
  done

  files=(/data/*.csv)                 ✓ Cache once
  for file in "${files[@]}"; do
    process1 "$file"
  done
  for file in "${files[@]}"; do
    process2 "$file"
  done

  Globbing creates list of matching files.
  Caching this list avoids redundant filesystem operations.
  Performance: ~2x faster for many files.

---
id: bash-perf-function-result-caching
language: bash
severity: info
message: "Performance: Cache expensive function results using associative arrays."
tags:
  - performance
  - caching
rule:
  kind: function
  pattern: "# Pattern for repeated function calls"
note: |
  Expensive function calls can benefit from memoization.
  expensive_lookup() {                ❌ No caching - repeated calls slow
    curl -s "https://api.example.com/$1"
  }
  for item in "${items[@]}"; do
    result=$(expensive_lookup "$item")
    process "$result"
  done

  declare -A cache                    ✓ Cached lookup
  cached_lookup() {
    local key=$1
    if [ -z "${cache[$key]}" ]; then
      cache[$key]=$(curl -s "https://api.example.com/$key")
    fi
    echo "${cache[$key]}"
  }
  for item in "${items[@]}"; do
    result=$(cached_lookup "$item")
    process "$result"
  done

  Memoization stores results, avoiding repeated computation.
  Performance: With repeated lookups, could be 10-100x faster.

---
id: bash-perf-sequential-processing-of-independent-tasks
language: bash
severity: warning
message: "Performance: Use parallel execution for independent tasks - background jobs or GNU parallel."
tags:
  - performance
  - parallelization
rule:
  kind: loop
  pattern: "for.*in.*do.*\\n.*(curl|wget|rsync|scp).*\\n.*done"
note: |
  Processing independent tasks sequentially wastes time.
  for url in "${urls[@]}"; do                    ❌ Sequential - slow
    curl -s "$url" > "output_$i.txt"
  done

  Solution 1: Background jobs with limits:
  max_jobs=4
  for url in "${urls[@]}"; do
    while [ $(jobs -r | wc -l) -ge $max_jobs ]; do
      sleep 0.1
    done
    curl -s "$url" > "output_$((i++)).txt" &
  done
  wait

  Solution 2: GNU parallel:
  parallel curl -s {} ">" output_{#}.txt ::: "${urls[@]}"

  Parallel execution can make independent tasks much faster.
  Performance: With N tasks and M CPUs, speed up is ~min(N, M)x.

---
id: bash-perf-xargs-without-parallel
language: bash
severity: warning
message: "Performance: Use xargs -P for parallel execution of independent jobs."
tags:
  - performance
  - parallelization
rule:
  kind: pipeline
  pattern: "xargs\\s+(?!.*-P)(?!.*--max-procs)"
note: |
  xargs processes items sequentially by default.
  find . -name "*.txt" | xargs gzip       ❌ Sequential processing

  find . -name "*.txt" -print0 | xargs -0 -P 4 gzip     ✓ 4 parallel jobs
  find . -name "*.txt" -print0 | xargs -0 -P $(nproc) gzip  ✓ Use CPU count

  The -P flag specifies number of parallel processes.
  Using -P $(nproc) auto-scales to CPU count.
  Performance: 4x faster on 4-CPU system, linear scaling.

---
id: bash-perf-no-job-control-for-long-tasks
language: bash
severity: warning
message: "Performance: Use background processes for long independent tasks instead of sequential execution."
tags:
  - performance
  - parallelization
rule:
  kind: command_sequence
  pattern: "(rsync|scp|tar|gzip).*\\n(rsync|scp|tar|gzip)"
note: |
  Long sequential tasks can run in parallel.
  rsync -a server1:/data /backup/server1     ❌ Sequential - slow
  rsync -a server2:/data /backup/server2
  rsync -a server3:/data /backup/server3

  rsync -a server1:/data /backup/server1 &   ✓ Parallel
  rsync -a server2:/data /backup/server2 &
  rsync -a server3:/data /backup/server3 &
  wait
  echo "All syncs complete"

  Background processes allow overlapping I/O and network operations.
  Performance: 3x faster with 3 parallel rsync jobs.

---
id: bash-perf-named-pipe-for-ipc
language: bash
severity: info
message: "Performance: Use named pipes (mkfifo) for inter-process communication instead of temporary files."
tags:
  - performance
  - parallelization
rule:
  kind: ipc
  pattern: "# Complex IPC patterns"
note: |
  Named pipes allow efficient inter-process communication.
  producer > /tmp/data.txt           ❌ Temporary file - disk I/O
  consumer < /tmp/data.txt

  mkfifo /tmp/pipe                   ✓ Named pipe (in-memory)
  producer > /tmp/pipe &
  consumer < /tmp/pipe
  rm /tmp/pipe

  Named pipes operate in-memory (usually in /dev/fd/).
  More efficient than temporary files for IPC.
  Performance: Similar to process substitution, avoids disk I/O.

---
id: bash-perf-gnu-parallel-for-complex-jobs
language: bash
severity: info
message: "Performance: Use GNU parallel for complex parallel job distribution and monitoring."
tags:
  - performance
  - parallelization
rule:
  kind: command
  pattern: "# Complex job distribution patterns"
note: |
  GNU parallel is powerful for complex parallel scenarios.
  Basic usage:
  find . -name "*.log" | parallel 'gzip {} && echo "Done: {}"'

  With progress and logging:
  parallel --progress --joblog jobs.log process_file {} ::: *.dat

  Distributed across machines:
  parallel --sshlogin server1,server2 process {} ::: files*

  GNU parallel features:
    - Automatic load balancing
    - Progress indication
    - Job logging
    - Remote execution
    - Fault tolerance
  Performance: Optimal parallelization with minimal overhead.

---
id: bash-perf-no-time-measurement
language: bash
severity: info
message: "Performance: Measure script performance to identify bottlenecks before optimizing."
tags:
  - performance
  - profiling
rule:
  kind: script_level
  pattern: "# Scripts without timing"
note: |
  Profiling guides optimization efforts.

  Simple timing:
  time ./script.sh

  Bash built-in timing with format:
  TIMEFORMAT='%3R seconds'
  time { commands; }

  Debug with millisecond precision:
  PS4='+ $(date "+%s.%N")\011 '
  set -x
  # commands
  set +x

  Hyperfine for benchmarking:
  hyperfine './script.sh' './optimized.sh'

  Always measure before and after optimization.
  Performance: Data-driven optimization is 10x more effective.

---
id: bash-perf-debug-mode-overhead
language: bash
severity: info
message: "Performance: Don't leave set -x or set -v enabled in production scripts."
tags:
  - performance
  - profiling
rule:
  kind: shell_option
  pattern: "set\\s+-x(?!.*\\+x)|set\\s+-v(?!.*\\+v)"
note: |
  Debug tracing has significant overhead.
  #!/bin/bash
  set -x                              ❌ All commands traced - 10-30% overhead
  # All commands traced

  #!/bin/bash                         ✓ Conditional debug
  if [ "${DEBUG:-}" = "1" ]; then
    set -x
  fi
  # Commands only traced if DEBUG=1

  Overhead of set -x: Every command printed before execution (~10-30% slower).
  Only enable debug when needed for troubleshooting.
  Performance: Can mean difference between usable and unusable scripts.
---
id: python-perf-string-concat-loop
language: python
severity: warning
message: "Performance: String concatenation in loops uses O(n²) complexity; use list and join() instead"
tags:
  - performance
  - strings
rule:
  kind: assignment
  pattern: |
    result = ""
    for ... in ...:
        result += ...
note: |
  String concatenation inside loops creates new string objects each iteration due to immutability.
  Each += operation allocates new memory and copies both operands.

  ANTI-PATTERN:
    result = ""
    for item in items:
        result += str(item)

  SOLUTION: Use list.append() + ''.join()
    parts = []
    for item in items:
        parts.append(str(item))
    result = ''.join(parts)

  Or use list comprehension + join:
    result = ''.join(str(item) for item in items)

  BENCHMARK: For 1000 strings, join() is ~4x faster than +=
  On 100k strings, difference becomes even more dramatic (20-100x for very large strings)

---
id: python-perf-list-concat-in-loop
language: python
severity: warning
message: "Performance: List concatenation with += in loops is O(n²); use append() instead"
tags:
  - performance
  - collections
rule:
  kind: assignment
  pattern: |
    result = []
    for ... in ...:
        result = result + [...]
note: |
  List += with list literals creates new list objects each iteration.
  Each operation copies the entire list.

  ANTI-PATTERN:
    result = []
    for item in items:
        result = result + [item]

  SOLUTION: Use append()
    result = []
    for item in items:
        result.append(item)

  BENCHMARK: append() is O(1) amortized, += with list literal is O(n)
  Performance difference grows exponentially with input size

---
id: python-perf-list-membership-check
language: python
severity: warning
message: "Performance: Use set/dict for membership checks, not list (O(n) vs O(1))"
tags:
  - performance
  - collections
  - lookups
rule:
  kind: call
  pattern: |
    if ... in list_variable:
      ...
note: |
  List membership (in operator) is O(n) - linear search.
  Set/Dict membership is O(1) - hash table lookup.

  ANTI-PATTERN:
    allowed = ['admin', 'user', 'guest']
    if username in allowed:
        ...

  SOLUTION: Use set for membership tests
    allowed = {'admin', 'user', 'guest'}
    if username in allowed:
        ...

  BENCHMARK: For 1,000,000 items, dict lookup is ~585,714x faster than list lookup
  Even for 1000 items, set lookup is ~100-1000x faster

---
id: python-perf-dict-double-lookup
language: python
severity: info
message: "Performance: Check dict membership before access; avoid double lookup with get()"
tags:
  - performance
  - collections
  - dicts
rule:
  kind: call
  pattern: |
    if key in dict_var:
        value = dict_var[key]
note: |
  Checking membership then accessing causes two dict lookups.

  ANTI-PATTERN:
    if key in config:
        value = config[key]

  SOLUTION: Use dict.get()
    value = config.get(key, default)

  Or use try/except:
    try:
        value = config[key]
    except KeyError:
        value = default

  OPTIMIZATION: get() combines membership check and access in one lookup

---
id: python-perf-dict-init-empty
language: python
severity: info
message: "Performance: Use dict literal {} instead of dict() constructor for empty dict"
tags:
  - performance
  - collections
rule:
  kind: assignment
  pattern: |
    var = dict()
note: |
  dict() requires name lookup in global scope; {} is a literal.

  ANTI-PATTERN:
    config = dict()
    state = dict()

  SOLUTION: Use empty dict literal
    config = {}
    state = {}

  PERFORMANCE: {} is faster because it's a literal, not a name lookup
  The difference is small but accumulates in hot loops

---
id: python-perf-list-comprehension-simple
language: python
severity: info
message: "Performance: Use list comprehension instead of for loop for better performance"
tags:
  - performance
  - loops
  - collections
rule:
  kind: call
  pattern: |
    result = []
    for item in iterable:
        result.append(...)
note: |
  List comprehensions are typically 10-20% faster than equivalent for loops.
  They're optimized at the bytecode level.

  ANTI-PATTERN:
    result = []
    for x in items:
        result.append(x * 2)

  SOLUTION: Use list comprehension
    result = [x * 2 for x in items]

  BENCHMARK: List comprehension is 10-30% faster depending on operation complexity
  Benefits increase with more complex logic
  Note: List comprehensions ARE slower if they create in-memory list when you only need iteration

---
id: python-perf-generator-expression
language: python
severity: info
message: "Performance: Use generator expression for large datasets to save memory"
tags:
  - performance
  - memory
  - generators
  - loops
rule:
  kind: call
  pattern: |
    for item in [x for x in large_iterable if ...]:
        ...
note: |
  Generator expressions use lazy evaluation - values produced on-demand.
  List comprehensions load entire result into memory.

  ANTI-PATTERN: (creates entire list in memory)
    for x in [y * 2 for y in range(10000000)]:
        print(x)

  SOLUTION: Use generator expression
    for x in (y * 2 for y in range(10000000)):
        print(x)

  Or pass to function:
    sum(y * 2 for y in range(10000000))

  MEMORY SAVINGS: Generators use constant memory regardless of dataset size
  List comprehension memory grows linearly with input size

---
id: python-perf-any-all-short-circuit
language: python
severity: info
message: "Performance: Use any() and all() for boolean checks; avoid comprehensions that lose short-circuit"
tags:
  - performance
  - collections
  - functions
rule:
  kind: call
  pattern: |
    result = [condition(x) for x in items]
    if any(result):
      ...
note: |
  any() and all() short-circuit: they stop at first True/False.
  Comprehensions always evaluate entire sequence.

  ANTI-PATTERN: (evaluates all items)
    results = [is_admin(x) for x in users]
    if any(results):
        grant_access()

  SOLUTION: Use any()/all() directly
    if any(is_admin(x) for x in users):
        grant_access()

  SHORT-CIRCUIT BENEFIT: Stops after first True for any()
  Significant speedup when early termination is likely

---
id: python-perf-numpy-vectorize
language: python
severity: warning
message: "Performance: Use NumPy vectorized operations instead of iterating elements"
tags:
  - performance
  - numpy
  - data
rule:
  kind: call
  pattern: |
    for i in range(len(array)):
        result[i] = func(array[i])
note: |
  NumPy vectorization uses optimized C code, not Python loops.
  Vectorized operations are 50-1000x faster than element iteration.

  ANTI-PATTERN: (Python loop, slow)
    result = np.zeros(10000)
    for i in range(len(data)):
        result[i] = data[i] * 2 + 3

  SOLUTION: Use NumPy vectorization
    result = data * 2 + 3

  BENCHMARK: For 100k elements:
    - Loop: 5-10 seconds
    - Vectorized: 0.01 seconds (500-1000x faster)

---
id: python-perf-pandas-apply-vs-vectorized
language: python
severity: warning
message: "Performance: Use pandas vectorized operations instead of apply() on entire DataFrame"
tags:
  - performance
  - pandas
  - data
rule:
  kind: call
  pattern: |
    df.apply(lambda x: ...)
note: |
  pandas.apply() iterates rows in Python; vectorized operations use NumPy/C.
  Vectorized operations can be 50-1000x faster.

  ANTI-PATTERN: (slow apply)
    df['result'] = df.apply(lambda row: row['a'] * row['b'], axis=1)

  SOLUTION: Use vectorization
    df['result'] = df['a'] * df['b']

  BENCHMARK: For 100k rows, min/max with apply:
    - Apply + lambda: 1000+ ms
    - Vectorized min(): 3-4 ms (300-1000x faster)

---
id: python-perf-pandas-iterrows-avoid
language: python
severity: warning
message: "Performance: Avoid iterrows(); use apply() or vectorization instead"
tags:
  - performance
  - pandas
  - data
rule:
  kind: call
  pattern: |
    for idx, row in df.iterrows():
        ...
note: |
  iterrows() returns Series objects, causing Python-level iteration (very slow).

  ANTI-PATTERN:
    for idx, row in df.iterrows():
        print(row['name'], row['age'])

  SOLUTION: Use itertuples() for faster iteration
    for row in df.itertuples():
        print(row.name, row.age)

  Or use vectorization:
    names = df['name'].values
    ages = df['age'].values

  BENCHMARK: For 100M rows:
    - iterrows: timeout (too slow)
    - Series.apply: 37 seconds
    - itertuples: ~5 seconds
    - vectorized: 0.3 seconds

---
id: python-perf-string-format-f-string
language: python
severity: info
message: "Performance: Use f-strings for string formatting when applicable"
tags:
  - performance
  - strings
rule:
  kind: call
  pattern: |
    result = "{}".format(var)
    result = "%s" % var
note: |
  f-strings have special bytecode optimization (OPCODE level).
  f-strings are faster than .format() and % operator.

  ANTI-PATTERN:
    message = "User: {}, ID: {}".format(name, user_id)

  SOLUTION: Use f-string
    message = f"User: {name}, ID: {user_id}"

  BENCHMARK: f-strings are ~50% faster than .format()
  Difference grows with complexity and frequency

---
id: python-perf-string-multiple-concat
language: python
severity: info
message: "Performance: For complex string building, use f-strings rather than multiple +"
tags:
  - performance
  - strings
rule:
  kind: assignment
  pattern: |
    result = var1 + var2 + var3 + var4
note: |
  Multiple concatenations trigger multiple allocations.
  f-strings and join() are more efficient.

  ANTI-PATTERN:
    full_url = protocol + "://" + host + ":" + str(port) + path

  SOLUTION: Use f-string
    full_url = f"{protocol}://{host}:{port}{path}"

  BENCHMARK: f-string is faster and more readable for complex concatenation

---
id: python-perf-slots-reduce-memory
language: python
severity: info
message: "Performance: Use __slots__ in classes with many instances to reduce memory"
tags:
  - performance
  - memory
  - classes
rule:
  kind: class_definition
  pattern: |
    class MyClass:
        def __init__(self, x, y):
            self.x = x
            self.y = y
note: |
  By default, Python instances have __dict__ for dynamic attributes (overhead).
  __slots__ prevents __dict__ creation, reducing per-object memory by 40-50%.

  ANTI-PATTERN: (has __dict__ overhead)
    class Point:
        def __init__(self, x, y):
            self.x = x
            self.y = y

  SOLUTION: Use __slots__ for fixed attributes
    class Point:
        __slots__ = ('x', 'y')
        def __init__(self, x, y):
            self.x = x
            self.y = y

  MEMORY IMPACT: For 1M instances, __slots__ saves 100-200 MB (40-50% reduction)
  Trade-off: Can't add new attributes dynamically

---
id: python-perf-global-variable-lookup
language: python
severity: info
message: "Performance: Cache global variable access in local variable within hot loops"
tags:
  - performance
  - functions
  - scope
rule:
  kind: assignment
  pattern: |
    global_var = some_global
    for ... in ...:
        use(global_var)
note: |
  Global variable lookups use dict lookup (slower than local array lookup).
  In CPython 3.11+, this is less of an issue due to instruction specialization.
  However, accessing module attributes and deep chains still benefits from caching.

  LESS IMPORTANT NOW (CPython 3.11+):
    for i in range(1000000):
        use(sys.version)  # Direct global is now optimized

  STILL WORTH CACHING (attribute chain):
    json_dumps = json.dumps
    for item in items:
        json_dumps(item)  # Avoids repeated attribute lookups

  BENCHMARK (CPython <3.11): 2-5% improvement
  BENCHMARK (CPython 3.11+): <1% improvement for simple globals
  Still relevant for: module.function chains, deep attribute access

---
id: python-perf-function-call-overhead
language: python
severity: info
message: "Performance: Minimize function calls in hot loops"
tags:
  - performance
  - functions
rule:
  kind: call
  pattern: |
    for item in items:
        expensive_function(item)
note: |
  Function calls have overhead: stack frame creation, argument binding, return.
  In tight loops, this overhead adds up.

  PATTERN TO WATCH:
    for item in huge_list:
        result = expensive_validation(item)

  OPTIMIZATION:
    - Consider inlining simple functions
    - Use built-in functions (implemented in C)
    - Vectorize if possible
    - Use lambda or local functions to reduce namespace lookup

  GUIDELINE: Function call overhead ~100-200 ns each
  For 1M calls: 100-200 ms overhead just from calling

---
id: python-perf-local-variable-caching
language: python
severity: info
message: "Performance: Cache built-in function references in hot loops"
tags:
  - performance
  - functions
  - scope
rule:
  kind: call
  pattern: |
    for item in items:
        len(item)
note: |
  Built-in lookups like len, int, str involve namespace lookups.

  PATTERN TO WATCH:
    for item in items:
        for x in range(len(item)):  # len lookup each iteration
            ...

  OPTIMIZATION:
    item_len = len(item)
    for x in range(item_len):  # Single lookup
        ...

  GUIDELINE: Caching built-in functions can provide 2-5% improvement in tight loops
  Modern Python optimizes some of these, but not all

---
id: python-perf-lru-cache-decorator
language: python
severity: info
message: "Performance: Use @lru_cache for expensive pure functions with repeated arguments"
tags:
  - performance
  - caching
  - functions
rule:
  kind: function_definition
  pattern: |
    def expensive_func(arg):
        ...
        return result
note: |
  @lru_cache from functools memoizes results based on arguments.
  Dramatically speeds up functions called repeatedly with same arguments.

  PATTERN:
    def fibonacci(n):
        if n < 2:
            return n
        return fibonacci(n-1) + fibonacci(n-2)

  SOLUTION:
    from functools import lru_cache

    @lru_cache(maxsize=128)
    def fibonacci(n):
        if n < 2:
            return n
        return fibonacci(n-1) + fibonacci(n-2)

  BENCHMARK: fibonacci(100) without cache times out, with cache: instant
  Use for: Pure functions, expensive I/O operations, recursive algorithms
  REQUIREMENT: Arguments must be hashable (no lists, dicts)

---
id: python-perf-cache-decorator
language: python
severity: info
message: "Performance: Use @cache for unlimited memoization of pure functions"
tags:
  - performance
  - caching
  - functions
rule:
  kind: function_definition
  pattern: |
    @lru_cache(maxsize=None)
    def func(arg):
        ...
note: |
  Python 3.9+ provides @cache which is equivalent to lru_cache(maxsize=None).
  Slightly faster than lru_cache with maxsize, no eviction overhead.

  ANTI-PATTERN (Python 3.9+):
    from functools import lru_cache
    @lru_cache(maxsize=None)
    def compute(x):
        return expensive_op(x)

  SOLUTION:
    from functools import cache
    @cache
    def compute(x):
        return expensive_op(x)

  BENEFIT: Faster and cleaner than lru_cache(maxsize=None)
  Use when: Cache should never evict values

---
id: python-perf-lru-cache-maxsize
language: python
severity: info
message: "Performance: Set lru_cache maxsize to power of 2 for optimal hash performance"
tags:
  - performance
  - caching
  - functions
rule:
  kind: decorator
  pattern: |
    @lru_cache(maxsize=100)
note: |
  Hash table performance is optimal with power-of-2 sizes.

  ANTI-PATTERN:
    @lru_cache(maxsize=100)
    @lru_cache(maxsize=1000)

  SOLUTION: Use power of 2
    @lru_cache(maxsize=128)
    @lru_cache(maxsize=256)
    @lru_cache(maxsize=512)

  RECOMMENDATION: Common sizes: 128, 256, 512 (default is 128)
  IMPACT: Better hash distribution, fewer collisions

---
id: python-perf-lazy-import
language: python
severity: info
message: "Performance: Use lazy imports for expensive modules to reduce startup time"
tags:
  - performance
  - imports
rule:
  kind: import_statement
  pattern: |
    import heavy_module

    def rarely_used():
        ...
note: |
  Importing modules at module level causes startup overhead.
  Lazy imports (import inside function) defer until actually needed.

  ANTI-PATTERN: (entire app waits for import)
    import tensorflow  # 10 seconds on import
    import pandas     # 5 seconds on import

    def rarely_called():
        return pandas.DataFrame()

  SOLUTION: Import in function when needed
    def rarely_called():
        import pandas
        return pandas.DataFrame()

  BENEFIT: Python stdlib shows ~17% of imports are already in functions
  Startup time improvements: 30-70% for CLI tools with heavy dependencies
  No runtime overhead after first import (CPython adaptive optimization)

---
id: python-perf-circular-import-avoid
language: python
severity: warning
message: "Performance: Avoid circular imports; they cause repeated module loading overhead"
tags:
  - performance
  - imports
rule:
  kind: import_statement
  pattern: |
    # module_a.py
    import module_b
    # module_b.py
    import module_a
note: |
  Circular imports force Python to repeatedly load and parse modules.
  Causes startup slowness and can cause unexpected import failures.

  ANTI-PATTERN: (circular dependency)
    # user.py
    import order
    class User:
        def get_orders(self):
            return order.Order.all()

    # order.py
    import user
    class Order:
        def get_user(self):
            return user.User.get(self.user_id)

  SOLUTION: Import inside function or restructure
    # user.py
    class User:
        def get_orders(self):
            import order
            return order.Order.all()

  Or create shared module:
    # models.py (shared)
    class User: ...
    class Order: ...

---
id: python-perf-builtin-functions
language: python
severity: info
message: "Performance: Prefer built-in functions over manual loops (e.g., sum, map, filter)"
tags:
  - performance
  - functions
  - collections
rule:
  kind: call
  pattern: |
    result = 0
    for item in items:
        result += item
note: |
  Built-in functions are implemented in C, much faster than Python loops.

  ANTI-PATTERN: (Python loop)
    total = 0
    for x in numbers:
        total += x

  SOLUTION: Use built-in sum()
    total = sum(numbers)

  BENCHMARK: For 1M integers, built-in sum() is 5-10x faster
  Other examples: min(), max(), any(), all(), len()

---
id: python-perf-exception-control-flow
language: python
severity: warning
message: "Performance: Avoid using exceptions for normal control flow"
tags:
  - performance
  - correctness
rule:
  kind: try_except
  pattern: |
    try:
        value = dict[key]
    except KeyError:
        value = default
note: |
  Exceptions are slow (100-1000x slower than normal checks).
  Except blocks should handle exceptional cases, not normal flow.

  ANTI-PATTERN: (exception in common path)
    try:
        user = users[user_id]
    except KeyError:
        user = create_default_user()

  SOLUTION: Check first
    user = users.get(user_id) or create_default_user()

  EXCEPTION OVERHEAD: 1-10 microseconds per exception vs nanoseconds for checks
  For frequent operations, this is significant

---
id: python-perf-multiprocessing-cpu-bound
language: python
severity: info
message: "Performance: Use multiprocessing for CPU-bound tasks; threading doesn't parallelize due to GIL"
tags:
  - performance
  - concurrency
  - threading
rule:
  kind: import_statement
  pattern: |
    from threading import Thread
    for item in items:
        Thread(target=cpu_bound_task, args=(item,)).start()
note: |
  Python's GIL (Global Interpreter Lock) prevents true parallelism in threads.
  CPU-bound work in threads runs sequentially, no speedup.
  Multiprocessing uses separate Python processes, each with own GIL.

  ANTI-PATTERN: (threading for CPU work - no parallel gain)
    from threading import Thread
    threads = []
    for i in range(4):
        t = Thread(target=expensive_computation, args=(i,))
        threads.append(t)
        t.start()

  SOLUTION: Use multiprocessing for CPU-bound
    from multiprocessing import Pool
    with Pool(4) as p:
        results = p.map(expensive_computation, range(4))

  GUIDELINE:
    - Threading: I/O-bound tasks (network, disk, database)
    - Multiprocessing: CPU-bound tasks (math, processing, computation)

---
id: python-perf-threading-io-bound
language: python
severity: info
message: "Performance: Use threading or asyncio for I/O-bound tasks, not multiprocessing"
tags:
  - performance
  - concurrency
  - threading
rule:
  kind: import_statement
  pattern: |
    from multiprocessing import Pool
    with Pool() as p:
        results = p.map(fetch_url, urls)
note: |
  Multiprocessing has higher overhead than threading (separate processes).
  For I/O-bound work, threading is sufficient and faster.
  asyncio is even better for I/O (single-threaded, non-blocking).

  ANTI-PATTERN: (multiprocessing for I/O)
    from multiprocessing import Pool
    with Pool(4) as p:
        data = p.map(requests.get, urls)  # Overkill

  SOLUTION: Use threading for I/O
    from concurrent.futures import ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(requests.get, urls))

  BETTER SOLUTION: Use asyncio
    import asyncio
    import aiohttp
    async def main():
        async with aiohttp.ClientSession() as session:
            tasks = [session.get(url) for url in urls]
            results = await asyncio.gather(*tasks)

---
id: python-perf-buffered-io
language: python
severity: info
message: "Performance: Use buffered I/O and appropriate buffer sizes for file operations"
tags:
  - performance
  - io
rule:
  kind: call
  pattern: |
    with open(file) as f:
        for line in f:
            process(line)
note: |
  File I/O is buffered by default (typically 8KB buffer).
  For large files, larger buffers reduce number of system calls.

  GOOD (default buffering):
    with open('file.txt') as f:
        for line in f:
            print(line)

  BETTER (for large file I/O):
    with open('file.txt', buffering=65536) as f:  # 64KB buffer
        for line in f:
            print(line)

  CONSIDERATION: Larger buffer = fewer system calls but more memory
  Default (8KB) is usually fine; increase only if profiling shows I/O bottleneck

---
id: python-perf-context-manager-file
language: python
severity: warning
message: "Performance: Always use context manager (with) for file operations"
tags:
  - performance
  - io
  - resource-management
rule:
  kind: call
  pattern: |
    f = open(file)
    data = f.read()
    f.close()
note: |
  Context managers ensure files are closed promptly, freeing resources.
  Without context manager, file cleanup may be delayed.

  ANTI-PATTERN: (manual close, slower resource cleanup)
    f = open('file.txt')
    data = f.read()
    f.close()

  SOLUTION: Use context manager
    with open('file.txt') as f:
        data = f.read()

  BENEFIT: Automatic cleanup, exception-safe, more Pythonic

---
id: python-perf-file-readlines-avoid
language: python
severity: warning
message: "Performance: Avoid readlines() for large files; iterate directly instead"
tags:
  - performance
  - io
  - memory
rule:
  kind: call
  pattern: |
    lines = f.readlines()
    for line in lines:
        process(line)
note: |
  readlines() loads entire file into memory as list.
  Direct iteration reads lines lazily from buffer.

  ANTI-PATTERN: (loads entire file in memory)
    with open('large_file.txt') as f:
        lines = f.readlines()
        for line in lines:
            print(line)

  SOLUTION: Iterate directly
    with open('large_file.txt') as f:
        for line in f:
            print(line)

  MEMORY IMPACT: Direct iteration uses constant memory regardless of file size
  readlines() memory grows linearly with file size

---
id: python-perf-dict-lookup-efficiency
language: python
severity: info
message: "Performance: Initialize dict with default values at construction when possible"
tags:
  - performance
  - collections
rule:
  kind: call
  pattern: |
    result = {}
    for item in items:
        if item not in result:
            result[item] = []
        result[item].append(data)
note: |
  Checking membership then accessing causes extra lookups.
  Use defaultdict or setdefault() to simplify.

  ANTI-PATTERN:
    from collections import defaultdict
    result = {}
    for item in items:
        if item not in result:
            result[item] = []
        result[item].append(data)

  SOLUTION: Use defaultdict
    from collections import defaultdict
    result = defaultdict(list)
    for item in items:
        result[item].append(data)

  ALTERNATIVE: Use setdefault
    result = {}
    for item in items:
        result.setdefault(item, []).append(data)

  BENEFIT: Reduces code, slightly faster due to fewer lookups

---
id: python-perf-sorted-large-datasets
language: python
severity: info
message: "Performance: Use key parameter in sorted() for complex sorting; avoid multiple sorts"
tags:
  - performance
  - collections
rule:
  kind: call
  pattern: |
    sorted(items, key=lambda x: x.age)
    sorted(items, key=lambda x: x.name)
note: |
  Multiple sorts require re-sorting entire dataset.
  Single sort with complex key is more efficient.

  ANTI-PATTERN: (two full sorts)
    students = sorted(students, key=lambda x: x.name)
    students = sorted(students, key=lambda x: x.age)

  SOLUTION: Single sort with tuple key for multiple criteria
    students = sorted(students, key=lambda x: (x.age, x.name))

  BENCHMARK: For 100k items, combined key sort is ~2x faster than two separate sorts

---
id: python-perf-tuple-vs-list
language: python
severity: info
message: "Performance: Use tuples instead of lists when immutability needed and no modification required"
tags:
  - performance
  - collections
rule:
  kind: assignment
  pattern: |
    coords = [x, y, z]
note: |
  Tuples are slightly faster and use less memory than lists.
  Use tuples for fixed sequences (no appending/removing).

  PATTERN:
    # Immutable coordinates
    coords = [1, 2, 3]

  SOLUTION: Use tuple
    coords = (1, 2, 3)

  BENEFIT: Slightly faster creation and access
  Can be used as dict keys (lists cannot)
  Memory: tuple uses ~20% less memory per element

---
id: python-perf-set-initialization
language: python
severity: info
message: "Performance: Use set literal {} instead of set() constructor"
tags:
  - performance
  - collections
rule:
  kind: assignment
  pattern: |
    s = set()
note: |
  Set literal is faster than set() constructor call.

  ANTI-PATTERN:
    allowed = set()
    allowed.add('admin')
    allowed.add('user')

  SOLUTION: Use set literal
    allowed = {'admin', 'user'}

  CAVEAT: Empty set must use set() since {} is empty dict
    empty_set = set()  # Correct

---
id: python-perf-generator-function
language: python
severity: info
message: "Performance: Use generator functions (yield) for large datasets instead of returning lists"
tags:
  - performance
  - generators
  - memory
rule:
  kind: function_definition
  pattern: |
    def get_items():
        items = []
        for i in range(huge_number):
            items.append(process(i))
        return items
note: |
  Generator functions use lazy evaluation and constant memory.
  Returning lists loads entire result into memory.

  ANTI-PATTERN: (entire list in memory)
    def read_large_file(path):
        lines = []
        with open(path) as f:
            for line in f:
                lines.append(line.strip())
        return lines

  SOLUTION: Use generator
    def read_large_file(path):
        with open(path) as f:
            for line in f:
                yield line.strip()

  USAGE: Works with for loops, generators, itertools, etc.
    for line in read_large_file('huge_file.txt'):
        process(line)

  MEMORY IMPACT: Constant vs O(n) for list

---
id: python-perf-itertools-chain
language: python
severity: info
message: "Performance: Use itertools.chain() to combine iterables without concatenating lists"
tags:
  - performance
  - itertools
  - generators
rule:
  kind: assignment
  pattern: |
    combined = list1 + list2 + list3
note: |
  List concatenation creates new list and copies elements.
  itertools.chain() creates generator without copying.

  ANTI-PATTERN: (creates new list each time)
    result = []
    for list_var in all_lists:
        result = result + list_var

  SOLUTION: Use itertools.chain
    import itertools
    combined = itertools.chain(list1, list2, list3)
    for item in combined:
        process(item)

  Or with unpacking:
    combined = itertools.chain.from_iterable(all_lists)

  BENCHMARK: Chain is O(1) overhead, concatenation is O(n) per operation

---
id: python-perf-itertools-islice
language: python
severity: info
message: "Performance: Use itertools.islice() to extract subset of iterable without full evaluation"
tags:
  - performance
  - itertools
  - generators
rule:
  kind: call
  pattern: |
    subset = large_list[0:1000]
note: |
  List slicing creates new list with all elements.
  itertools.islice() creates generator for requested range.

  PATTERN:
    # Get first N items from list
    first_10 = large_list[:10]

  BETTER (for large sequences):
    import itertools
    first_10 = itertools.islice(large_list, 10)

  BENEFIT: Lazy evaluation, constant memory regardless of list size

---
id: python-perf-zip-multiple-iterables
language: python
severity: info
message: "Performance: Use zip() to iterate multiple sequences; avoids index-based loops"
tags:
  - performance
  - collections
rule:
  kind: call
  pattern: |
    for i in range(len(list1)):
        process(list1[i], list2[i])
note: |
  Index-based loops require len() calls and index lookups.
  zip() directly iterates corresponding elements.

  ANTI-PATTERN:
    names = ['Alice', 'Bob', 'Charlie']
    ages = [25, 30, 35]
    for i in range(len(names)):
        print(f"{names[i]} is {ages[i]}")

  SOLUTION: Use zip
    for name, age in zip(names, ages):
        print(f"{name} is {age}")

  BENEFIT: Cleaner, more Pythonic, slightly faster

---
id: python-perf-enumerate-index-loop
language: python
severity: info
message: "Performance: Use enumerate() instead of range(len()) for index + value loops"
tags:
  - performance
  - collections
rule:
  kind: call
  pattern: |
    for i in range(len(items)):
        print(i, items[i])
note: |
  range(len()) requires len() call and index lookup.
  enumerate() provides index and value directly.

  ANTI-PATTERN:
    for i in range(len(colors)):
        print(f"Color {i}: {colors[i]}")

  SOLUTION: Use enumerate
    for i, color in enumerate(colors):
        print(f"Color {i}: {color}")

  BENEFIT: Faster, more readable, Pythonic

---
id: python-perf-map-vs-comprehension
language: python
severity: info
message: "Performance: Use map() for simple transformations; list comprehension for complex logic"
tags:
  - performance
  - functions
  - collections
rule:
  kind: call
  pattern: |
    result = [func(x) for x in items]
note: |
  map() is optimized in C; list comprehension is more flexible.
  For simple single-function calls, map() is slightly faster.

  WHEN TO USE MAP:
    # Simple function application
    numbers = [1, 2, 3, 4, 5]
    doubled = map(lambda x: x * 2, numbers)
    # Or with function
    doubled = map(str.upper, strings)

  WHEN TO USE COMPREHENSION:
    # Complex logic
    result = [x * 2 for x in items if x > 0]

  HYBRID: Use generator expression with map
    doubled = (x * 2 for x in numbers)

---
id: python-perf-filter-vs-comprehension
language: python
severity: info
message: "Performance: Use filter() for simple conditions; comprehension for complex filtering"
tags:
  - performance
  - functions
  - collections
rule:
  kind: call
  pattern: |
    result = [x for x in items if condition(x)]
note: |
  filter() is slightly faster for simple conditions.
  Comprehension is more readable for complex logic.

  SIMPLE CONDITION (filter is fine):
    evens = filter(lambda x: x % 2 == 0, numbers)

  COMPLEX LOGIC (use comprehension):
    valid = [x for x in items if validate(x) and x > 0 and x < 100]

  RECOMMENDATION: Prefer comprehensions for readability
  Use filter() when already using functional style

---
id: python-perf-for-else-optimization
language: python
severity: info
message: "Performance: Use for-else to avoid flag variables in search loops"
tags:
  - performance
  - loops
rule:
  kind: for_statement
  pattern: |
    found = False
    for item in items:
        if item == target:
            found = True
            break
    if found:
        ...
note: |
  Python for-else eliminates need for flag variables.
  for-else block runs if loop completes without break.

  ANTI-PATTERN: (flag variable)
    found = False
    for user in users:
        if user.id == target_id:
            found = True
            break
    if found:
        grant_access()

  SOLUTION: Use for-else
    for user in users:
        if user.id == target_id:
            break
    else:
        raise ValueError("User not found")

  BENEFIT: Cleaner code, eliminates flag variable

---
id: python-perf-dict-setdefault
language: python
severity: info
message: "Performance: Use dict.setdefault() instead of get() + assignment"
tags:
  - performance
  - collections
rule:
  kind: assignment
  pattern: |
    if key not in dict_var:
        dict_var[key] = default
note: |
  setdefault() performs lookup and assignment atomically.
  Avoids double lookup (check + assign).

  ANTI-PATTERN:
    if 'count' not in counters:
        counters['count'] = 0
    counters['count'] += 1

  SOLUTION: Use setdefault
    counters.setdefault('count', 0)
    counters['count'] += 1

  BETTER: Use Counter
    from collections import Counter
    counters = Counter()
    counters['count'] += 1

---
id: python-perf-copy-vs-reference
language: python
severity: info
message: "Performance: Avoid unnecessary deep copies; use references when possible"
tags:
  - performance
  - memory
rule:
  kind: call
  pattern: |
    copy.deepcopy(large_object)
note: |
  Deep copying is expensive (recursive traversal).
  Use references when modification isn't needed.

  ANTI-PATTERN: (unnecessary copy)
    import copy
    backup = copy.deepcopy(original)
    use(backup)

  SOLUTION: Use reference if not modifying
    backup = original  # Shared reference

  IF MODIFICATION NEEDED: Use shallow copy
    import copy
    modified = copy.copy(original)

  BENCHMARK: deepcopy can be 100-1000x slower than reference assignment

---
id: python-perf-reduce-function
language: python
severity: info
message: "Performance: Use reduce() for cumulative operations; watch for performance trade-offs"
tags:
  - performance
  - functions
rule:
  kind: call
  pattern: |
    result = items[0]
    for item in items[1:]:
        result = operation(result, item)
note: |
  reduce() from functools combines iteration with operation.
  Sometimes cleaner than explicit loop.

  PATTERN:
    # Calculate product of all numbers
    result = 1
    for x in numbers:
        result *= x

  SOLUTION: Use reduce
    from functools import reduce
    import operator
    product = reduce(operator.mul, numbers, 1)

  NOTE: For simple aggregations, built-ins are faster
    # Better: Use built-in sum, max, min
    total = sum(numbers)

  GUIDELINE: Use reduce() for custom operations
  Use built-in functions for standard operations (sum, max, min, any, all)

---
id: python-perf-walrus-operator
language: python
severity: info
message: "Performance: Use walrus operator (:=) to avoid redundant function calls"
tags:
  - performance
  - syntax
rule:
  kind: assignment
  pattern: |
    if condition:
        result = expensive_function()
note: |
  Walrus operator (:=) assigns and uses value in same expression.
  Avoids calling function twice in check + usage pattern.

  BEFORE (Python <3.8):
    data = process(item)
    if data:
        use(data)

  WITH WALRUS (Python 3.8+):
    if (data := process(item)):
        use(data)

  BENEFIT: Single function call instead of potential duplicate
  Also useful in comprehensions to avoid calling function per element

---
id: python-perf-division-vs-multiplication
language: python
severity: info
message: "Performance: Use multiplication by reciprocal instead of repeated division in loops"
tags:
  - performance
  - math
rule:
  kind: binary_operation
  pattern: |
    for x in data:
        result = x / constant
note: |
  Division is slower than multiplication.
  When dividing by constant repeatedly, compute reciprocal once.

  ANTI-PATTERN: (repeated division)
    for value in large_list:
        normalized = value / 1000000

  SOLUTION: Multiply by reciprocal
    factor = 1.0 / 1000000
    for value in large_list:
        normalized = value * factor

  BENCHMARK: Multiplication ~1.5-2x faster than division on most CPUs

---
id: python-perf-modulo-bitwise
language: python
severity: info
message: "Performance: Use bitwise operations instead of modulo for power-of-2 operations"
tags:
  - performance
  - math
rule:
  kind: binary_operation
  pattern: |
    index = value % (2 ** n)
note: |
  Bitwise AND is faster than modulo for power-of-2 masks.

  PATTERN: Check if power of 2
    if x % 16 == 0:  # Every 16th element

  SOLUTION: Use bitwise AND
    if not (x & 0xF):  # Same as % 16 == 0

  MORE READABLE VERSION:
    MASK = 0b1111  # or 0xF or 15
    if x & MASK == 0:

  BENCHMARK: Bitwise is 2-5x faster for tight loops

---
id: python-perf-boolean-short-circuit
language: python
severity: info
message: "Performance: Place fastest conditions first in boolean expressions"
tags:
  - performance
  - logic
rule:
  kind: boolean_operation
  pattern: |
    if expensive_check() and fast_check():
        ...
note: |
  Python short-circuits boolean operators (stops at first False/True).
  Arrange conditions so fast/likely checks come first.

  ANTI-PATTERN:
    if is_premium_user() and user.account_verified:  # Slow call first
        ...

  SOLUTION: Fast check first
    if user.account_verified and is_premium_user():
        ...

  GUIDELINE: Order checks by:
    1. Fastest checks first
    2. Most likely to fail first (stops evaluation)
    3. Expensive checks last

---
id: python-perf-math-sqrt
language: python
severity: info
message: "Performance: Use squared values instead of sqrt() for comparisons"
tags:
  - performance
  - math
rule:
  kind: call
  pattern: |
    if math.sqrt(x) > threshold:
note: |
  sqrt() is expensive. For comparisons, square both sides.

  ANTI-PATTERN:
    import math
    if math.sqrt(distance) > max_radius:
        ...

  SOLUTION: Square both sides
    if distance > max_radius ** 2:
        ...

  BENEFIT: Avoids expensive sqrt() call

---
id: python-perf-string-split-for-condition
language: python
severity: info
message: "Performance: Use string methods instead of split() when checking for substrings"
tags:
  - performance
  - strings
rule:
  kind: call
  pattern: |
    if "," in string.split(","):
        ...
note: |
  split() creates list even if only checking for substring.

  ANTI-PATTERN:
    if "python" in text.split():
        ...

  SOLUTION: Use "in" operator directly
    if "python" in text:
        ...

  BENEFIT: No list creation, O(n) string search instead of O(n) split + list search

---
id: python-perf-constant-folding
language: python
severity: info
message: "Performance: Pre-compute constants instead of computing in hot loops"
tags:
  - performance
  - optimization
rule:
  kind: binary_operation
  pattern: |
    for x in items:
        result = x * (3.14159 * 2)
note: |
  Computing same constant repeatedly wastes CPU cycles.

  ANTI-PATTERN:
    for radius in radii:
        circumference = radius * (3.14159 * 2)

  SOLUTION: Pre-compute constant
    TWO_PI = 3.14159 * 2
    for radius in radii:
        circumference = radius * TWO_PI

  BETTER: Import from math
    from math import pi, tau
    for radius in radii:
        circumference = radius * tau

---
id: python-perf-string-lower-case-matching
language: python
severity: info
message: "Performance: Pre-compute case conversions for case-insensitive comparisons in loops"
tags:
  - performance
  - strings
rule:
  kind: call
  pattern: |
    for item in items:
        if item.lower() == target.lower():
note: |
  Converting to lowercase repeatedly in loop wastes CPU.

  ANTI-PATTERN:
    target = "Admin"
    for item in user_list:
        if item.lower() == target.lower():
            process(item)

  SOLUTION: Convert once before loop
    target_lower = target.lower()
    for item in user_list:
        if item.lower() == target_lower:
            process(item)

  BENEFIT: Single conversion instead of N conversions

---
id: python-perf-regex-compile
language: python
severity: info
message: "Performance: Compile regex patterns outside loops with re.compile()"
tags:
  - performance
  - regex
rule:
  kind: call
  pattern: |
    for text in texts:
        if re.match(pattern, text):
note: |
  Compiling regex repeatedly wastes time.

  ANTI-PATTERN:
    import re
    for email in emails:
        if re.match(r'[^@]+@[^@]+\.[^@]+', email):
            validate(email)

  SOLUTION: Compile once
    import re
    email_pattern = re.compile(r'[^@]+@[^@]+\.[^@]+')
    for email in emails:
        if email_pattern.match(email):
            validate(email)

  BENCHMARK: Compiled regex ~100x faster in tight loops

---
id: python-perf-avoid-string-escape
language: python
severity: info
message: "Performance: Use raw strings for regex/paths to avoid escape processing"
tags:
  - performance
  - strings
rule:
  kind: string_literal
  pattern: |
    path = "C:\\Users\\Name\\file.txt"
    regex = "\\d+\\."
note: |
  Non-raw strings process escape sequences, slower.
  Raw strings skip escape processing.

  ANTI-PATTERN:
    regex = "\\d+\\.\\w+"
    path = "C:\\Users\\Documents\\file.txt"

  SOLUTION: Use raw strings
    regex = r"\d+\.\w+"
    path = r"C:\Users\Documents\file.txt"

  BENEFIT: Cleaner, faster, prevents escape errors

---
id: python-perf-list-copy-vs-slice
language: python
severity: info
message: "Performance: Use list slicing [:] for shallow copy, not copy.copy()"
tags:
  - performance
  - collections
rule:
  kind: call
  pattern: |
    import copy
    new_list = copy.copy(original_list)
note: |
  Slicing is faster than copy module for lists.

  PATTERN:
    original = [1, 2, 3]
    copy_module = copy.copy(original)

  SOLUTION: Use slice
    original = [1, 2, 3]
    copy_list = original[:]

  OR: Use list constructor
    copy_list = list(original)

  BENCHMARK: Slicing is ~2x faster than copy.copy()

---
id: python-perf-list-extend-vs-concat
language: python
severity: info
message: "Performance: Use list.extend() instead of += with new list"
tags:
  - performance
  - collections
rule:
  kind: assignment
  pattern: |
    result += [new_item]
note: |
  += with list literal creates new list, += with reference extends in-place.

  ANTI-PATTERN:
    items = [1, 2, 3]
    items = items + [4]  # Creates new list

  SOLUTION: Use extend()
    items = [1, 2, 3]
    items.extend([4])  # In-place extension
    items += [4]  # Also works in-place here due to CPython optimization

  NOTE: Modern CPython optimizes += for lists, but extend() is clearer

---
id: python-perf-in-memory-cache
language: python
severity: info
message: "Performance: Use dictionary as in-memory cache for expensive lookups"
tags:
  - performance
  - caching
  - collections
rule:
  kind: function_definition
  pattern: |
    def lookup_by_id(item_id):
        for item in all_items:
            if item.id == item_id:
                return item
note: |
  Linear search through collection is O(n).
  Dictionary lookup is O(1).

  ANTI-PATTERN: (linear search)
    def find_user(user_id):
        for user in all_users:
            if user.id == user_id:
                return user

  SOLUTION: Cache in dictionary
    user_cache = {user.id: user for user in all_users}
    def find_user(user_id):
        return user_cache.get(user_id)

  BENCHMARK: Dictionary lookup is 100-1000x faster for large collections

---
id: python-perf-slice-bounds-check
language: python
severity: info
message: "Performance: Avoid bounds checking; Python handles slice indices gracefully"
tags:
  - performance
  - collections
rule:
  kind: call
  pattern: |
    if len(items) > 10:
        subset = items[:10]
note: |
  Python slicing handles out-of-bounds indices gracefully.
  Explicit bounds checking is unnecessary and slower.

  ANTI-PATTERN:
    if len(items) > 100:
        first_100 = items[:100]
    else:
        first_100 = items

  SOLUTION: Just slice directly
    first_100 = items[:100]  # Works regardless of length

  BENEFIT: Simpler code, no bounds check overhead

---
id: python-perf-while-true-break
language: python
severity: info
message: "Performance: Use while True with break for cleaner control flow in some cases"
tags:
  - performance
  - loops
  - style
rule:
  kind: while_statement
  pattern: |
    while condition:
        ...
        if inner_condition:
            break
note: |
  while True with break can be clearer than complex loop conditions.
  No performance difference (same bytecode).

  WHEN USEFUL:
    # Multiple exit conditions
    while True:
        data = queue.get()
        if data is None:  # Poison pill
            break
        process(data)

  VS:
    # More complex condition
    while queue.qsize() > 0 or not queue.empty():
        data = queue.get()
        if data is None:
            break
        process(data)

---
id: python-perf-large-constant-lists
language: python
severity: info
message: "Performance: Use frozenset instead of list for large immutable constant collections"
tags:
  - performance
  - collections
rule:
  kind: assignment
  pattern: |
    ALLOWED = ['admin', 'moderator', 'user', ...]
note: |
  List lookup is O(n); frozenset lookup is O(1).
  Frozenset is immutable and hashable.

  ANTI-PATTERN: (slow list lookup)
    ROLES = ['admin', 'moderator', 'user', 'guest', 'banned']
    if role in ROLES:
        ...

  SOLUTION: Use frozenset
    ROLES = frozenset(['admin', 'moderator', 'user', 'guest', 'banned'])
    if role in ROLES:
        ...

  BENEFIT: O(1) lookup, immutable, can be used as dict key

---
id: python-perf-bytes-vs-string-io
language: python
severity: info
message: "Performance: Use BytesIO for binary data accumulation; StringIO for text"
tags:
  - performance
  - io
  - memory
rule:
  kind: assignment
  pattern: |
    data = b''
    for chunk in chunks:
        data += chunk
note: |
  String/binary concatenation in loops is O(n²).
  IO classes use efficient internal buffers.

  ANTI-PATTERN: (repeated byte concatenation)
    result = b''
    for chunk in chunks:
        result += chunk

  SOLUTION: Use BytesIO
    from io import BytesIO
    result = BytesIO()
    for chunk in chunks:
        result.write(chunk)
    final = result.getvalue()

  BENEFIT: O(n) instead of O(n²), single allocation

---
id: python-perf-type-checking-isinstance
language: python
severity: info
message: "Performance: Use isinstance() for type checking; avoid type() ==comparison"
tags:
  - performance
  - types
rule:
  kind: comparison
  pattern: |
    if type(obj) == MyClass:
        ...
note: |
  type() == comparison doesn't consider inheritance.
  isinstance() is faster and handles subclasses.

  ANTI-PATTERN:
    if type(item) == list:
        process(item)

  SOLUTION: Use isinstance
    if isinstance(item, list):
        process(item)

  BENEFIT: Faster, more Pythonic, handles inheritance

---
id: python-perf-none-comparison
language: python
severity: info
message: "Performance: Use 'is None' for None comparison, not '== None'"
tags:
  - performance
  - style
rule:
  kind: comparison
  pattern: |
    if value == None:
        ...
note: |
  None is singleton; 'is' checks identity (faster than equality).

  ANTI-PATTERN:
    if result == None:
        ...

  SOLUTION: Use 'is'
    if result is None:
        ...

  BENCHMARK: 'is' is ~5-10% faster (identity check vs equality)

---
id: python-perf-truthy-check
language: python
severity: info
message: "Performance: Use truthiness checks instead of len() comparisons"
tags:
  - performance
  - style
rule:
  kind: comparison
  pattern: |
    if len(items) > 0:
        ...
note: |
  Truthiness check is faster than len() call.
  Empty collection is falsy, non-empty is truthy.

  ANTI-PATTERN:
    if len(items) > 0:
        process(items)
    if len(items) == 0:
        raise ValueError

  SOLUTION: Use truthiness
    if items:
        process(items)
    if not items:
        raise ValueError

  BENCHMARK: Truthiness is faster (no function call overhead)
  Note: Some types (pandas DataFrames) may need explicit len() for clarity

---
id: python-perf-dataclass-slots
language: python
severity: info
message: "Performance: Use dataclass with slots=True for memory efficiency (Python 3.10+)"
tags:
  - performance
  - memory
  - classes
rule:
  kind: class_definition
  pattern: |
    @dataclass
    class Point:
        x: float
        y: float
note: |
  Python 3.10+ dataclasses support __slots__ parameter.
  Reduces memory overhead of instances.

  MODERN APPROACH (Python 3.10+):
    from dataclasses import dataclass

    @dataclass(slots=True)
    class Point:
        x: float
        y: float

  OLDER APPROACH:
    from dataclasses import dataclass

    @dataclass
    class Point:
        x: float
        y: float
        __slots__ = ('x', 'y')

  BENEFIT: 40-50% memory reduction per instance with many instances

---
id: python-perf-match-statement
language: python
severity: info
message: "Performance: Use match statement instead of multiple if-elif for clarity (Python 3.10+)"
tags:
  - performance
  - style
  - syntax
rule:
  kind: if_statement
  pattern: |
    if status == 'active':
        ...
    elif status == 'inactive':
        ...
note: |
  Python 3.10+ match statement is cleaner, performs similar to if-elif.
  Better readability, not necessarily faster.

  BEFORE (Python <3.10):
    if status == 'active':
        start_service()
    elif status == 'inactive':
        stop_service()
    else:
        raise ValueError

  MODERN (Python 3.10+):
    match status:
        case 'active':
            start_service()
        case 'inactive':
            stop_service()
        case _:
            raise ValueError

  NOTE: Performance similar to if-elif, but more expressive

---
id: python-perf-walrus-function-call
language: python
severity: info
message: "Performance: Use walrus operator to cache function results in conditionals"
tags:
  - performance
  - syntax
rule:
  kind: if_statement
  pattern: |
    if condition_function():
        value = get_value()
        use(value)
note: |
  Walrus operator avoids calling function multiple times.

  PATTERN (checking result then using it):
    data = fetch_data()
    if data:
        process(data)

  WITH WALRUS (combine check and assignment):
    if (data := fetch_data()):
        process(data)

  BENEFIT: Combines operation and check, avoids duplicate calls
  Useful with expensive functions

---
id: python-perf-global-const-lookup
language: python
severity: info
message: "Performance: Define module-level constants instead of computing them repeatedly"
tags:
  - performance
  - functions
rule:
  kind: constant_definition
  pattern: |
    def get_config():
        return {'timeout': 30, 'retries': 3}
note: |
  Computing same constants in function calls wastes CPU.
  Define once at module level.

  ANTI-PATTERN:
    def get_config():
        return {'timeout': 30, 'retries': 3, 'hosts': ['a', 'b', 'c']}

    config = get_config()

  SOLUTION: Module constant
    DEFAULT_CONFIG = {'timeout': 30, 'retries': 3, 'hosts': ['a', 'b', 'c']}
    config = DEFAULT_CONFIG

  BENEFIT: Single allocation, fastest access time
# JavaScript/Node.js Performance Rules
# Comprehensive ruleset covering 60+ performance anti-patterns and best practices
# Categories: DOM, Loops, Memory, Async, Objects, Strings, Arrays, Bundling, V8, Event Loop

---
id: javascript-perf-dom-batch-reads-writes
language: javascript
severity: warning
message: "Performance: DOM layout thrashing detected - separate read and write operations"
tags:
  - performance
  - dom
  - layout-thrashing
rule:
  kind: block_statement
  pattern: |
    element.offsetHeight;
    element.style.height = '100px';
note: |
  Layout thrashing occurs when DOM reads and writes are interleaved, forcing the browser to
  recalculate layout multiple times. Group all reads first, then all writes.

  Recommended: Cache DOM properties and batch updates.
  ```javascript
  // BAD: Layout thrashing
  element.style.width = element.offsetWidth + 10 + 'px';

  // GOOD: Batch operations
  const width = element.offsetWidth;
  element.style.width = (width + 10) + 'px';
  ```

---
id: javascript-perf-dom-reflow-in-loop
language: javascript
severity: warning
message: "Performance: DOM manipulation in loop causes repeated reflows"
tags:
  - performance
  - dom
  - loops
rule:
  kind: for_statement
  pattern: |
    for (let i = 0; i < items.length; i++) {
      document.appendChild(...)
    }
note: |
  Each appendChild triggers a reflow. Use DocumentFragment to batch multiple DOM operations.

  Recommended: Accumulate elements in a DocumentFragment before adding to DOM.
  ```javascript
  // BAD: Multiple reflows
  items.forEach(item => {
    const el = document.createElement('div');
    document.body.appendChild(el);
  });

  // GOOD: Single reflow
  const fragment = document.createDocumentFragment();
  items.forEach(item => {
    const el = document.createElement('div');
    fragment.appendChild(el);
  });
  document.body.appendChild(fragment);
  ```

---
id: javascript-perf-dom-requery-selector
language: javascript
severity: info
message: "Performance: DOM element queried multiple times instead of cached"
tags:
  - performance
  - dom
  - caching
rule:
  kind: member_expression
  pattern: "document.querySelector"
note: |
  Repeatedly querying the DOM is expensive. Cache selectors in variables.

  Recommended: Use a variable to reference the element once.
  ```javascript
  // BAD
  document.querySelector('.button').addEventListener('click', handler);
  document.querySelector('.button').style.color = 'red';

  // GOOD
  const button = document.querySelector('.button');
  button.addEventListener('click', handler);
  button.style.color = 'red';
  ```

---
id: javascript-perf-requestanimationframe-over-settimeout
language: javascript
severity: warning
message: "Performance: Use requestAnimationFrame for DOM animations instead of setTimeout"
tags:
  - performance
  - dom
  - animation
rule:
  kind: call_expression
  pattern: "setTimeout.*1000/60"
note: |
  setTimeout with interval approximating 16.67ms doesn't sync with browser refresh rate.
  requestAnimationFrame ensures updates happen before repaint.

  Recommended: Use requestAnimationFrame for any animation or visual updates.
  ```javascript
  // BAD
  setTimeout(() => { element.style.left = x + 'px'; }, 1000/60);

  // GOOD
  requestAnimationFrame(() => { element.style.left = x + 'px'; });
  ```

---
id: javascript-perf-for-loop-faster-than-foreach
language: javascript
severity: info
message: "Performance: Use for loop instead of forEach for large arrays where speed matters"
tags:
  - performance
  - loops
  - array-methods
rule:
  kind: call_expression
  pattern: "arr.forEach"
note: |
  For loops are 3+ times faster than forEach because they avoid callback overhead.
  forEach creates new stack frames and invokes function callbacks.

  Recommended: Use for loops for performance-critical code with large arrays.
  ```javascript
  // SLOWER
  arr.forEach(item => {
    process(item);
  });

  // FASTER
  for (let i = 0; i < arr.length; i++) {
    process(arr[i]);
  }
  ```

---
id: javascript-perf-avoid-map-for-side-effects
language: javascript
severity: warning
message: "Performance: Don't use map() for side effects - use forEach or for loop instead"
tags:
  - performance
  - loops
  - array-methods
rule:
  kind: call_expression
  pattern: "arr.map.*=>"
note: |
  map() creates a new array and copies values, even if you discard the result.
  Use forEach or for loop when you only care about side effects.

  Recommended: Reserve map() for transformations where you use the returned array.
  ```javascript
  // INEFFICIENT: Creates unused array
  arr.map(item => console.log(item));

  // EFFICIENT: No unnecessary array creation
  arr.forEach(item => console.log(item));
  ```

---
id: javascript-perf-cache-array-length
language: javascript
severity: info
message: "Performance: Cache array length in loop condition for slightly better performance"
tags:
  - performance
  - loops
  - micro-optimization
rule:
  kind: for_statement
  pattern: "for (let i = 0; i < arr.length; i++)"
note: |
  While modern V8 optimizes this automatically, explicitly caching can help in older engines.

  Recommended: Cache length for maximum compatibility.
  ```javascript
  // Works but less explicit
  for (let i = 0; i < arr.length; i++) { }

  // Explicit optimization
  for (let i = 0, len = arr.length; i < len; i++) { }
  ```

---
id: javascript-perf-avoid-nested-loops
language: javascript
severity: warning
message: "Performance: Nested loops with O(n²) complexity should use hash lookups instead"
tags:
  - performance
  - loops
  - complexity
rule:
  kind: for_statement
  pattern: |
    for (let i = 0; i < arr1.length; i++) {
      for (let j = 0; j < arr2.length; j++) {
        if (arr1[i] === arr2[j]) {}
      }
    }
note: |
  O(n²) nested loops are slow for large datasets. Use Map/Set for O(n) lookup.

  Recommended: Build a lookup structure first, then iterate once.
  ```javascript
  // BAD: O(n²)
  for (let i = 0; i < arr1.length; i++) {
    for (let j = 0; j < arr2.length; j++) {
      if (arr1[i] === arr2[j]) found = true;
    }
  }

  // GOOD: O(n)
  const lookup = new Set(arr2);
  for (let i = 0; i < arr1.length; i++) {
    if (lookup.has(arr1[i])) found = true;
  }
  ```

---
id: javascript-perf-memory-leak-event-listener-not-removed
language: javascript
severity: warning
message: "Performance: Event listener not removed - potential memory leak"
tags:
  - performance
  - memory
  - event-listeners
rule:
  kind: call_expression
  pattern: "addEventListener"
note: |
  Event listeners hold references to captured variables in closures.
  If listener isn't removed, the listener and its closure prevent garbage collection.

  Recommended: Always remove listeners, especially for dynamically removed elements.
  ```javascript
  // RISKY: Listener never removed
  element.addEventListener('click', handler);

  // SAFER: Auto-remove after first call
  element.addEventListener('click', handler, { once: true });

  // BEST: Explicitly remove when done
  element.addEventListener('click', handler);
  element.removeEventListener('click', handler);
  ```

---
id: javascript-perf-memory-leak-setinterval-not-cleared
language: javascript
severity: warning
message: "Performance: setInterval without corresponding clearInterval - memory leak"
tags:
  - performance
  - memory
  - timers
rule:
  kind: call_expression
  pattern: "setInterval"
note: |
  setInterval keeps running indefinitely, holding references to captured variables.
  This prevents garbage collection of the entire closure scope.

  Recommended: Always store intervalId and clear when done.
  ```javascript
  // BAD: Leak - interval never stops
  setInterval(() => {
    updateUI(largeDataStructure);
  }, 1000);

  // GOOD: Explicitly cleared
  const intervalId = setInterval(() => {
    updateUI(largeDataStructure);
  }, 1000);

  // Later: cleanup
  clearInterval(intervalId);
  ```

---
id: javascript-perf-memory-leak-closure-captures-dom
language: javascript
severity: warning
message: "Performance: Closure captures DOM element preventing garbage collection"
tags:
  - performance
  - memory
  - closures
rule:
  kind: function_declaration
  pattern: "function.*{.*element.*}"
note: |
  Closures that capture references to DOM elements prevent garbage collection
  even after the element is removed from the DOM.

  Recommended: Clear DOM references explicitly or use weak closures.
  ```javascript
  // PROBLEMATIC: element stays in memory forever
  let element = document.getElementById('large-section');
  setInterval(() => {
    console.log(element.textContent); // closure captures element
  }, 1000);

  // BETTER: Clear reference
  let element = document.getElementById('large-section');
  const intervalId = setInterval(() => {
    if (element) console.log(element.textContent);
  }, 1000);

  // Cleanup:
  clearInterval(intervalId);
  element = null;
  ```

---
id: javascript-perf-memory-leak-global-variables
language: javascript
severity: warning
message: "Performance: Global variables prevent garbage collection"
tags:
  - performance
  - memory
  - global-scope
rule:
  kind: assignment_expression
  pattern: "^[A-Z][A-Za-z]*\\s*="
note: |
  Global variables are never garbage collected. They accumulate in memory
  throughout the entire application lifetime.

  Recommended: Use module scope, closures, or WeakMap for temporary references.
  ```javascript
  // BAD: Global persists forever
  var globalCache = {};
  function process(data) {
    globalCache[data.id] = data; // keeps growing
  }

  // GOOD: Scoped with cleanup
  (function() {
    const cache = new Map();
    function process(data) {
      cache.set(data.id, data);
      if (cache.size > 1000) cache.clear();
    }
  })();
  ```

---
id: javascript-perf-promise-all-parallel-operations
language: javascript
severity: info
message: "Performance: Use Promise.all for parallel async operations instead of sequential"
tags:
  - performance
  - async
  - promises
rule:
  kind: await_expression
  pattern: |
    await fetch(url1);
    await fetch(url2);
note: |
  Sequential awaits waste time waiting for each promise. Use Promise.all
  to execute independent promises in parallel.

  Recommended: Only use sequential await when later operations depend on earlier results.
  ```javascript
  // SLOW: Sequential, 4 seconds if each takes 2s
  const user = await fetchUser(id);
  const posts = await fetchPosts(id);
  const comments = await fetchComments(id);

  // FAST: Parallel, ~2 seconds
  const [user, posts, comments] = await Promise.all([
    fetchUser(id),
    fetchPosts(id),
    fetchComments(id)
  ]);
  ```

---
id: javascript-perf-async-await-overhead
language: javascript
severity: info
message: "Performance: Each await incurs microtask queue overhead - minimize unnecessary awaits"
tags:
  - performance
  - async
  - microtasks
rule:
  kind: await_expression
  pattern: "await .*"
note: |
  Each await creates multiple microtasks (V8 creates 2+ promises per await).
  For values already resolved, unwrap to avoid overhead.

  Recommended: Only await when necessary; chain promises for resolved values.
  ```javascript
  // OVERHEAD: Multiple awaits for already-resolved values
  const a = await Promise.resolve(1);
  const b = await Promise.resolve(2);
  const result = a + b;

  // OPTIMIZED: Resolve together
  const [a, b] = await Promise.all([
    Promise.resolve(1),
    Promise.resolve(2)
  ]);
  const result = a + b;
  ```

---
id: javascript-perf-microtask-queue-flooding
language: javascript
severity: warning
message: "Performance: Flooding microtask queue can starve rendering - use setImmediate or setTimeout(0)"
tags:
  - performance
  - event-loop
  - microtasks
rule:
  kind: call_expression
  pattern: "for.*Promise"
note: |
  Creating many promises/microtasks in sequence prevents rendering and macrotasks.
  Break heavy work into macrotasks using setTimeout or setImmediate.

  Recommended: For CPU-heavy work, use macrotasks with setImmediate or setTimeout.
  ```javascript
  // BAD: Microtask flooding blocks rendering
  async function processMany(items) {
    for (const item of items) {
      await processItem(item);
    }
  }

  // GOOD: Macrotask batches keep UI responsive
  function processMany(items) {
    let index = 0;
    function batch() {
      if (index >= items.length) return;
      // Process 10 items per macrotask
      for (let i = 0; i < 10 && index < items.length; i++) {
        processItem(items[index++]);
      }
      setImmediate(batch); // macrotask, allows rendering
    }
    batch();
  }
  ```

---
id: javascript-perf-avoid-unnecessary-promise-wrapping
language: javascript
severity: info
message: "Performance: Don't wrap already-resolved values in Promise - avoid wrapper promises"
tags:
  - performance
  - promises
  - async
rule:
  kind: call_expression
  pattern: "Promise.resolve\\(value\\)"
note: |
  Wrapping non-promise values or re-wrapping promises creates unnecessary heap allocations.
  Return values directly from async functions when possible.

  Recommended: Return values directly; only wrap when necessary.
  ```javascript
  // CREATES WRAPPER: Unnecessary allocation
  async function getValue() {
    return Promise.resolve(cachedValue);
  }

  // OPTIMIZED: Direct return
  async function getValue() {
    return cachedValue; // no wrapper promise
  }
  ```

---
id: javascript-perf-spread-operator-large-arrays
language: javascript
severity: warning
message: "Performance: Spread operator slower than slice() for copying large arrays"
tags:
  - performance
  - arrays
  - spread-operator
rule:
  kind: spread_element
  pattern: "\\[...largeArray\\]"
note: |
  Spread operator is up to 10x slower than slice() for array copying.
  Spread iterates through each element, while slice allocates and copies more efficiently.

  Recommended: Use slice() or Array.from() for large arrays.
  ```javascript
  // SLOWER: Spread operator
  const copy = [...largeArray];

  // FASTER: slice method
  const copy = largeArray.slice();

  // ALSO FAST: Array.from
  const copy = Array.from(largeArray);
  ```

---
id: javascript-perf-avoid-spreading-in-hot-path
language: javascript
severity: warning
message: "Performance: Avoid spread operator in loops or frequently called functions"
tags:
  - performance
  - arrays
  - loops
rule:
  kind: spread_element
  pattern: "\\.\\.\\."
note: |
  Each spread creates a new array and iterates all elements.
  In hot paths, this allocation overhead accumulates significantly.

  Recommended: Use array methods or traditional loops in performance-critical code.
  ```javascript
  // INEFFICIENT: Spread in hot path
  function merge(arr1, arr2) {
    return [...arr1, ...arr2]; // called 1000s of times
  }

  // BETTER: Direct concatenation
  function merge(arr1, arr2) {
    return arr1.concat(arr2);
  }
  ```

---
id: javascript-perf-array-slice-vs-splice
language: javascript
severity: info
message: "Performance: slice() doesn't modify original; splice() does - choose carefully"
tags:
  - performance
  - arrays
  - mutation
rule:
  kind: call_expression
  pattern: "array.slice|array.splice"
note: |
  slice() creates a copy, splice() modifies in-place.
  For large arrays, splice saves allocation cost but modifies the original.

  Recommended: Use splice() only when you need in-place modification.
  ```javascript
  // slice: Safe but allocates new array
  const subset = bigArray.slice(0, 100);

  // splice: Modifies original, faster, but changes array
  bigArray.splice(100); // removes 100+ elements
  ```

---
id: javascript-perf-template-literals-vs-concatenation
language: javascript
severity: info
message: "Performance: Template literals vs concatenation - similar speed but template literals more readable"
tags:
  - performance
  - strings
rule:
  kind: template_literal
  pattern: "`.*\\${.*}`"
note: |
  Modern engines optimize template literals and + concatenation similarly (~20M ops/sec).
  In loops, template literals can have slight edge as engine optimizes the construction.

  Recommended: Use template literals for readability; performance difference is negligible.
  ```javascript
  // EQUIVALENT PERFORMANCE, but template literals more readable
  const message = "Hello " + name + "!";
  const message = `Hello ${name}!`;

  // In tight loops, template literals may optimize better
  for (const item of items) {
    const str = `Item: ${item}`; // readable and optimizable
  }
  ```

---
id: javascript-perf-string-concat-in-loop
language: javascript
severity: warning
message: "Performance: String concatenation in loop creates many intermediate strings"
tags:
  - performance
  - strings
  - loops
rule:
  kind: for_statement
  pattern: |
    for (let i = 0; i < items.length; i++) {
      result = result + items[i];
    }
note: |
  Each concatenation creates a new string. Use array join() or StringBuilder pattern.

  Recommended: Use array.join() or accumulate in array and join at end.
  ```javascript
  // SLOW: Creates n new strings
  let result = '';
  for (const item of items) {
    result += item;
  }

  // FAST: Single join operation
  const result = items.join('');
  ```

---
id: javascript-perf-object-initialization-order
language: javascript
severity: info
message: "Performance: Initialize all object properties in constructor to maintain hidden class"
tags:
  - performance
  - objects
  - v8-optimization
rule:
  kind: function_declaration
  pattern: "constructor.*{.*}|this\\.[a-z]+ ="
note: |
  V8 creates hidden classes based on property shapes. Adding properties after construction
  forces hidden class changes and disables optimizations.

  Recommended: Initialize all properties in constructor in consistent order.
  ```javascript
  // BAD: Property added after construction - hidden class change
  class Point {
    constructor(x, y) {
      this.x = x;
      this.y = y;
    }
  }
  const p = new Point(0, 0);
  p.z = 0; // hidden class changes, optimization disabled

  // GOOD: All properties in constructor
  class Point {
    constructor(x, y, z = 0) {
      this.x = x;
      this.y = y;
      this.z = z;
    }
  }
  const p = new Point(0, 0, 0); // hidden class stable
  ```

---
id: javascript-perf-avoid-delete-operator
language: javascript
severity: warning
message: "Performance: Avoid delete operator - switches to Dictionary Mode, slows all property access"
tags:
  - performance
  - objects
  - v8-optimization
rule:
  kind: unary_expression
  pattern: "delete .*"
note: |
  The delete operator fundamentally changes object structure, forcing V8 to switch from
  hidden-class optimization to slow dictionary-mode property lookups.

  Recommended: Set properties to undefined or null instead of deleting.
  ```javascript
  // BAD: Triggers Dictionary Mode for entire object
  delete obj.property;

  // GOOD: Property removed logically, optimization intact
  obj.property = null;
  ```

---
id: javascript-perf-hidden-class-polymorphism
language: javascript
severity: info
message: "Performance: Avoid passing objects with different shapes to same function"
tags:
  - performance
  - objects
  - v8-optimization
rule:
  kind: call_expression
  pattern: "function.*\\(.*\\)"
note: |
  Functions optimized for monomorphic (single type) are 3x+ faster than polymorphic.
  If called with different object shapes, inline caching degrades to megamorphic.

  Recommended: Keep consistent object shapes for frequently called functions.
  ```javascript
  // OPTIMIZABLE: Monomorphic - always same shape
  function process(obj) {
    return obj.x + obj.y;
  }
  process({x: 1, y: 2});
  process({x: 3, y: 4});

  // DEGRADES: Polymorphic - different shapes
  process({x: 1, y: 2});
  process({a: 1, b: 2}); // different shape - loses optimization
  process({x: 1, y: 2, z: 3}); // yet another shape - megamorphic
  ```

---
id: javascript-perf-prototype-chain-depth
language: javascript
severity: info
message: "Performance: Deep prototype chains slow property access - keep chains shallow"
tags:
  - performance
  - objects
  - prototypes
rule:
  kind: function_declaration
  pattern: "Object.create.*Object.create"
note: |
  Each prototype level adds overhead to property lookup. Very deep chains
  (like DOM elements) require many checks.

  Recommended: Keep prototype chains 2-3 levels maximum.
  ```javascript
  // SLOW: Property lookup traverses many prototypes
  let obj = Object.create(Object.create(Object.create({value: 1})));

  // FAST: Direct property or shallow chain
  const obj = Object.create({value: 1});
  ```

---
id: javascript-perf-object-pooling-pattern
language: javascript
severity: warning
message: "Performance: High-allocation objects should use object pooling pattern"
tags:
  - performance
  - objects
  - memory
rule:
  kind: new_expression
  pattern: "new.*\\(\\)"
note: |
  Creating many objects causes garbage collection pauses. Object pooling
  pre-allocates and reuses objects, giving 2-5x performance boost.

  Recommended: For frequently created/destroyed objects (particles, vectors), use pooling.
  ```javascript
  // CREATES GARBAGE: Many allocations
  function createVector() {
    return new Vector(0, 0, 0);
  }
  for (let i = 0; i < 1000; i++) {
    const v = createVector(); // GC eventually collects
  }

  // POOLED: Reuses objects
  class VectorPool {
    constructor(size) {
      this.pool = [];
      for (let i = 0; i < size; i++) {
        this.pool.push(new Vector(0, 0, 0));
      }
    }
    get() {
      return this.pool.pop() || new Vector(0, 0, 0);
    }
    release(v) {
      v.set(0, 0, 0);
      this.pool.push(v);
    }
  }
  ```

---
id: javascript-perf-bundle-size-unused-imports
language: javascript
severity: warning
message: "Performance: Importing entire module for single function prevents tree-shaking"
tags:
  - performance
  - bundling
  - tree-shaking
rule:
  kind: import_statement
  pattern: "import .* from 'lodash'"
note: |
  Importing entire library for one utility disables tree-shaking.
  Use named imports or import the specific module.

  Recommended: Use named imports or import individual modules.
  ```javascript
  // BAD: Entire lodash included in bundle (~70KB)
  import _ from 'lodash';
  const sorted = _.sortBy(arr, 'name');

  // GOOD: Only sortBy included
  import sortBy from 'lodash/sortBy';
  const sorted = sortBy(arr, 'name');

  // ALSO GOOD: Named import if library supports it
  import { sortBy } from 'lodash-es';
  const sorted = sortBy(arr, 'name');
  ```

---
id: javascript-perf-bundle-code-splitting-lazy-routes
language: javascript
severity: info
message: "Performance: Use dynamic imports for non-critical routes to enable code splitting"
tags:
  - performance
  - bundling
  - code-splitting
rule:
  kind: call_expression
  pattern: "import\\(.*\\)"
note: |
  Code splitting via dynamic imports loads route/feature modules on demand.
  Reduces initial bundle size and improves first-page-load time.

  Recommended: Use dynamic imports for routes, large features, and optional components.
  ```javascript
  // MONOLITHIC: All routes in initial bundle
  import Dashboard from './routes/Dashboard';
  import Settings from './routes/Settings';
  import Admin from './routes/Admin'; // rarely used

  // OPTIMIZED: Load on demand
  const Dashboard = lazy(() => import('./routes/Dashboard'));
  const Settings = lazy(() => import('./routes/Settings'));
  const Admin = lazy(() => import('./routes/Admin')); // loaded only when needed
  ```

---
id: javascript-perf-tree-shaking-side-effects
language: javascript
severity: info
message: "Performance: Mark packages as side-effect-free in package.json for better tree-shaking"
tags:
  - performance
  - bundling
  - tree-shaking
rule:
  kind: object
  pattern: "package.json"
note: |
  If a package has no side effects, bundlers can safely remove unused exports.
  Add "sideEffects": false to package.json.

  Recommended: Declare side-effect-free modules in package.json.
  ```json
  {
    "name": "my-lib",
    "sideEffects": false,
    "main": "index.js"
  }
  ```

---
id: javascript-perf-minify-and-compress-bundle
language: javascript
severity: info
message: "Performance: Ensure bundle is minified and gzipped in production"
tags:
  - performance
  - bundling
  - compression
rule:
  kind: call_expression
  pattern: "webpack|parcel|rollup"
note: |
  Minification + gzip typically reduces bundle by 60-70%.
  Compressed bundles transfer much faster, especially on mobile networks.

  Recommended: Configure bundler to minify and enable gzip compression.
  ```javascript
  // Webpack example
  module.exports = {
    mode: 'production', // enables minification
    optimization: {
      minimize: true,
      minimizer: [new TerserPlugin()]
    }
  };
  ```

---
id: javascript-perf-v8-inline-cache-monomorphic
language: javascript
severity: info
message: "Performance: Keep functions monomorphic for inline cache optimization"
tags:
  - performance
  - v8-optimization
  - inline-cache
rule:
  kind: function_declaration
  pattern: "function.*{.*}|const .* = .*=>.*{.*}"
note: |
  V8 inlines property access for monomorphic (single type) call sites.
  Polymorphic (multiple types) sites lose this optimization.

  Recommended: Call functions with consistent argument types.
  ```javascript
  // OPTIMIZABLE: Monomorphic
  function add(a, b) { return a + b; }
  add(1, 2); add(3, 4); add(5, 6); // always numbers

  // DEGRADES: Polymorphic
  add(1, 2);      // numbers
  add("1", "2");  // strings - different hidden class
  add({}, {});    // objects - yet another shape, becomes megamorphic
  ```

---
id: javascript-perf-avoid-megamorphic-call-sites
language: javascript
severity: warning
message: "Performance: Megamorphic call sites (3+ types) disable optimizations - refactor for monomorphism"
tags:
  - performance
  - v8-optimization
rule:
  kind: member_expression
  pattern: "\\..*"
note: |
  Accessing properties on objects with 3+ different shapes causes V8 to give up
  optimization and fall back to slow hash-map lookups.

  Recommended: Use dispatch patterns or separate code paths for different types.
  ```javascript
  // MEGAMORPHIC: 4+ shapes, no optimization
  function render(obj) {
    return obj.value; // could be Point, Vector, User, or Config
  }

  // MONOMORPHIC: Separate functions per type
  function renderPoint(p) { return p.value; }
  function renderVector(v) { return v.value; }
  // OR: interface conformance
  function render(obj) {
    if (obj instanceof Point) return obj.value;
    if (obj instanceof Vector) return obj.value;
  }
  ```

---
id: javascript-perf-instanceof-vs-typeof
language: javascript
severity: info
message: "Performance: typeof faster than instanceof for primitives; instanceof for type checking"
tags:
  - performance
  - type-checking
  - v8-optimization
rule:
  kind: binary_expression
  pattern: "instanceof|typeof"
note: |
  typeof is very fast for primitives. instanceof requires prototype chain traversal.

  Recommended: Use typeof for primitives, instanceof for objects.
  ```javascript
  // FAST: typeof for primitives
  if (typeof x === 'number') { }
  if (typeof x === 'string') { }

  // SLOWER: instanceof always checks prototype chain
  if (x instanceof Array) { }

  // BETTER: Use Array.isArray, Object.is, etc.
  if (Array.isArray(x)) { }
  ```

---
id: javascript-perf-regex-performance
language: javascript
severity: warning
message: "Performance: Complex regexes can cause ReDoS (Regular Expression Denial of Service)"
tags:
  - performance
  - regex
  - security
rule:
  kind: call_expression
  pattern: "test\\(|exec\\(|match\\("
note: |
  Poorly designed regexes cause exponential backtracking, blocking event loop
  for seconds on moderate input.

  Recommended: Use safe-regex to validate regexes or use string methods.
  ```javascript
  // DANGEROUS: Catastrophic backtracking on input like "aaa...aaab"
  const dangerous = /(a+)+b/;

  // SAFER: Use validation library or simpler approach
  const input = "hello";
  if (input.startsWith("hel")) { }
  ```

---
id: javascript-perf-synchronous-io-blocks-event-loop
language: javascript
severity: warning
message: "Performance: Synchronous I/O (fs, crypto) blocks entire event loop - use async instead"
tags:
  - performance
  - event-loop
  - blocking
rule:
  kind: call_expression
  pattern: "fs\\.readFileSync|fs\\.writeFileSync"
note: |
  Synchronous I/O blocks the event loop, preventing all other operations.
  Always use async methods in Node.js.

  Recommended: Use async versions of I/O operations.
  ```javascript
  // BAD: Blocks event loop
  const data = fs.readFileSync('file.txt', 'utf8');
  process(data);

  // GOOD: Non-blocking
  fs.readFile('file.txt', 'utf8', (err, data) => {
    process(data);
  });

  // BEST: Async/await
  const data = await fs.promises.readFile('file.txt', 'utf8');
  process(data);
  ```

---
id: javascript-perf-cpu-intensive-offload-workers
language: javascript
severity: warning
message: "Performance: CPU-intensive tasks should be offloaded to Worker threads to avoid blocking"
tags:
  - performance
  - event-loop
  - workers
rule:
  kind: function_declaration
  pattern: "for.*{.*complex.*}"
note: |
  Long-running calculations block the event loop and stall all other operations.
  Use Worker threads or child processes for CPU-intensive work.

  Recommended: Offload CPU work to dedicated threads.
  ```javascript
  // BAD: Blocks event loop, UI freezes
  function fibonacci(n) {
    if (n <= 1) return n;
    return fibonacci(n - 1) + fibonacci(n - 2);
  }
  const result = fibonacci(40); // ~1 second, blocking

  // GOOD: Use Worker thread
  // main.js
  const { Worker } = require('worker_threads');
  const worker = new Worker('./fib-worker.js');
  worker.on('message', (result) => console.log(result));
  worker.postMessage(40);
  ```

---
id: javascript-perf-json-parse-large-objects
language: javascript
severity: warning
message: "Performance: Parsing very large JSON can block event loop - consider streaming or splitting"
tags:
  - performance
  - event-loop
  - json
rule:
  kind: call_expression
  pattern: "JSON.parse"
note: |
  JSON.parse is synchronous and blocks event loop for large payloads.
  For >50MB JSON, consider streaming or splitting into chunks.

  Recommended: Stream large JSON or split into smaller pieces.
  ```javascript
  // BAD: Large object parsing blocks event loop
  const largeData = JSON.parse(hugJsonString);

  // BETTER: Use streaming parser for very large objects
  const parser = JSONStream.parse('*');
  fs.createReadStream('huge.json')
    .pipe(parser)
    .on('data', (item) => processItem(item));
  ```

---
id: javascript-perf-avoid-eval-dynamic-code
language: javascript
severity: warning
message: "Performance: eval() and Function() constructors disable V8 optimizations"
tags:
  - performance
  - v8-optimization
  - security
rule:
  kind: call_expression
  pattern: "eval\\(|new Function"
note: |
  eval and Function constructors prevent V8 from applying optimizations to surrounding code.
  They also create security vulnerabilities (code injection).

  Recommended: Use JSON.parse, templates, or pre-compiled code instead.
  ```javascript
  // BAD: Disables optimizations, security risk
  const result = eval(userInput);

  // GOOD: For JSON
  const result = JSON.parse(userInput);

  // GOOD: For templates
  const template = compile(template);
  const result = template(data);
  ```

---
id: javascript-perf-weakmap-for-caches
language: javascript
severity: info
message: "Performance: Use WeakMap for caches to allow garbage collection"
tags:
  - performance
  - memory
  - caching
rule:
  kind: new_expression
  pattern: "new Map\\(\\)|new Object\\(\\)"
note: |
  Regular Map/Object caches hold strong references, preventing garbage collection.
  Use WeakMap when cache keys are objects that may be garbage collected.

  Recommended: Use WeakMap for object-keyed caches.
  ```javascript
  // MEMORY LEAK: Prevents GC of DOM elements
  const cache = new Map();
  function getCachedValue(element) {
    if (!cache.has(element)) {
      cache.set(element, expensive(element));
    }
    return cache.get(element);
  }

  // CORRECT: Allows GC
  const cache = new WeakMap();
  function getCachedValue(element) {
    if (!cache.has(element)) {
      cache.set(element, expensive(element));
    }
    return cache.get(element);
  }
  ```

---
id: javascript-perf-batch-dom-style-changes
language: javascript
severity: warning
message: "Performance: Batch style changes using class changes instead of individual properties"
tags:
  - performance
  - dom
  - css
rule:
  kind: member_expression
  pattern: "style\\.[a-z]+ ="
note: |
  Setting individual style properties triggers reflows. Use CSS class
  changes to batch multiple property updates.

  Recommended: Modify classes or use cssText for multiple style changes.
  ```javascript
  // SLOW: Multiple reflows
  element.style.width = '100px';
  element.style.height = '100px';
  element.style.borderRadius = '50%';
  element.style.backgroundColor = 'red';

  // FAST: Single reflow
  element.classList.add('circle');

  // ALSO FAST: cssText
  element.style.cssText = 'width:100px;height:100px;border-radius:50%;background-color:red;';
  ```

---
id: javascript-perf-will-change-css-hint
language: javascript
severity: info
message: "Performance: Use will-change CSS for animations to hint browser optimization"
tags:
  - performance
  - dom
  - css
  - animation
rule:
  kind: member_expression
  pattern: "style\\."
note: |
  will-change hints to browser to prepare for animated properties,
  enabling hardware acceleration and optimizations.

  Recommended: Apply will-change to animated elements, remove after animation.
  ```javascript
  // OPTIMIZABLE: Hints hardware acceleration
  element.style.willChange = 'transform';
  animate(element);
  // After animation:
  element.style.willChange = 'auto';
  ```

---
id: javascript-perf-image-lazy-loading
language: javascript
severity: info
message: "Performance: Use native lazy loading or Intersection Observer for images"
tags:
  - performance
  - dom
  - images
rule:
  kind: html_element
  pattern: "<img"
note: |
  Load images on-demand using native lazy loading or Intersection Observer.
  Reduces initial page load and bandwidth.

  Recommended: Use native loading="lazy" or Intersection Observer.
  ```javascript
  // NATIVE: Simplest, supported in modern browsers
  <img src="image.jpg" loading="lazy" alt="..." />

  // OBSERVER: For older browsers or custom behavior
  const observer = new IntersectionObserver((entries) => {
    entries.forEach(entry => {
      if (entry.isIntersecting) {
        entry.target.src = entry.target.dataset.src;
        observer.unobserve(entry.target);
      }
    });
  });
  document.querySelectorAll('img[data-src]').forEach(img => observer.observe(img));
  ```

---
id: javascript-perf-avoid-layout-shift-cumulative
language: javascript
severity: info
message: "Performance: Reserve space for dynamic content to avoid Cumulative Layout Shift (CLS)"
tags:
  - performance
  - core-web-vitals
  - dom
rule:
  kind: member_expression
  pattern: "innerHTML|textContent"
note: |
  Dynamically loaded content causes layout shift when it appears, degrading CLS score.
  Reserve space with skeleton screens or size containers upfront.

  Recommended: Use aspect ratio boxes or skeleton screens for media.
  ```javascript
  // CAUSES CLS: Layout shifts when image loads
  <div id="container"></div>
  // later...
  container.innerHTML = '<img src="image.jpg" />';

  // PREVENTS CLS: Reserved space
  <div style="aspect-ratio: 16/9;">
    <img src="image.jpg" />
  </div>
  ```

---
id: javascript-perf-cache-dom-references
language: javascript
severity: info
message: "Performance: Cache frequently accessed DOM references to avoid repeated queries"
tags:
  - performance
  - dom
  - caching
rule:
  kind: member_expression
  pattern: "document.querySelector|document.getElementById"
note: |
  DOM queries are expensive. Cache references to frequently accessed elements.

  Recommended: Query once, reuse reference multiple times.
  ```javascript
  // INEFFICIENT: Multiple queries
  document.querySelector('#button').addEventListener('click', handler);
  document.querySelector('#button').style.color = 'red';
  document.querySelector('#button').disabled = true;

  // EFFICIENT: Single query, cached reference
  const button = document.querySelector('#button');
  button.addEventListener('click', handler);
  button.style.color = 'red';
  button.disabled = true;
  ```

---
id: javascript-perf-passive-event-listeners
language: javascript
severity: info
message: "Performance: Use passive event listeners for scroll/touch to avoid blocking"
tags:
  - performance
  - event-listeners
  - dom
rule:
  kind: call_expression
  pattern: "addEventListener"
note: |
  By default, event listeners can call preventDefault(), blocking scroll.
  Use {passive: true} for listeners that don't call preventDefault().

  Recommended: Use passive for scroll, touch, and wheel events.
  ```javascript
  // POTENTIALLY BLOCKING: Can delay scrolling
  window.addEventListener('scroll', handler);
  window.addEventListener('touchmove', handler);

  // NON-BLOCKING: Passive hint
  window.addEventListener('scroll', handler, { passive: true });
  window.addEventListener('touchmove', handler, { passive: true });
  ```

---
id: javascript-perf-throttle-debounce-events
language: javascript
severity: warning
message: "Performance: Throttle/debounce frequent events (scroll, resize, input) to reduce handler calls"
tags:
  - performance
  - event-handlers
rule:
  kind: call_expression
  pattern: "addEventListener.*scroll|addEventListener.*resize|addEventListener.*input"
note: |
  Frequent events like scroll, resize, input fire many times per second.
  Throttle/debounce to reduce handler execution.

  Recommended: Use throttle for scroll, debounce for input/resize.
  ```javascript
  // CALLED 100+ TIMES/SEC: Performance killer
  window.addEventListener('scroll', () => {
    console.log('Scrolling');
  });

  // THROTTLED: Max 10 times/sec
  function throttle(func, limit) {
    let inThrottle;
    return function() {
      if (!inThrottle) {
        func.apply(this, arguments);
        inThrottle = true;
        setTimeout(() => inThrottle = false, limit);
      }
    };
  }
  window.addEventListener('scroll', throttle(() => {
    console.log('Scrolling');
  }, 100));
  ```

---
id: javascript-perf-intersection-observer-visibility
language: javascript
severity: info
message: "Performance: Use Intersection Observer for element visibility checks instead of scroll events"
tags:
  - performance
  - dom
  - event-handlers
rule:
  kind: call_expression
  pattern: "addEventListener.*scroll"
note: |
  Checking element visibility on scroll events causes expensive layout recalculations.
  Intersection Observer is optimized by the browser.

  Recommended: Use Intersection Observer for visibility detection.
  ```javascript
  // EXPENSIVE: Layout recalculation on every scroll
  window.addEventListener('scroll', () => {
    const rect = element.getBoundingClientRect();
    if (rect.top < window.innerHeight) {
      loadContent();
    }
  });

  // OPTIMIZED: Browser-native intersection detection
  const observer = new IntersectionObserver((entries) => {
    entries.forEach(entry => {
      if (entry.isIntersecting) {
        loadContent();
        observer.unobserve(entry.target);
      }
    });
  });
  observer.observe(element);
  ```

---
id: javascript-perf-memoize-expensive-calculations
language: javascript
severity: warning
message: "Performance: Memoize expensive pure functions to cache results"
tags:
  - performance
  - caching
  - functions
rule:
  kind: function_declaration
  pattern: "function.*{.*for.*{.*}.*}"
note: |
  Pure functions with expensive calculations should cache results.
  Memoization trades memory for speed.

  Recommended: Use memoization for frequently called pure functions.
  ```javascript
  // RECALCULATES: Every call does expensive work
  function fibonacci(n) {
    if (n <= 1) return n;
    return fibonacci(n - 1) + fibonacci(n - 2);
  }

  // MEMOIZED: Caches results
  const memoize = (fn) => {
    const cache = new Map();
    return (...args) => {
      const key = JSON.stringify(args);
      if (cache.has(key)) return cache.get(key);
      const result = fn(...args);
      cache.set(key, result);
      return result;
    };
  };
  const fib = memoize(fibonacci);
  ```

---
id: javascript-perf-reduce-closure-overhead
language: javascript
severity: info
message: "Performance: Closures capture entire scope - extract only needed variables"
tags:
  - performance
  - closures
  - memory
rule:
  kind: function_declaration
  pattern: "function.*{.*return.*function"
note: |
  Closures capture their entire parent scope, even variables not used.
  Extract only needed values to reduce captured scope.

  Recommended: Minimize closure scope by extracting needed variables.
  ```javascript
  // CAPTURES ENTIRE SCOPE: Entire userObj and config in closure
  function createHandler(userObj, config) {
    return () => {
      console.log(userObj.name); // only uses name
    };
  }

  // OPTIMIZED: Closure only captures needed value
  function createHandler(userObj, config) {
    const name = userObj.name;
    return () => {
      console.log(name);
    };
  }
  ```

---
id: javascript-perf-use-float32array-numeric
language: javascript
severity: info
message: "Performance: Use Float32Array for numeric operations to reduce memory and increase speed"
tags:
  - performance
  - arrays
  - numeric
rule:
  kind: array_expression
  pattern: "\\[.*[0-9].*\\]"
note: |
  Float32Array is much more compact than regular arrays of numbers.
  Useful for graphics, simulations, and scientific computing.

  Recommended: Use typed arrays for numeric-heavy code.
  ```javascript
  // MEMORY HEAVY: Regular array, each number is a JS object
  const vertices = [1.0, 2.0, 3.0, 4.0, 5.0];

  // OPTIMIZED: Compact binary representation
  const vertices = new Float32Array([1.0, 2.0, 3.0, 4.0, 5.0]);
  ```

---
id: javascript-perf-requestidlecallback-non-urgent
language: javascript
severity: info
message: "Performance: Use requestIdleCallback for non-urgent work to not block main thread"
tags:
  - performance
  - event-loop
rule:
  kind: call_expression
  pattern: "setTimeout.*0"
note: |
  requestIdleCallback runs work only when browser is idle, after high-priority tasks.
  Better than setTimeout(fn, 0) for background work.

  Recommended: Use requestIdleCallback for analytics, logging, cleanup.
  ```javascript
  // BLOCKS: setTimeout still takes time slice
  setTimeout(() => {
    analytics.log(event);
  }, 0);

  // DEFERRED: Runs only during idle time
  if ('requestIdleCallback' in window) {
    requestIdleCallback(() => {
      analytics.log(event);
    });
  } else {
    setTimeout(() => {
      analytics.log(event);
    }, 0);
  }
  ```

---
id: javascript-perf-defer-non-critical-scripts
language: javascript
severity: info
message: "Performance: Use defer/async attributes for non-critical scripts to avoid blocking"
tags:
  - performance
  - bundling
  - html
rule:
  kind: html_element
  pattern: "<script"
note: |
  Scripts block HTML parsing by default. Use defer (execute after HTML parsed)
  or async (download in parallel) for non-critical scripts.

  Recommended: Use defer for scripts that don't need to run before page interactive.
  ```html
  <!-- BLOCKS: Parsing halts until downloaded and executed -->
  <script src="script.js"></script>

  <!-- DEFERRED: Executes after HTML parsed -->
  <script src="script.js" defer></script>

  <!-- ASYNC: Downloads parallel to HTML parsing -->
  <script src="analytics.js" async></script>
  ```

---
id: javascript-perf-server-side-rendering-ssr
language: javascript
severity: info
message: "Performance: Consider Server-Side Rendering (SSR) for faster initial page load"
tags:
  - performance
  - bundling
  - architecture
rule:
  kind: call_expression
  pattern: "ReactDOM.render|mount"
note: |
  Client-side rendering requires downloading, parsing, and executing JavaScript
  before showing content. SSR renders HTML on server, serving ready content.

  Recommended: Use SSR/Static Generation for content-heavy sites.
  ```javascript
  // CSR: User waits for JS bundle download, parse, React initialization
  // index.html: <div id="root"></div>
  // app.js: ReactDOM.render(<App />, root);

  // SSR: Server sends ready HTML
  // Node.js: ReactDOMServer.renderToString(<App />)
  // HTML sent to client immediately with content
  ```

---
id: javascript-perf-core-web-vitals-lcp
language: javascript
severity: info
message: "Performance: Optimize Largest Contentful Paint (LCP) - defer above-fold images"
tags:
  - performance
  - core-web-vitals
rule:
  kind: html_element
  pattern: "<img"
note: |
  LCP measures when largest visible element loads. Prioritize LCP images
  over non-critical images.

  Recommended: Preload critical images, lazy-load non-critical.
  ```html
  <!-- CRITICAL: Preload LCP image -->
  <link rel="preload" as="image" href="hero.jpg" />

  <!-- NON-CRITICAL: Lazy load -->
  <img src="below-fold.jpg" loading="lazy" />
  ```

---
id: javascript-perf-core-web-vitals-fid
language: javascript
severity: info
message: "Performance: Optimize First Input Delay (FID) - break up long tasks"
tags:
  - performance
  - core-web-vitals
  - event-loop
rule:
  kind: function_declaration
  pattern: "function.*{.*for.*{.*}.*}"
note: |
  FID measures response time to user input. Long tasks delay response.
  Break processing into chunks with yields.

  Recommended: Keep tasks under 50ms for responsive feel.
  ```javascript
  // BAD: Long task delays input response
  function processLargeArray(items) {
    for (let i = 0; i < items.length; i++) {
      expensiveOperation(items[i]);
    }
  }

  // GOOD: Tasks yield to allow input response
  function processLargeArray(items) {
    let index = 0;
    function batch() {
      const deadline = performance.now() + 50;
      while (index < items.length && performance.now() < deadline) {
        expensiveOperation(items[index++]);
      }
      if (index < items.length) {
        setTimeout(batch, 0);
      }
    }
    batch();
  }
  ```

---
id: javascript-perf-resource-hints-dns-prefetch
language: javascript
severity: info
message: "Performance: Use dns-prefetch and preconnect for third-party origins"
tags:
  - performance
  - bundling
  - network
rule:
  kind: html_element
  pattern: "<link"
note: |
  dns-prefetch and preconnect hints tell browser to prepare connections early.
  Saves time for third-party resources.

  Recommended: Use for critical third-party domains.
  ```html
  <!-- DNS lookup in advance -->
  <link rel="dns-prefetch" href="https://cdn.example.com" />

  <!-- Full connection: DNS + TCP + TLS -->
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  ```

---
id: javascript-perf-compression-brotli-gzip
language: javascript
severity: info
message: "Performance: Serve bundles with Brotli compression (better than gzip)"
tags:
  - performance
  - bundling
  - compression
rule:
  kind: http_header
  pattern: "Content-Encoding"
note: |
  Brotli compresses 10-20% better than gzip. Use Brotli for modern browsers,
  fall back to gzip for older browsers.

  Recommended: Configure server to serve Brotli when supported.
  ```javascript
  // Express.js example
  const compression = require('compression');
  app.use(compression({ level: 11 })); // Brotli level 11 (slowest but smallest)
  ```

---
id: javascript-perf-http2-push
language: javascript
severity: info
message: "Performance: Use HTTP/2 server push for critical resources to avoid round trips"
tags:
  - performance
  - network
rule:
  kind: http_header
  pattern: "Link"
note: |
  HTTP/2 server push preemptively sends critical resources before client requests.
  Saves round-trip time for stylesheets, fonts, and critical scripts.

  Recommended: Configure HTTP/2 push for critical CSS and fonts.
  ```javascript
  // Node.js with HTTP/2
  const http2 = require('http2');
  const fs = require('fs');

  const server = http2.createSecureServer({
    key: fs.readFileSync('./key.pem'),
    cert: fs.readFileSync('./cert.pem')
  });

  server.on('stream', (stream) => {
    stream.pushStream({ ':path': '/style.css' }, (err, pushStream) => {
      pushStream.end(fs.readFileSync('./style.css'));
    });
    stream.end('<html>...</html>');
  });
  ```

---
id: javascript-perf-cdn-edge-caching
language: javascript
severity: info
message: "Performance: Use CDN with edge caching for static assets"
tags:
  - performance
  - network
  - caching
rule:
  kind: http_header
  pattern: "Cache-Control"
note: |
  CDNs cache assets near users, reducing latency and server load.
  Set appropriate Cache-Control headers.

  Recommended: Use CDN with long cache duration for versioned assets.
  ```javascript
  // Express.js: Cache static assets with versioning
  app.use(express.static('public', {
    maxAge: '1y', // Long cache for versioned files
    etag: false
  }));

  app.use(express.static('public', {
    maxAge: '5m', // Short cache for non-versioned files
  }));
  ```

---
id: javascript-perf-database-connection-pooling
language: javascript
severity: warning
message: "Performance: Use database connection pooling to reuse connections"
tags:
  - performance
  - database
  - node.js
rule:
  kind: new_expression
  pattern: "new.*Connection"
note: |
  Creating new database connections is expensive. Use connection pools
  to reuse existing connections.

  Recommended: Configure connection pool with appropriate size.
  ```javascript
  // BAD: New connection per query
  const mysql = require('mysql');
  function query(sql) {
    const conn = mysql.createConnection({...});
    return new Promise((resolve) => {
      conn.query(sql, (err, res) => {
        conn.end();
        resolve(res);
      });
    });
  }

  // GOOD: Connection pool
  const pool = mysql.createPool({
    connectionLimit: 10,
    host: 'localhost',
    user: 'user',
    password: 'password',
    database: 'db'
  });

  function query(sql) {
    return new Promise((resolve) => {
      pool.query(sql, (err, res) => {
        resolve(res);
      });
    });
  }
  ```

---
id: javascript-perf-enable-http-caching-headers
language: javascript
severity: info
message: "Performance: Set proper HTTP caching headers (ETag, Last-Modified, Cache-Control)"
tags:
  - performance
  - network
  - http
rule:
  kind: http_header
  pattern: "ETag|Last-Modified|Cache-Control"
note: |
  HTTP caching headers tell browsers/CDNs when to use cached versions.
  Reduces bandwidth and server load significantly.

  Recommended: Set ETag for validation and Cache-Control for duration.
  ```javascript
  // Express.js
  app.get('/api/data', (req, res) => {
    const data = { /* ... */ };
    const etag = crypto.createHash('md5').update(JSON.stringify(data)).digest('hex');

    res.set('ETag', etag);
    res.set('Cache-Control', 'public, max-age=3600');
    res.json(data);
  });
  ```

# End of JavaScript Performance Rules
# Java Performance Rules - Comprehensive YAML Rules File
# 75 performance rules covering 10 major categories
# Based on research: https://www.baeldung.com/, https://medium.com/, https://www.infoq.com/
# Last Updated: 2025-01-03

---
id: java-perf-string-concat-loop
language: java
severity: warning
message: "Performance: Avoid string concatenation with + operator inside loops"
tags:
  - performance
  - string-handling
rule:
  kind: binary_expression
  pattern: |
    $var = $var + $expr
  contains: ["ForStatement", "WhileStatement", "DoWhileStatement", "ForEachStatement"]
note: |
  String concatenation in loops creates temporary String objects with O(n²) complexity.
  Each iteration creates a new String object copying the entire previous result.

  Anti-pattern:
    String result = "";
    for (int i = 0; i < 1000; i++) {
      result = result + i;  // Creates 1000+ temporary objects
    }

  Fix: Use StringBuilder with pre-sized capacity for known sizes:
    StringBuilder sb = new StringBuilder(estimatedSize);
    for (int i = 0; i < 1000; i++) {
      sb.append(i);  // O(n) complexity, single object
    }

  Reference: Modern JDK (11+) optimizes simple concatenation, but loops still require StringBuilder.
  Impact: 10-100x slowdown in tight loops with many concatenations.

---
id: java-perf-stringbuilder-no-capacity
language: java
severity: info
message: "Performance: Pre-size StringBuilder with expected capacity to avoid reallocations"
tags:
  - performance
  - string-handling
rule:
  kind: object_creation
  pattern: "new StringBuilder()"
  not_contains: ["new StringBuilder\\(\\d+\\)"]
note: |
  StringBuilder without initial capacity defaults to 16 characters.
  When capacity is exceeded, the internal array is doubled and reallocated.
  This causes repeated memory allocations and copying.

  Anti-pattern:
    StringBuilder sb = new StringBuilder();  // Starts with 16 chars
    // Appending 10,000 characters causes ~10 reallocations

  Fix: Pre-size based on expected content:
    StringBuilder sb = new StringBuilder(10_000);  // Allocate once

  Estimation strategies:
    - If building CSV: (colCount * avgColWidth) + (rowCount * lineOverhead)
    - If building JSON: estimatedJsonSize
    - Default safe estimate: expectedStringLength * 1.2

  Impact: Eliminates O(log n) reallocations, reducing GC pressure.
  Benchmark: 10-15% faster for large string building operations.

---
id: java-perf-string-intern-abuse
language: java
severity: warning
message: "Performance: Avoid excessive String.intern() calls - it uses memory for lookup"
tags:
  - performance
  - string-handling
rule:
  kind: method_call
  pattern: "\\$str\\.intern\\(\\)"
  frequency: "multiple_times_loop"
note: |
  String.intern() adds the string to the string pool, which is stored in PermGen/Metaspace.
  Excessive interning consumes memory, slows down string lookups, and can cause OutOfMemory.

  Anti-pattern:
    for (String line : lines) {
      String interned = line.intern();  // Millions of interned strings
    }

  Fix: Use intern() only for strings you expect to match frequently:
    // Good: Common constants
    private static final String STATUS_OK = "OK".intern();

    // Bad: All input strings
    for (String line : lines) {
      String s = line.intern();  // Don't do this
    }

  Real use case:
    - Comparing large numbers of strings with == (rare, use .equals() instead)
    - Reducing memory when strings have many duplicates (profile first)

  Impact: Unbounded memory growth; PermGen/Metaspace exhaustion after 100K+ interned strings.
  Recommended: Use if application has <1000 unique interned strings total.

---
id: java-perf-string-concatenation-simple
language: java
severity: info
message: "Performance: Simple string concatenation is optimized by compiler - prefer readability"
tags:
  - performance
  - string-handling
rule:
  kind: binary_expression
  pattern: "\"string\" + var + \"literal\""
  not_contains: ["ForStatement", "WhileStatement"]
note: |
  Modern Java (JDK 11+) compiler automatically optimizes simple concatenations using StringBuilder.
  Single concatenation operations outside loops are safe and readable.

  Good patterns (auto-optimized):
    String msg = "Error: " + error + " at line " + line;

  The compiler translates to:
    String msg = new StringBuilder()
      .append("Error: ")
      .append(error)
      .append(" at line ")
      .append(line)
      .toString();

  When NOT optimized:
    - Inside loops: Manual StringBuilder required
    - In methods called millions of times
    - Dynamic string building with conditions

  Rule of thumb: Prefer readability for single/double concatenations outside loops.
  Use StringBuilder explicitly only in loops or high-frequency methods.

  Impact: Zero penalty for simple concatenation in modern JVMs.

---
id: java-perf-arraylist-initial-capacity
language: java
severity: info
message: "Performance: Initialize ArrayList with expected capacity to avoid resizing"
tags:
  - performance
  - collection-selection
rule:
  kind: object_creation
  pattern: "new ArrayList<>()"
  not_contains: ["new ArrayList<>\\(\\d+\\)", "new ArrayList<>\\([a-zA-Z_].*\\)"]
note: |
  ArrayList defaults to capacity 10, which is almost always wrong.
  When size exceeds capacity, ArrayList copies all elements to a new array (1.5x larger).

  Anti-pattern:
    ArrayList<String> list = new ArrayList<>();  // Capacity 10
    for (int i = 0; i < 1_000_000; i++) {
      list.add(...);  // Causes ~20 reallocations
    }

  Fix: Pre-size based on expected count:
    ArrayList<String> list = new ArrayList<>(1_000_000);  // Single allocation
    for (int i = 0; i < 1_000_000; i++) {
      list.add(...);
    }

  Sizing strategies:
    - If known: ArrayList<>(exactSize)
    - If uncertain: ArrayList<>(estimatedSize + 10%)
    - Conservative estimate: ArrayList<>(Math.max(16, expectedSize))

  Performance impact:
    - 100 items: ~2% speedup
    - 10K items: ~10% speedup
    - 1M items: ~15-20% speedup

  Memory impact: Wasted space if over-estimated; reallocation cost if under-estimated.

---
id: java-perf-linkedlist-random-access
language: java
severity: warning
message: "Performance: LinkedList has O(n) random access - use ArrayList for index-based access"
tags:
  - performance
  - collection-selection
rule:
  kind: method_call
  pattern: "\\$linkedList\\.get\\(\\$index\\)"
  in_loop: true
note: |
  LinkedList traverses from head/tail to reach index position - O(n) per access.
  For random access patterns, this becomes O(n²) in a loop.

  Anti-pattern:
    LinkedList<String> list = new LinkedList<>(items);
    for (int i = 0; i < list.size(); i++) {
      String item = list.get(i);  // O(n) - traverses from head!
    }

  Fix: Use ArrayList for index-based access:
    ArrayList<String> list = new ArrayList<>(items);
    for (int i = 0; i < list.size(); i++) {
      String item = list.get(i);  // O(1) - direct array access
    }

  LinkedList is optimal for:
    - Frequent insertions/deletions at beginning/end: add/remove O(1)
    - Iterator-based loops: O(1) per element
    - Queue/Deque implementations: FIFO/LIFO patterns

  LinkedList is poor for:
    - Random access by index
    - Searching (O(n))
    - Sorting (O(n log n) comparisons + O(n) access per operation)

  Impact: 100x slowdown for LinkedList.get(i) in tight loops on large lists.

---
id: java-perf-linkedlist-iteration
language: java
severity: info
message: "Performance: Use iterator for LinkedList, not index-based loops"
tags:
  - performance
  - collection-selection
rule:
  kind: for_statement
  pattern: "for\\s*\\(\\s*int\\s+i\\s*=\\s*0\\s*;\\s*i\\s*<\\s*\\$list\\.size.*\\)\\s*\\{.*\\$list\\.get\\(i\\)"
note: |
  For-each iterator on LinkedList uses Iterator.next() - O(1) per element.
  Index-based loop requires LinkedList.get(i) - O(n) per access.

  Anti-pattern (O(n²)):
    for (int i = 0; i < linkedList.size(); i++) {
      process(linkedList.get(i));
    }

  Fix - Method 1: For-each iterator (O(n)):
    for (String item : linkedList) {
      process(item);  // Uses optimized iterator internally
    }

  Fix - Method 2: Explicit iterator (O(n)):
    for (Iterator<String> it = linkedList.iterator(); it.hasNext(); ) {
      process(it.next());
    }

  How it works:
    LinkedList.iterator() returns an iterator with cursor tracking position.
    Iterator.next() moves cursor forward in O(1), not O(n).
    For-each automatically uses iterator for Iterable implementations.

  Impact: 50-100x faster for LinkedList iteration vs index-based access.

---
id: java-perf-hashmap-load-factor
language: java
severity: info
message: "Performance: Tune HashMap load factor and capacity based on access patterns"
tags:
  - performance
  - collection-selection
rule:
  kind: object_creation
  pattern: "new HashMap<>()"
  note_threshold: "high_frequency_access"
note: |
  HashMap defaults: initial capacity 16, load factor 0.75.
  Load factor 0.75 means HashMap doubles when 75% full (12 entries).
  Rehashing copies all entries to new buckets - expensive operation.

  Trade-offs:
    - Load factor 0.75 (default): Balanced space/time, ~5.3 bits/entry overhead
    - Load factor 0.5: More space, fewer rehashes, better cache locality
    - Load factor 1.0: Less space, more rehashes and collisions

  Anti-pattern (excessive rehashing):
    HashMap<String, Data> map = new HashMap<>();  // Default capacity 16
    for (int i = 0; i < 100_000; i++) {
      map.put(key, value);  // Causes ~13 rehashes
    }

  Fix - Known size:
    HashMap<String, Data> map = new HashMap<>(150_000, 0.75f);  // One allocation

  Fix - Frequent lookups, less inserts:
    HashMap<String, Data> map = new HashMap<>(100_000, 0.5f);  // Faster lookups

  Sizing formula: capacity >= size / loadFactor
    - For 100K entries at 0.75: use HashMap<>(133_334)
    - For 100K entries at 0.5: use HashMap<>(200_000)

  Impact:
    - Undersized initial capacity: 10-20% slowdown due to rehashing
    - Right-sized capacity: No rehashing, 1 allocation
    - Over-provisioned: Wasted memory (typically acceptable)

---
id: java-perf-concurrenthashmap-vs-synchronized
language: java
severity: warning
message: "Performance: Prefer ConcurrentHashMap over Collections.synchronizedMap() for concurrent access"
tags:
  - performance
  - synchronization
rule:
  kind: method_call
  pattern: "Collections\\.synchronizedMap\\(.*\\)"
note: |
  Collections.synchronizedMap() locks the entire map for each operation.
  Multiple threads become serialized, reducing to single-threaded performance.
  ConcurrentHashMap uses segment/bucket-level locking (Java 8+ uses node locks).

  Anti-pattern (poor scalability):
    Map<String, Data> map = Collections.synchronizedMap(new HashMap<>());
    // All operations serialized by single lock
    // 4 threads = 25% throughput
    // 16 threads = 6% throughput (lock contention dominates)

  Fix: Use ConcurrentHashMap:
    Map<String, Data> map = new ConcurrentHashMap<>();
    // Reads are non-blocking
    // Writes lock only the affected bucket
    // 16 threads ≈ 12-14x throughput of synchronized map

  Design:
    ConcurrentHashMap (Java 8+):
      - Reads: Non-blocking lock-free
      - Writes: Synchronized on bucket head node
      - Default concurrency level: dynamic (segment buckets)
      - Collision resolution: hash table -> tree for collision chains > 8 nodes

  When to use Collections.synchronizedMap():
    - Legacy code that needs Map interface
    - Rare concurrent access (mostly single-threaded)
    - Explicit need for atomicity across multiple operations

  When to use ConcurrentHashMap:
    - Multi-threaded read-heavy workloads
    - Performance-critical applications
    - Most new code

  Impact: 5-15x throughput improvement with 4-16 concurrent threads.
  Benchmark: ConcurrentHashMap scales nearly linearly; synchronizedMap hits plateau at 2-4 threads.

---
id: java-perf-concurrenthashmap-locking
language: java
severity: info
message: "Performance: Avoid explicit synchronization on ConcurrentHashMap - it already handles concurrency"
tags:
  - performance
  - synchronization
rule:
  kind: method_call
  pattern: "synchronized\\s*\\(.*ConcurrentHashMap.*\\)"
note: |
  Wrapping ConcurrentHashMap operations in synchronized blocks negates performance benefits.
  ConcurrentHashMap already provides thread-safe individual operations.
  Adding external synchronization serializes access, defeating the purpose.

  Anti-pattern:
    ConcurrentHashMap<String, Integer> map = new ConcurrentHashMap<>();
    synchronized (map) {  // WRONG - removes all concurrency benefits
      map.put(key, value);
    }

  Correct usage:
    ConcurrentHashMap<String, Integer> map = new ConcurrentHashMap<>();
    map.put(key, value);  // Already thread-safe

  When external synchronization IS needed:
    Compound operations requiring atomicity:

    Bad - Not atomic:
      if (!map.containsKey(key)) {  // Window between check and put
        map.put(key, value);
      }

    Good - Atomic (ConcurrentHashMap method):
      map.putIfAbsent(key, value);

    Or explicit lock for true compound operations:
      synchronized (map) {
        if (!map.containsKey(key)) {
          map.put(key, computeExpensiveValue());
        }
      }

  Impact: Explicit synchronization can reduce throughput by 50% or more.

---
id: java-perf-lock-contention-spinlock
language: java
severity: warning
message: "Performance: Avoid busy-wait loops - use wait/notify or higher-level synchronization"
tags:
  - performance
  - synchronization
rule:
  kind: while_statement
  pattern: "while\\s*\\(.*flag.*\\)\\s*\\{.*\\}"
note: |
  Busy-wait (spin lock) loops consume 100% CPU and prevent other threads from running.
  Each iteration polls shared state without yielding, causing cache thrashing.

  Anti-pattern:
    volatile boolean done = false;
    while (!done) {  // Spins, consuming CPU
      // Check condition repeatedly
    }

  Fix 1 - Object.wait/notify (for simple signaling):
    synchronized (obj) {
      while (!condition) {
        obj.wait();  // Releases lock, blocks until notify
      }
    }
    // Elsewhere:
    synchronized (obj) {
      condition = true;
      obj.notifyAll();  // Wake waiting threads
    }

  Fix 2 - CountDownLatch (for one-way signaling):
    CountDownLatch latch = new CountDownLatch(1);
    thread1.start(() -> {
      // Do work
      latch.countDown();  // Signal completion
    });
    latch.await();  // Blocks until countdown reaches zero

  Fix 3 - CyclicBarrier (for multi-way synchronization):
    CyclicBarrier barrier = new CyclicBarrier(3);
    // Each thread calls barrier.await() - blocks until all 3 threads reach it

  Fix 4 - Condition (for complex wait patterns):
    Condition condition = lock.newCondition();
    condition.await();  // Park thread efficiently
    condition.signalAll();  // Wake waiting threads

  Impact of busy-wait:
    - CPU waste: 50-100% of core devoted to spinning
    - Cache contention: Repeated polling invalidates caches
    - OS scheduling overhead: Not marked as blocked, occupies scheduler time

  Fix impact: Virtually zero CPU usage while waiting; immediate wakeup on signal.

---
id: java-perf-stream-parallel-small-data
language: java
severity: warning
message: "Performance: Parallel streams are slower on small datasets due to fork/join overhead"
tags:
  - performance
  - stream-api
rule:
  kind: method_call
  pattern: "parallelStream\\(\\)"
  element_count: "< 10000"
note: |
  parallelStream() creates ForkJoinPool tasks with fixed overhead:
    - Task creation: ~microseconds
    - Thread startup: ~milliseconds
    - Context switching: ~microseconds

  On datasets < 10K elements, overhead dominates processing time.

  Anti-pattern:
    list.parallelStream()  // list has 100 elements
      .map(this::expensiveOp)
      .collect(Collectors.toList());
    // Fork/join overhead > benefit; slower than sequential

  Overhead breakdown (rough estimates):
    - Sequential on 100 items: 1-2 ms
    - Parallel on 100 items: 5-10 ms (fork/join setup, task creation, merging)
    - Break-even point: 10,000-100,000 elements (depends on operation cost)

  Fix: Use sequential streams for small data:
    list.stream()  // Auto-uses sequential
      .map(this::expensiveOp)
      .collect(Collectors.toList());

  When parallel streams help:
    - Element count: > 10,000 (typically)
    - Operation cost: > 1 microsecond per element (CPU-bound work)
    - Data structure: ArrayList, array (cheap split); NOT LinkedList
    - Collector: CONCURRENT and UNORDERED for true parallel reduction

  Micro-benchmarking parallelStream():
    Use JMH (Java Microbenchmark Harness) to measure.
    Sequential often wins on modern CPUs due to cache locality.

  Impact: Parallel on small data can be 5-10x SLOWER than sequential.

---
id: java-perf-parallel-stream-linkedlist
language: java
severity: warning
message: "Performance: LinkedList is terrible for parallel streams - must traverse to find split point"
tags:
  - performance
  - stream-api
rule:
  kind: method_call
  pattern: "new LinkedList.*parallelStream"
note: |
  Parallel streams use Spliterator to divide data for worker threads.
  LinkedList.spliterator() must traverse O(n) nodes to find midpoint for splits.
  This negates all parallelism benefits: O(n) split + O(n) processing = O(n) + overhead.

  Anti-pattern:
    LinkedList<Integer> list = new LinkedList<>(millionElements);
    list.parallelStream()
      .map(x -> x * 2)
      .collect(Collectors.toList());
    // Split cost: O(n) traversal
    // Better off single-threaded

  Fix 1 - Convert to ArrayList first:
    ArrayList<Integer> list = new ArrayList<>(linkedList);
    list.parallelStream()...  // Splits in O(1)

  Fix 2 - Use sequential stream:
    linkedList.stream()...  // Iterates in O(n), but no split overhead

  Good splitters (parallel-friendly):
    - Arrays: O(1) split via midpoint index
    - ArrayList: O(1) split via midpoint index
    - Streams of primitives (range): O(1) split
    - Collections.nCopies(): O(1) split

  Poor splitters (parallel-unfriendly):
    - LinkedList: O(n) traversal required
    - TreeSet: O(log n) per node creation
    - Any iterator-based: O(n) to find split

  Takeaway: ParallelStream.of() on LinkedList guarantees slowdown.
  Profile before using parallelStream(); sequential often wins.

  Impact: Parallel on LinkedList can be 10-100x SLOWER than sequential.

---
id: java-perf-stream-collector-unordered
language: java
severity: info
message: "Performance: Use Collectors.UNORDERED for parallel streams to enable concurrent reduction"
tags:
  - performance
  - stream-api
rule:
  kind: method_call
  pattern: "parallelStream.*collect.*\\(Collectors\\.toList\\(\\)\\)"
note: |
  Collectors.toList() (ordered) requires maintaining element order across parallel tasks.
  This synchronization negates parallelism benefits.
  Unordered collectors like toUnmodifiableSet(), groupingByConcurrent() enable true parallelism.

  Anti-pattern (ordered, single-threaded bottleneck):
    list.parallelStream()
      .filter(x -> x > 10)
      .collect(Collectors.toList());  // Ordered - must sync across threads

  Fix 1 - When order doesn't matter:
    list.parallelStream()
      .filter(x -> x > 10)
      .collect(Collectors.toCollection(ConcurrentLinkedQueue::new));  // Unordered

  Fix 2 - Concurrent reduction (ideal for parallel):
    list.parallelStream()
      .collect(Collectors.groupingByConcurrent(
        Function.identity(),
        Collectors.counting()
      ));  // Parallel-friendly

  Collector characteristics (check JavaDoc):
    - CONCURRENT: Can run in parallel without synchronization
    - UNORDERED: Doesn't require order preservation
    - IDENTITY_FINISH: Accumulator is final result (no expensive finish step)

  Poor performers in parallel:
    - Collectors.toList(): Ordered, non-concurrent
    - Collectors.joining(): Sequential merging of strings
    - Collectors.toMap(): Ordered, requires synchronization on merge

  Good performers in parallel:
    - Collectors.groupingByConcurrent(): Designed for parallelism
    - Collectors.toConcurrentMap(): ConcurrentHashMap-backed
    - Collectors.filtering() with concurrent downstreams

  Impact: Parallel with ordered collector: 1-2x SLOWER than sequential.
  Parallel with unordered collector: 3-8x FASTER (with 4-8 cores, large dataset).

---
id: java-perf-stream-expensive-intermediate
language: java
severity: info
message: "Performance: Stream intermediate operations (filter, map) are lazy - only compute when terminal op called"
tags:
  - performance
  - stream-api
rule:
  kind: method_call
  pattern: "\\$stream\\.filter.*\\.map.*\\."
  not_contains: ["collect", "count", "forEach", "toArray", "findFirst"]
note: |
  Intermediate operations (filter, map, flatMap) are lazy.
  They don't execute until a terminal operation (collect, count, forEach) is called.
  Building a stream pipeline without terminal operation does nothing.

  Anti-pattern (no terminal operation):
    Stream<Integer> stream = list.stream()
      .filter(x -> x > 10)
      .map(x -> x * 2);
    // Does nothing until terminal operation

  Anti-pattern (inefficient ordering):
    list.parallelStream()
      .map(this::expensiveOperation)  // Runs on all elements
      .filter(x -> x > 10);  // Filters after expensive work

    Fix: Filter first to reduce work:
    list.parallelStream()
      .filter(x -> x > 10)  // Reduce dataset
      .map(this::expensiveOperation)  // Only on filtered elements

  Correct usage (terminal operation required):
    List<Integer> result = list.stream()
      .filter(x -> x > 10)
      .map(x -> x * 2)
      .collect(Collectors.toList());  // Terminal - triggers computation

  Optimization strategy - Short-circuit operations:
    int first = list.stream()
      .filter(x -> x > 10)
      .map(x -> x * 2)
      .findFirst()  // Returns immediately on first match
      .orElse(0);  // No unnecessary processing

    Boolean hasAny = list.stream()
      .anyMatch(x -> x > 10);  // Short-circuits on first true

  Impact: Proper ordering can reduce computation by 50-90% for filtered datasets.

---
id: java-perf-stream-custom-collector
language: java
severity: warning
message: "Performance: For custom reductions, implement Collector interface or use reduce() carefully"
tags:
  - performance
  - stream-api
rule:
  kind: method_call
  pattern: "stream\\.reduce\\(.*new.*\\(\\).*\\)"
note: |
  Stream.reduce(identity, accumulator, combiner) creates intermediate objects.
  Custom Collector with supplier factory is more efficient.

  Anti-pattern (creates intermediate objects):
    list.stream()
      .reduce(new StringBuilder(),
              (sb, item) -> sb.append(item),
              (sb1, sb2) -> sb1.append(sb2.toString()))
      .toString();
    // Creates new objects on each parallel subtask

  Fix: Use custom Collector (proper concurrent semantics):
    Collector<String, StringBuilder, String> collector =
      Collector.of(
        StringBuilder::new,  // Supplier - creates per-thread accumulator
        (sb, item) -> sb.append(item),  // Accumulator
        (sb1, sb2) -> sb1.append(sb2),  // Combiner for parallel merge
        StringBuilder::toString  // Finisher
      );

    String result = list.stream()
      .collect(collector);

  Why Collector is better:
    - Supplier() creates per-thread accumulator in parallel tasks
    - reduce() accumulates sequentially, worse for parallel
    - Collector can mark CONCURRENT and UNORDERED for optimization

  When reduce() is appropriate:
    - Simple reductions: sum, min, max (use Stream shortcuts instead)
    - Non-parallel streams only
    - Immutable accumulator (no object creation)

  Stream shortcuts (preferred for simple operations):
    list.stream().mapToInt(x -> x).sum();  // Not reduce()
    list.stream().max(Comparator.naturalOrder());  // Not reduce()
    list.stream().count();  // Not reduce()

  Impact: Custom Collector vs reduce() for large parallel stream: 2-5x faster.

---
id: java-perf-object-allocation-primitive-wrapper
language: java
severity: warning
message: "Performance: Use primitive streams (IntStream, LongStream) instead of Stream<Integer> to avoid boxing"
tags:
  - performance
  - memory-allocation
rule:
  kind: generic_type
  pattern: "Stream<Integer>|Stream<Long>|Stream<Double>|Stream<Boolean>"
note: |
  Stream<Integer> boxes every int into an Integer object.
  Each box: 16 bytes (8B object overhead + 4B int + 4B padding) vs 4 bytes raw.
  IntStream avoids boxing entirely.

  Anti-pattern (boxing overhead):
    List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5);
    int sum = numbers.stream()  // Stream<Integer> = boxed
      .mapToInt(Integer::intValue)  // Unboxing
      .sum();

  Fix: Use IntStream directly:
    IntStream.rangeClosed(1, 5)
      .sum();  // No boxing, efficient

  Memory impact:
    - Stream<Integer> with 1M elements: ~16 MB for Integer objects
    - IntStream with 1M elements: ~4 MB for int array
    - Savings: 75% less memory allocation

  Collection input (still boxing):
    List<Integer> list = ...;
    int sum = list.stream()
      .mapToInt(Integer::intValue)  // Still unboxes from List
      .sum();

  Better: Store as int[] or use primitive collection:
    int[] array = ...;
    int sum = Arrays.stream(array).sum();  // No boxing

  Primitive stream types:
    - IntStream: for byte, short, int, char
    - LongStream: for long
    - DoubleStream: for float, double
    - Note: No BooleanStream - use IntStream with 0/1

  Performance impact:
    - Allocation: 4-8x less memory pressure
    - GC: Fewer objects → less GC overhead
    - Speed: 2-3x faster for large datasets due to memory efficiency

  When boxing is unavoidable:
    - List<Integer> required by API (accept the cost)
    - Use mapToInt() to convert to primitive stream ASAP

---
id: java-perf-object-pooling-reuse
language: java
severity: info
message: "Performance: For frequently allocated objects, implement object pooling to reduce GC pressure"
tags:
  - performance
  - memory-allocation
  - gc-pressure
rule:
  kind: object_creation
  pattern: "new\\s+\\$CLASS\\(\\)"
  frequency: "high_in_loop"
note: |
  Frequent short-lived object allocation stresses garbage collector.
  Object pooling reuses objects instead of allocating new ones.

  Suitable for pooling:
    - Large objects (> 1 KB)
    - Frequently allocated (> 1000/second)
    - Heavy initialization cost
    - Reusable state (resettable)

  Anti-pattern (high GC overhead):
    List<byte[]> results = new ArrayList<>();
    for (int i = 0; i < 1_000_000; i++) {
      byte[] buffer = new byte[1024];  // 1M allocations, 1 GB GC pressure
      process(buffer);
      results.add(Arrays.copyOf(buffer, buffer.length));
    }

  Fix: Object pool with ArrayDeque:
    class BufferPool {
      private final Deque<byte[]> available = new ArrayDeque<>();
      private final int poolSize = 100;

      BufferPool(int bufferSize) {
        for (int i = 0; i < poolSize; i++) {
          available.push(new byte[bufferSize]);
        }
      }

      byte[] acquire() {
        return available.isEmpty() ?
          new byte[1024] : available.pop();
      }

      void release(byte[] buffer) {
        if (available.size() < poolSize) {
          available.push(buffer);
        }
      }
    }

  Production library: Apache Commons Pool2
    GenericObjectPool<MyObject> pool =
      new GenericObjectPool<>(new MyObjectFactory());

  Pooling benefits:
    - Allocation: Single allocation amortized across many uses
    - GC: Fewer objects → less pause time
    - Memory locality: Reused objects stay in cache

  Tradeoff: Complexity vs performance
    - Simple pooling: 10-20% GC overhead reduction
    - Sophisticated pooling: 50-80% GC reduction (for suitable objects)

  Guidelines:
    - Profile heap allocations first (use JProfiler, YourKit)
    - Pool only objects where allocation shows up in profile
    - Keep pool size conservative (100-1000) to avoid memory waste
    - Thread-safe pooling (use ConcurrentLinkedDeque for thread pools)

---
id: java-perf-large-object-allocation
language: java
severity: warning
message: "Performance: Large object allocation (> 256 KB) goes to old generation - triggers major GC"
tags:
  - performance
  - memory-allocation
  - gc-pressure
rule:
  kind: object_creation
  pattern: "new.*\\[.*[0-9]{6,}.*\\]|new byte\\[\\s*([0-9]+ \\* ){0,3}[0-9]+K.*\\]"
note: |
  JVM's young generation (Eden/Survivor) is typically 64-256 MB.
  Objects > 256 KB (default TLAB size) bypass Eden, allocated directly in old generation.
  Old generation GC (full/major GC) is 10-100x slower than young generation GC.

  Anti-pattern (large allocation bypasses young gen):
    byte[] largeBuffer = new byte[1_000_000];  // 1 MB, allocated in old gen
    // Immediately tenured, triggers old gen GC when full

  Issues with large allocations:
    - Bypasses young generation (where most garbage is collected efficiently)
    - Fragments old generation
    - Triggers full GC more frequently
    - Each full GC pauses entire application

  Fix 1 - Use smaller allocations if possible:
    byte[] buffer1 = new byte[100_000];
    byte[] buffer2 = new byte[100_000];
    // Both fit in young gen, collected quickly

  Fix 2 - Reuse large buffers (pooling):
    class BufferManager {
      private byte[] reusableBuffer = new byte[1_000_000];

      void process() {
        // Reuse large buffer multiple times
        // Single allocation instead of many
      }
    }

  Fix 3 - Stream/chunk large data instead of loading all:
    // Bad: Load entire file into memory
    byte[] entire = Files.readAllBytes(largeFile);
    process(entire);

    // Good: Process in chunks
    byte[] chunk = new byte[8192];
    try (InputStream is = Files.newInputStream(largeFile)) {
      while (is.read(chunk) > 0) {
        process(chunk);
      }
    }

  Impact of large allocation:
    - Full GC pause: 10-100 ms (1 GB heap), 100-1000 ms (10 GB heap)
    - Application freeze: Visible to users
    - Throughput: Reduced by 10-50% under sustained load

  Monitoring:
    Use jstat -gc <pid> to monitor GC:
      FGC column shows full GC count
      FGCT column shows full GC cumulative time

    Excessive full GCs (> 1 per second) indicate tuning needed.

---
id: java-perf-jit-method-size
language: java
severity: info
message: "Performance: Keep hot methods small (< 35 bytes bytecode) for JIT inlining"
tags:
  - performance
  - jit-optimization
rule:
  kind: method_definition
  pattern: "public.*\\{.*\\}"
  bytecode_size: "> 35"
  note_condition: "called_in_loop"
note: |
  JIT compiler inlines methods < 35 bytes (default -XX:MaxInlineSize=35).
  Inlining eliminates method call overhead and enables further optimizations.

  How inlining works:
    1. Compiler detects "hot" method (called frequently)
    2. If bytecode < 35 bytes, inlines method body into caller
    3. Eliminates: stack frame setup, register saving, branch prediction
    4. Enables: escape analysis, loop unrolling, better caching

  Anti-pattern (method too large):
    public String formatData(Object[] data) {  // > 35 bytes
      StringBuilder sb = new StringBuilder();
      for (Object obj : data) {
        sb.append(obj.toString());
      }
      return sb.toString();
    }
    // Not inlined, called 1M times in tight loop = slower

  Fix: Refactor to simpler method:
    public String formatData(Object[] data) {
      StringBuilder sb = new StringBuilder();
      for (Object obj : data) {
        appendFormatted(sb, obj);  // Inlined if < 35 bytes
      }
      return sb.toString();
    }

    private void appendFormatted(StringBuilder sb, Object obj) {
      sb.append(obj.toString());  // Simple, inlined
    }

  JIT thresholds:
    - CompileThreshold: Method called 10,000 times → JIT compile
    - MaxInlineSize: 35 bytes → aggressively inline
    - FreqInlineSize: 325 bytes → inline if hot (called often)

  Tuning:
    -XX:MaxInlineSize=100  // Larger inlining threshold
    -XX:FreqInlineSize=1000  // Inline larger hot methods
    -XX:+PrintInlining  // Debug output (verbose, performance impact)

  Getters/setters:
    Trivial getters are always inlined:
      public int getId() { return id; }  // 1 byte, always inlined

  Impact of inlining:
    - Inlined: Method call eliminated (2-3 ns per call)
    - Escape analysis enabled: Object allocation optimized
    - Loop unrolling enabled: Further optimization possible
    - Total speedup for hot methods: 10-50% in tight loops

  Measurement:
    Use JMH (Java Microbenchmark Harness) to measure method performance.

---
id: java-perf-jit-escape-analysis
language: java
severity: info
message: "Performance: Rely on JIT escape analysis - don't manually optimize object allocation away"
tags:
  - performance
  - jit-optimization
rule:
  kind: object_creation
  pattern: "new .*\\(\\);"
  escapes_scope: false
note: |
  JIT escape analysis (enabled by default) optimizes local object allocation:
    - Stack allocation: Object lives on stack (auto-freed on return)
    - Scalar replacement: Object decomposed to individual fields
    - Lock elision: Synchronized removed if only one thread accesses

  How it works:
    1. Analysis determines if object can "escape" the current method/thread
    2. NoEscape: Object can't be seen outside method → stack allocation
    3. ArgEscape: Passed to method but can't escape further → stack allocation
    4. GlobalEscape: Can escape (reachable from global state) → heap allocation

  Example - Stack allocation:
    public long sumDistances(Point[] points) {
      long sum = 0;
      for (Point p1 : points) {
        for (Point p2 : points) {
          Point diff = new Point(p1.x - p2.x, p1.y - p2.y);  // NoEscape
          sum += diff.distance();  // Stack allocated by JIT
        }
      }
      return sum;
    }

    Without escape analysis: 10M allocations → heavy GC
    With escape analysis: Point lives on stack, 0 allocations

  Scalar replacement example:
    Point diff = new Point(p1.x - p2.x, p1.y - p2.y);

    Compiled to:
      int diffX = p1.x - p2.x;
      int diffY = p1.y - p2.y;
      // No object, just two ints

  Synchronization elimination:
    public class StringBuffer {  // Synchronized for thread-safety
      private String[] array;

      public void append(String s) {
        synchronized(this) {
          // ...
        }
      }
    }

    StringBuffer buf = new StringBuffer();  // NoEscape
    buf.append("a");
    buf.append("b");

    JIT detects 'buf' is local, removes synchronization:
      // Compiled without synchronized - faster!

  Enabling/disabling:
    -XX:+DoEscapeAnalysis  (default: enabled in Java 6+)
    -XX:-DoEscapeAnalysis  (disable for debugging)

  Limitations:
    - Only works for "hot" methods (JIT compiled)
    - Warmup time: ~10K method invocations before JIT kicks in
    - Not visible in source code - transparent optimization
    - Disabled in debug/development (usually)

  Don't manually avoid allocation:
    // Bad - premature optimization:
    private Point diffCache;
    public long sumDistances(Point[] points) {
      long sum = 0;
      for (Point p1 : points) {
        for (Point p2 : points) {
          diffCache.x = p1.x - p2.x;  // Reuse same object
          diffCache.y = p1.y - p2.y;  // Confusing, JIT optimizes it anyway
          sum += diffCache.distance();
        }
      }
      return sum;
    }

    // Good - trust JIT:
    public long sumDistances(Point[] points) {
      long sum = 0;
      for (Point p1 : points) {
        for (Point p2 : points) {
          Point diff = new Point(p1.x - p2.x, p1.y - p2.y);
          // JIT stack-allocates this
          sum += diff.distance();
        }
      }
      return sum;
    }

  Impact: Escape analysis can reduce object allocation by 50-95% in suitable code.

---
id: java-perf-reflection-caching
language: java
severity: warning
message: "Performance: Cache reflection lookups - Method/Field/Constructor objects are expensive to create"
tags:
  - performance
  - reflection-overhead
rule:
  kind: method_call
  pattern: "Class\\.forName\\(|getMethod\\(|getField\\(|getConstructor\\(|getDeclaredMethod\\("
  in_loop: true
note: |
  Reflection metadata lookup (Class.forName, getMethod) traverses class hierarchy, checks security.
  Each lookup takes microseconds; in a loop, this adds up.
  Cache Method/Field/Constructor objects.

  Anti-pattern (repeated lookups):
    for (Object obj : objects) {
      Method method = obj.getClass().getMethod("process");  // O(n) lookups
      method.invoke(obj);
    }

  Fix: Cache Method object:
    Method method = String.class.getMethod("length");  // Lookup once
    for (Object obj : objects) {
      method.invoke(obj);  // Reuse cached Method
    }

  Caching strategies:
    1. Static field (class-level cache):
      private static final Method METHOD =
        String.class.getMethod("length");

    2. HashMap cache (runtime registration):
      private static final Map<String, Method> methodCache =
        new ConcurrentHashMap<>();

      Method getMethod(Class<?> clazz, String name) {
        return methodCache.computeIfAbsent(
          clazz.getName() + "." + name,
          k -> clazz.getMethod(name)
        );
      }

    3. ClassValue (specialized for class-based caching):
      private static final ClassValue<Method> methodCache =
        new ClassValue<Method>() {
          protected Method computeValue(Class<?> type) {
            return type.getMethod("process");
          }
        };

  Reflection performance:
    - Direct call: 2.5 ns
    - Reflection (first call): 5000-10000 ns (method lookup overhead)
    - Reflection (cached): 5-10 ns (invoke overhead only)

  When caching helps:
    - Method called > 100 times in application lifetime
    - Reflection used in hot loops
    - Any dynamic invocation with frequency > 1/ms

  When caching is overkill:
    - One-time initialization
    - Rarely used code paths
    - Already using MethodHandle (better alternative)

  Better alternative - MethodHandle (Java 7+):
    MethodHandle mh = MethodHandles.lookup()
      .findVirtual(String.class, "length",
        MethodType.methodType(int.class));

    for (String obj : strings) {
      int len = (int) mh.invoke(obj);  // 10x faster than reflection
    }

  MethodHandle advantages:
    - Direct method invocation compiled by JIT
    - No security checks per invocation (done at lookup)
    - Near-native performance after JIT warmup
    - Type-safe at compile time

  Impact:
    - Reflection caching: 2-5x speedup vs uncached
    - MethodHandle: 10-100x speedup vs reflection

---
id: java-perf-methodhandle-caching
language: java
severity: info
message: "Performance: Use MethodHandle for dynamic invocation - faster than reflection with JIT optimization"
tags:
  - performance
  - reflection-overhead
rule:
  kind: method_call
  pattern: "getMethod\\(.*\\)\\.invoke\\(|getDeclaredMethod\\(.*\\)\\.invoke\\("
note: |
  MethodHandle provides low-level method invocation optimizable by JIT.
  After JIT compilation, MethodHandle.invoke() is nearly as fast as direct calls.

  Reflection performance (per invocation):
    - Direct method call: 2.5 ns
    - Reflection.invoke (method cached): 5-10 ns
    - MethodHandle.invoke (cached, compiled): 3-5 ns
    - First-time MethodHandle lookup: 100-500 ns

  Anti-pattern (reflection in tight loop):
    Method method = obj.getClass().getMethod("getValue");
    for (int i = 0; i < 1_000_000; i++) {
      Object result = method.invoke(obj);  // 5-10 ns per call
    }
    // 5-10 million nanoseconds = 5-10 ms for 1M iterations

  Fix: Use MethodHandle:
    MethodHandle mh = MethodHandles.lookup()
      .findVirtual(obj.getClass(), "getValue",
        MethodType.methodType(Object.class));

    for (int i = 0; i < 1_000_000; i++) {
      Object result = mh.invokeExact(obj);  // 3-5 ns after JIT warmup
    }
    // 3-5 million nanoseconds = 3-5 ms for 1M iterations

  MethodHandle types:
    - invokeExact(args): Type-checked invoke, faster (use if types known)
    - invoke(args): Type-conversions allowed, slightly slower

  Setup MethodHandle:
    1. Get MethodHandles.Lookup:
      MethodHandles.Lookup lookup = MethodHandles.lookup();

    2. Find method:
      MethodHandle mh = lookup.findVirtual(
        clazz,                              // Class where method is defined
        methodName,                         // Method name
        MethodType.methodType(             // Method signature
          returnType,                      // Return type
          paramType1, paramType2, ...      // Parameter types
        )
      );

    3. Bind to object (if needed):
      MethodHandle boundMh = mh.bindTo(obj);  // Pre-bind 'this'

    4. Invoke:
      boundMh.invokeExact(arg1, arg2);  // Type-safe

  Example - invoking lambda:
    Supplier<String> supplier = () -> "hello";
    MethodHandle mh = MethodHandles.lookup()
      .findVirtual(supplier.getClass(), "get",
        MethodType.methodType(Object.class));

    String result = (String) mh.invoke(supplier);

  MethodHandle advantages:
    1. Type-safe (compile-time checking with invokeExact)
    2. JIT-optimizable (no reflection interception)
    3. No security checks per invocation (only at lookup)
    4. Supports method references natively
    5. Lower overhead than reflection.invoke()

  MethodHandle caching:
    private static final MethodHandle GET_VALUE =
      MethodHandles.lookup().findVirtual(
        MyClass.class, "getValue",
        MethodType.methodType(Object.class)
      );

    // Reuse GET_VALUE throughout application

  Bootstrap cost:
    - First MethodHandle lookup: 100-500 ns (one-time)
    - Reflection Method lookup: 5000-10000 ns (one-time)
    - Per-invocation: MethodHandle 3-5 ns, Reflection 5-10 ns

  When to use MethodHandle:
    - Dynamic invocation in hot loops
    - Performance-critical reflection
    - Alternative to anonymous inner classes

  When reflection is acceptable:
    - One-time invocations
    - Initialization code (not performance-critical)
    - API requires reflection (then cache Method object)

  Impact: MethodHandle vs Reflection in tight loop: 2-10x speedup.

---
id: java-perf-capturing-lambda-allocation
language: java
severity: warning
message: "Performance: Avoid capturing lambdas in tight loops - creates new object per iteration"
tags:
  - performance
  - lambda-functional
rule:
  kind: lambda_expression
  captures_variable: "external"
  in_loop: true
note: |
  Capturing lambda (references external variable) creates new object instance per invocation.
  Non-capturing lambda (only uses parameters) creates single object (reused).

  Anti-pattern (capturing lambda in loop):
    int[] counter = {0};
    for (int i = 0; i < 1_000_000; i++) {
      // Capturing lambda - creates 1M objects
      Supplier<Integer> supplier = () -> counter[0]++;
      process(supplier);  // Allocation on each iteration
    }
    // 1M+ Integer objects allocated, GC pressure

  Fix 1 - Use non-capturing lambda:
    for (int i = 0; i < 1_000_000; i++) {
      // Non-capturing lambda - single object created (reused)
      Supplier<Integer> supplier = () -> i;  // Compiler error - i not effectively final
    }

    // If you need loop variable, use different approach

  Fix 2 - Hoist lambda outside loop:
    Supplier<Integer> supplier = () -> computeValue();  // Created once
    for (int i = 0; i < 1_000_000; i++) {
      process(supplier);  // Reused 1M times
    }

  Fix 3 - Use method reference (non-capturing):
    class Processor {
      public static Integer computeValue() { ... }
    }

    Supplier<Integer> supplier = Processor::computeValue;  // Non-capturing
    for (int i = 0; i < 1_000_000; i++) {
      process(supplier);  // Reused
    }

  Why capturing causes allocation:
    Capturing lambdas require storing captured variables.
    Each lambda instance is a new object.

    public class CapturedLambda {
      private final int[] counter;  // Captured variable
      public Integer get() { return counter[0]++; }
    }

  Non-capturing lambda compilation:
    // Compiled to single static method
    private static Integer getConstant() { return CONSTANT; }

    // Lambda instance is single, reused object
    supplier = CapturedLambda::getConstant;

  Memory impact:
    - Capturing lambda: 1M objects × 16-24 bytes = 16-24 MB memory
    - Non-capturing lambda: 1 object × 16-24 bytes = 16-24 bytes
    - Difference: 1000x memory for capturing lambdas

  Detection:
    Enable allocation profiling in JProfiler/YourKit
    Look for xxx$$Lambda allocations in hot methods

  When capturing is unavoidable:
    Use method reference instead of lambda:
      Runnable r = System::gc;  // Lighter than lambda capture

  Performance impact:
    - Capturing lambda in loop: 1M allocations, significant GC
    - Non-capturing/hoisted: Negligible allocation
    - Speedup: 10-50% faster by avoiding allocations

---
id: java-perf-method-reference-vs-lambda
language: java
severity: info
message: "Performance: Method references are slightly more efficient than lambdas - prefer when applicable"
tags:
  - performance
  - lambda-functional
rule:
  kind: lambda_expression
  can_be_method_reference: true
note: |
  Method references compile slightly more efficiently than lambdas.
  Both are JIT-optimized similarly, but method references avoid lambda wrapper.

  Equivalent code:
    // Lambda - creates lambda wrapper
    list.forEach(item -> item.process());

    // Method reference - direct invocation
    list.forEach(MyClass::process);

    // Compiled similarly, but method reference is more direct

  Method reference types:
    1. Static method: ClassName::staticMethod
    2. Instance method: obj::instanceMethod
    3. Constructor: ClassName::new
    4. Wildcard: (x) -> x.method() has no direct reference form

  Converting lambda to method reference:
    Lambda: (x) -> x.length()
    Reference: String::length

    Lambda: (x, y) -> x.compareTo(y)
    Reference: String::compareTo

    Lambda: () -> new String()
    Reference: String::new

  When lambda can't be converted to method reference:
    - Multiple statements: (x) -> { x.process(); return x; }
    - Conditional logic: (x) -> x > 10 ? process(x) : skip(x)
    - Custom computation: (x) -> x * 2 + 5

  Performance comparison:
    Micro-benchmark results (1M invocations):
    - Direct method call: 1.5 ms
    - Method reference: 1.6 ms (negligible difference after JIT)
    - Lambda: 1.6-1.7 ms (negligible difference after JIT)

  At scale (billions of invocations):
    - Direct call: 0.1% overhead reduction
    - Method reference: Near-identical to direct (after JIT)
    - Lambda: Near-identical to method reference (after JIT)

  Why prefer method references:
    1. Slightly more readable intent
    2. No lambda overhead wrapper (minimal)
    3. Compiler recognizes pattern optimizations
    4. Equivalent compiled performance

  JIT optimization:
    Both are inlined by JIT after warmup.
    Performance difference: < 1% (negligible).

  Real performance gains from correct structure:
    // Slow - captures variable
    int[] counter = {0};
    list.forEach(x -> process(counter[0]++));  // Allocation per element

    // Fast - method reference, no capture
    list.forEach(this::process);  // No allocation

  Recommendation:
    - Use method reference when applicable (cleaner, same performance)
    - Lambda when dynamic/conditional behavior needed
    - Both negligibly different in performance after JIT

---
id: java-perf-this-method-reference-loop
language: java
severity: warning
message: "Performance: Avoid passing this::method reference repeatedly in loops - can cause allocation"
tags:
  - performance
  - lambda-functional
rule:
  kind: method_call
  pattern: "pass.*\\(.*this::[a-zA-Z_]+.*\\)"
  in_loop: true
note: |
  Method reference this::method passed in loop creates new object per iteration.
  Hoist outside loop to reuse single object.

  Anti-pattern (allocation per loop iteration):
    for (int i = 0; i < 1_000_000; i++) {
      executor.submit(this::processItem);  // Creates new bound method ref
    }
    // 1M allocations for bound method reference

  Fix: Hoist method reference outside loop:
    Runnable task = this::processItem;  // Created once
    for (int i = 0; i < 1_000_000; i++) {
      executor.submit(task);  // Reused 1M times
    }

  Why allocation happens:
    this::method is bound method reference (carries 'this' context).
    Each reference is new SAM (Single Abstract Method) instance.

  Unbound vs bound method reference:
    Unbound (static): ClassName::staticMethod  // No allocation in loop
    Bound: obj::instanceMethod  // Allocation per use
    Unbound instance: ClassName::instanceMethod  // No allocation, pass receiver

  Optimization:
    1. Store bound method reference in field:
      private Runnable task = this::processItem;

      for (int i = 0; i < 1_000_000; i++) {
        executor.submit(task);
      }

    2. Use unbound, provide receiver explicitly:
      Consumer<MyClass> action = MyClass::processItem;

      for (int i = 0; i < 1_000_000; i++) {
        action.accept(this);
      }

    3. Use static method reference (no binding needed):
      Runnable task = Utilities::processItem;  // Static method, no binding

      for (int i = 0; i < 1_000_000; i++) {
        executor.submit(task);
      }

  Impact:
    - Bound reference in loop: 1M allocations, GC pressure
    - Hoisted reference: 1 allocation, reused
    - Speedup: 5-20% faster by hoisting

---
id: java-perf-nio-channel-usage
language: java
severity: info
message: "Performance: Use FileChannel for large file I/O instead of streams for better throughput"
tags:
  - performance
  - io-operations
rule:
  kind: object_creation
  pattern: "new FileInputStream|new FileOutputStream|new FileReader|new FileWriter"
  file_size: "> 1MB"
note: |
  FileChannel (java.nio) provides memory-mapped I/O and direct buffer access.
  Much faster than streams for large files (> 1 MB).

  Stream I/O (java.io):
    - Byte-by-byte or buffered copying
    - Default buffer: 8 KB (inefficient for large files)
    - No direct memory access
    - Throughput: ~50-100 MB/s for large files

  Channel I/O (java.nio):
    - Memory-mapped files: Zero-copy access
    - Direct ByteBuffer: Native memory, cache-efficient
    - Large transfer() operation: Optimized for bulk transfer
    - Throughput: ~500-1000 MB/s (2-10x faster)

  Benchmark - Reading 1 GB file:
    FileInputStream: ~10-20 seconds
    FileChannel.map(): ~1-2 seconds (10x faster)

  Anti-pattern (slow stream I/O):
    try (FileInputStream fis = new FileInputStream(file);
         FileOutputStream fos = new FileOutputStream(output)) {
      byte[] buffer = new byte[8192];  // Default buffer
      int read;
      while ((read = fis.read(buffer)) > 0) {
        fos.write(buffer, 0, read);
      }
    }
    // Single-threaded, bounded by buffer size

  Fix 1 - Memory-mapped file (for read-only):
    try (RandomAccessFile raf = new RandomAccessFile(file, "r")) {
      FileChannel fc = raf.getChannel();
      long size = fc.size();
      MappedByteBuffer mbb = fc.map(
        FileChannel.MapMode.READ_ONLY, 0, size);

      while (mbb.hasRemaining()) {
        byte b = mbb.get();
        process(b);
      }
    }
    // Zero-copy memory mapping, fastest for read-only

  Fix 2 - Direct ByteBuffer with large transfer:
    try (FileInputStream fis = new FileInputStream(file);
         FileOutputStream fos = new FileOutputStream(output);
         FileChannel fcIn = fis.getChannel();
         FileChannel fcOut = fos.getChannel()) {

      long size = fcIn.size();
      fcIn.transferTo(0, size, fcOut);  // Optimized bulk transfer
    }
    // Single operation, optimized by kernel

  Fix 3 - Large direct ByteBuffer:
    try (FileChannel fc = FileChannel.open(
      Paths.get(file),
      StandardOpenOption.READ)) {

      ByteBuffer buffer = ByteBuffer.allocateDirect(1_000_000);  // 1 MB
      while (fc.read(buffer) > 0) {
        buffer.flip();
        process(buffer);
        buffer.clear();
      }
    }
    // Direct buffer (native memory), larger buffer = fewer I/O ops

  DirectByteBuffer vs HeapByteBuffer:
    - Direct: Allocated outside Java heap, faster for I/O
    - Heap: Allocated in Java heap, easier to manage (GC)
    - For files: Use direct (no garbage in native memory)

  Buffer size tuning:
    - Small buffer (8 KB): Many I/O operations, high overhead
    - Medium buffer (1 MB): Balanced, good for most files
    - Large buffer (10 MB+): Fewer operations, more memory use
    - Sweet spot: 256 KB - 4 MB per actual hardware

  Memory-mapped file limitations:
    - Max size: ~2 GB (integer indexing)
    - Requires VM heap space (not physical RAM)
    - Not suitable for files > 2 GB or small files

  Impact:
    - Stream I/O: ~50-100 MB/s
    - FileChannel.transferTo(): ~500-1000 MB/s (5-10x faster)
    - Memory-mapped: ~1000+ MB/s (near-disk-speed, 10-20x faster)

---
id: java-perf-buffering-io-operations
language: java
severity: warning
message: "Performance: Always wrap streams in BufferedInputStream/BufferedOutputStream"
tags:
  - performance
  - io-operations
rule:
  kind: object_creation
  pattern: "new FileInputStream|new FileOutputStream"
  not_contains: ["BufferedInputStream", "BufferedOutputStream"]
note: |
  Unbuffered stream I/O calls OS for every byte, causing context switches.
  BufferedInputStream/OutputStream batches I/O, reducing system calls 100-1000x.

  Anti-pattern (unbuffered):
    try (FileInputStream fis = new FileInputStream(file)) {
      byte[] buffer = new byte[1024];
      int read;
      while ((read = fis.read(buffer)) > 0) {
        for (byte b : buffer) {
          process(b);  // Many small reads = many system calls
        }
      }
    }

  Fix: Use buffering:
    try (BufferedInputStream bis = new BufferedInputStream(
      new FileInputStream(file), 65536)) {  // 64 KB buffer
      int b;
      while ((b = bis.read()) != -1) {
        process(b);  // Buffered read, amortized
      }
    }

  Default vs custom buffer size:
    - Default: 8 KB (usually too small)
    - Small buffer (8 KB): Many system calls
    - Medium buffer (64 KB): Balanced
    - Large buffer (1 MB+): Fewer calls, more memory

  Benchmark - 1 MB file, byte-by-byte read:
    Unbuffered: ~500 ms (1M system calls)
    Buffered (8 KB): ~5 ms (128 system calls)
    Buffered (64 KB): ~1 ms (16 system calls)
    Buffered (1 MB): ~1 ms (1 system call, memory overhead)

  Tuning buffer size:
    - Network streams: 64 KB (common)
    - Local files: 64 KB - 1 MB
    - Very large files: 1-4 MB (if memory allows)
    - Very small files: 8-16 KB (waste less memory)

  For write operations:
    try (BufferedOutputStream bos = new BufferedOutputStream(
      new FileOutputStream(file), 65536)) {
      for (byte b : data) {
        bos.write(b);  // Buffered, amortized
      }
    }  // Auto-flush on close

  Combined read/write:
    try (BufferedInputStream bis = new BufferedInputStream(
      new FileInputStream(input), 65536);
         BufferedOutputStream bos = new BufferedOutputStream(
      new FileOutputStream(output), 65536)) {

      byte[] buffer = new byte[65536];
      int read;
      while ((read = bis.read(buffer)) > 0) {
        bos.write(buffer, 0, read);
      }
    }

  Impact:
    - Small operations: 10-100x faster
    - Large operations: 2-5x faster (system call overhead amortized)
    - Memory cost: ~65 KB per stream (negligible)

---
id: java-perf-try-with-resources
language: java
severity: info
message: "Performance: Use try-with-resources to ensure timely resource cleanup and avoid descriptor leaks"
tags:
  - performance
  - io-operations
rule:
  kind: method_call
  pattern: "new.*Stream|new.*Reader|new.*Writer|new.*Channel"
  not_contains: ["try\\s*\\(.*AutoCloseable.*\\)"]
  resource_type: ["InputStream", "OutputStream", "Reader", "Writer"]
note: |
  Not closing resources timely causes file descriptor leaks.
  File descriptors are limited (usually ~1024), leaking exhausts them.
  OS then refuses to open new files, crashing application.

  Try-with-resources (Java 7+) auto-closes in order, suppresses exceptions.

  Anti-pattern (explicit close, manual exception handling):
    BufferedReader br = null;
    try {
      br = new BufferedReader(new FileReader(file));
      String line;
      while ((line = br.readLine()) != null) {
        process(line);
      }
    } catch (IOException e) {
      e.printStackTrace();
    } finally {
      if (br != null) {
        try {
          br.close();  // Manual close, exception-prone
        } catch (IOException e) {
          // Handle close exception
        }
      }
    }

  Fix: Try-with-resources (auto-close):
    try (BufferedReader br = new BufferedReader(
      new FileReader(file))) {
      String line;
      while ((line = br.readLine()) != null) {
        process(line);  // Auto-closes on exit (normal or exception)
      }
    } catch (IOException e) {
      e.printStackTrace();
    }

  Multiple resources (all auto-closed):
    try (FileInputStream fis = new FileInputStream(input);
         FileOutputStream fos = new FileOutputStream(output);
         BufferedInputStream bis = new BufferedInputStream(fis);
         BufferedOutputStream bos = new BufferedOutputStream(fos)) {

      byte[] buffer = new byte[65536];
      int read;
      while ((read = bis.read(buffer)) > 0) {
        bos.write(buffer, 0, read);
      }
    }  // All closed in reverse order (bos, bis, fos, fis)

  Exception handling:
    try-with-resources suppresses exceptions from close().
    Primary exception is thrown, close exceptions are added as suppressed.

    Retrieve suppressed exceptions:
      try {
        ...
      } catch (IOException e) {
        for (Throwable suppressed : e.getSuppressed()) {
          // Handle suppressed exceptions from close()
        }
      }

  Custom AutoCloseable resources:
    public class MyResource implements AutoCloseable {
      @Override
      public void close() {
        // Cleanup
      }
    }

    try (MyResource res = new MyResource()) {
      // Use res
    }  // Auto-closes

  File descriptor exhaustion:
    Without try-with-resources:
      for (int i = 0; i < 100_000; i++) {
        BufferedReader br = new BufferedReader(
          new FileReader("file.txt"));  // Never explicitly closed
        process(br);
      }
      // After ~1000 iterations: "Too many open files" exception

    With try-with-resources:
      for (int i = 0; i < 100_000; i++) {
        try (BufferedReader br = new BufferedReader(
          new FileReader("file.txt"))) {
          process(br);
        }  // Immediately closed, descriptor reusable
      }

  Impact:
    - Memory: No leaked file descriptors
    - Stability: Prevents application crash
    - Code: Cleaner, exception-safe
    - Performance: Timely resource release (beneficial for GC)

---
id: java-perf-nio-selector-polling
language: java
severity: info
message: "Performance: Use NIO Selector for many concurrent connections (better than thread-per-connection)"
tags:
  - performance
  - io-operations
rule:
  kind: method_call
  pattern: "new.*Thread.*|Executors\\.newFixedThreadPool\\(.*\\)"
  context: "network_io"
note: |
  Thread-per-connection model:
    - Each client connection = new thread
    - N connections = N threads, N stacks (1-2 MB each)
    - Context switching overhead = N * overhead
    - Max ~1000 threads before performance collapse

  NIO Selector model:
    - Single thread monitors all connections
    - Selector.select() blocks until event ready
    - Handle ready connections, loop
    - Can handle 10,000+ connections in single thread

  Resource usage comparison:
    1000 connections:
      Thread-per-connection: 1000 threads × 1 MB stack = 1 GB memory
      NIO Selector: 1 thread × 1 MB stack = 1 MB memory
      Difference: 1000x less memory

  Anti-pattern (thread-per-connection, scales poorly):
    ExecutorService executor = Executors.newCachedThreadPool();

    try (ServerSocket ss = new ServerSocket(8080)) {
      while (true) {
        Socket socket = ss.accept();
        executor.submit(() -> {
          try (Socket s = socket) {
            handleClient(s);  // Each client = new thread
          }
        });
      }
    }
    // Works for 100 clients, fails for 10,000

  Fix: Use NIO Selector:
    ServerSocketChannel ssc = ServerSocketChannel.open();
    ssc.bind(new InetSocketAddress(8080));
    ssc.configureBlocking(false);

    Selector selector = Selector.open();
    ssc.register(selector, SelectionKey.OP_ACCEPT);

    while (true) {
      selector.select();  // Blocks until event ready
      Set<SelectionKey> ready = selector.selectedKeys();

      for (SelectionKey key : ready) {
        if (key.isAcceptable()) {
          ServerSocketChannel server = (ServerSocketChannel) key.channel();
          SocketChannel client = server.accept();
          client.configureBlocking(false);
          client.register(selector, SelectionKey.OP_READ);
        } else if (key.isReadable()) {
          SocketChannel client = (SocketChannel) key.channel();
          readData(client);
        }
      }
      ready.clear();
    }

  Memory and CPU impact:
    Connections: 100
      Thread pool: 100 threads, ~100 MB, context switching cost
      Selector: 1 thread, ~1 MB, efficient polling

    Connections: 10,000
      Thread pool: Crash (can't create 10K threads)
      Selector: Single thread, scales smoothly

  When to use each:
    Thread-per-connection:
      - < 50 concurrent connections
      - Each connection does heavy blocking I/O (DB queries, etc.)
      - Simpler code, acceptable overhead

    NIO Selector:
      - > 1000 concurrent connections
      - Lightweight I/O (reading, parsing)
      - Need scalability

  Alternative: Virtual threads (Java 19+, Project Loom):
    Virtual threads provide million-thread scalability without Selector complexity.

    try (var scope = new StructuredTaskScope.ShutdownOnFailure()) {
      for (int i = 0; i < 1_000_000; i++) {
        scope.fork(() -> handleVirtualThread());
      }
    }

  Impact:
    NIO Selector vs Thread pool (10K connections):
      - Scalability: 100-1000x more connections
      - Memory: 1000x less (1 MB vs 1 GB)
      - Latency: More consistent (no context switching)

---
id: java-perf-unnecessary-exception
language: java
severity: warning
message: "Performance: Avoid throwing exceptions for control flow - expensive to create and throw"
tags:
  - performance
rule:
  kind: throw_statement
  pattern: "throw.*Exception\\(\\)"
  in_loop: true
note: |
  Exception creation captures full stack trace - expensive O(n) operation.
  Throwing exception unwinds stack - additional cost.
  Use exceptions for errors, not control flow.

  Anti-pattern (exceptions for control flow):
    public int indexOf(List<String> list, String target) {
      try {
        int index = 0;
        for (String item : list) {
          if (item.equals(target)) {
            throw new NotFoundException("Found at " + index);
          }
          index++;
        }
      } catch (NotFoundException e) {
        return Integer.parseInt(e.getMessage().split(" ")[2]);
      }
      return -1;  // Not found
    }
    // Awful - exception for success case

  Fix: Use normal control flow:
    public int indexOf(List<String> list, String target) {
      int index = 0;
      for (String item : list) {
        if (item.equals(target)) {
          return index;  // Early return
        }
        index++;
      }
      return -1;  // Not found
    }

  Exception cost breakdown:
    Stack trace creation: 1000-5000 ns (captures ~50 stack frames)
    Exception throwing: 100-500 ns
    Stack unwinding: 500-2000 ns (per frame)
    Total: 2000-10000 ns per exception

    vs normal return: 1-2 ns

    Difference: 1000-10000x slower

  Real exception handling (appropriate use):
    public String readFile(Path path) {
      try {
        return Files.readString(path);
      } catch (IOException e) {
        logger.error("Failed to read file", e);
        return "";  // Fallback or re-throw
      }
    }
    // Appropriate - actual error condition

  Anti-pattern (exception in expected path):
    public class Cache {
      public Object get(String key) {
        try {
          return map.get(key);  // Null return = normal
        } catch (NullPointerException e) {  // Exception for missing key?!
          return null;
        }
      }
    }

  Fix: Return Optional or null:
    public Optional<Object> get(String key) {
      return Optional.ofNullable(map.get(key));
    }

    // Usage
    cache.get("key").ifPresentOrElse(
      v -> process(v),
      () -> handleMissing()
    );

  Performance test - finding item in list:
    Using exception (1000 items, item at position 500):
      Time: ~5 ms (exception creation + throw + catch)

    Using return:
      Time: ~0.001 ms (direct return)

    Difference: 5000x faster

  When exceptions are appropriate:
    - Actual errors (I/O failures, network errors)
    - Invalid state (postcondition violation)
    - Resource exhaustion (out of memory)
    - Not: expected conditions, control flow alternatives

  Impact: Avoid exceptions in hot loops; 1000-10000x performance difference.

---
id: java-perf-null-check-eager
language: java
severity: info
message: "Performance: Check null early to avoid wasted computation in subsequent code"
tags:
  - performance
rule:
  kind: method_body
  pattern: "if.*== null|if.*!= null"
  position: "early"
note: |
  Checking null early prevents expensive computation on null values.

  Anti-pattern (compute before null check):
    public void process(String input) {
      String result = expensiveTransform(input);  // Could process null
      if (result == null) {
        return;  // Wasted computation
      }
      useResult(result);
    }

  Fix: Check null first:
    public void process(String input) {
      if (input == null) {
        return;  // Early exit, skip expensive transform
      }
      String result = expensiveTransform(input);  // Now safe
      useResult(result);
    }

  Multiple checks:
    public void process(String a, String b, String c) {
      // Check all parameters early
      if (a == null || b == null || c == null) {
        throw new IllegalArgumentException("Null parameter");
      }

      // Now safe to use all parameters
      String result = a + b + c;  // No null checks needed
    }

  Modern alternative (Objects.requireNonNull):
    public void process(String input) {
      Objects.requireNonNull(input, "input must not be null");  // Checked once
      String result = expensiveTransform(input);  // Safe, no re-checks
    }

  Impact: Avoiding wasted computation on null; savings depend on operation cost (0-90%).

---
id: java-perf-object-equality-fields
language: java
severity: info
message: "Performance: Order equals() field checks by cost and likely failure rate"
tags:
  - performance
rule:
  kind: method_definition
  method_name: "equals"
note: |
  Equals() checks multiple fields; order matters for performance.
  Check cheap fields first, fields likely to differ first.

  Good order:
    1. Primitive types (int, long, boolean): O(1), cheap
    2. String comparisons: O(n), cost depends on string length
    3. Complex object comparisons: O(n) or O(n²)
    4. Fields most likely to differ first (early exit)

  Anti-pattern (checks expensive field first):
    public boolean equals(Object obj) {
      if (!(obj instanceof MyClass)) return false;
      MyClass other = (MyClass) obj;

      // Field 1: Expensive array comparison O(n)
      if (!Arrays.equals(this.largeArray, other.largeArray)) {
        return false;
      }

      // Field 2: Cheap int comparison O(1)
      if (this.id != other.id) {
        return false;
      }

      return true;
    }

  Fix: Check cheap/likely fields first:
    public boolean equals(Object obj) {
      if (!(obj instanceof MyClass)) return false;
      MyClass other = (MyClass) obj;

      // Field 1: Cheap int comparison O(1) - likely to differ
      if (this.id != other.id) {
        return false;  // Early exit, skip array comparison
      }

      // Field 2: Expensive array comparison O(n)
      if (!Arrays.equals(this.largeArray, other.largeArray)) {
        return false;
      }

      return true;
    }

  Cost estimation:
    int comparison: < 1 ns
    String comparison (equal): O(length) = ~100 ns per 1000 chars
    Array comparison: O(length) = ~100 ns per 1000 elements
    Object comparison (recursive): Varies, 1000+ ns typical

  Likelihood estimation:
    - ID fields: Very likely to differ (99% of false cases)
    - Enum fields: Likely to differ (80% of false cases)
    - String fields: Moderately likely (50% of false cases)
    - Collections: Less likely (20% of false cases)

  Optimized equals():
    public boolean equals(Object obj) {
      if (this == obj) return true;  // Identity check (1 ns)
      if (!(obj instanceof Person)) return false;  // Type check
      Person other = (Person) obj;

      // Check cheap fields first
      if (this.id != other.id) return false;  // 1 ns, 99% false rate
      if (this.age != other.age) return false;  // 1 ns, 80% false rate

      // Then expensive fields
      if (!this.name.equals(other.name)) return false;  // 100+ ns, 50%
      if (!Arrays.equals(this.tags, other.tags)) return false;  // 1000+ ns

      return true;
    }

  Impact: Optimized equals() order: 50-90% faster on average (most comparisons fail early).

---
id: java-perf-cache-local-variable
language: java
severity: info
message: "Performance: Cache array length in local variable for tight loops"
tags:
  - performance
rule:
  kind: for_statement
  pattern: "for\\s*\\(.*i\\s*<\\s*\\$array\\.length.*\\)"
note: |
  Array.length property access compiles to bytecode instruction (fast).
  However, caching in local variable can enable better compiler optimization.

  Difference is negligible in modern JVMs (< 1% improvement).
  Caching is micro-optimization; only useful in extreme hot paths.

  Pattern:
    // Slight variation in bytecode, compiler handles both efficiently
    for (int i = 0; i < array.length; i++) {
      process(array[i]);
    }

    // Slightly better (caches bounds check):
    int len = array.length;
    for (int i = 0; i < len; i++) {
      process(array[i]);
    }

  Modern JIT optimization:
    Both compile to same efficient loop.
    Array bounds checking is hoisted out of loop automatically.
    Difference: < 1% performance variation.

  When to cache:
    - Only if array.length access is in hot loop profile
    - Not worth it for clarity unless profound performance needs
    - ArrayList.size() is O(1), safe to call in loop condition

  Impact: < 1% performance difference; negligible for almost all code.

---
id: java-perf-collection-clear-reuse
language: java
severity: info
message: "Performance: Reuse collection objects by clearing instead of re-allocating"
tags:
  - performance
  - memory-allocation
rule:
  kind: variable_assignment
  pattern: "\\$collection\\s*=\\s*new.*\\(\\)"
  in_loop: true
note: |
  Clearing and reusing collection avoids allocation/GC overhead.

  Anti-pattern (allocation per iteration):
    for (int batch = 0; batch < 1000; batch++) {
      List<Item> items = new ArrayList<>();  // 1000 allocations
      loadItems(items);
      process(items);
      // items discarded, 1000 GC collections
    }

  Fix: Reuse collection:
    List<Item> items = new ArrayList<>();
    for (int batch = 0; batch < 1000; batch++) {
      items.clear();  // O(1), reuses capacity
      loadItems(items);
      process(items);
      // Single allocation, reused 1000 times
    }

  Collections.clear() vs new:
    - new ArrayList(): Allocates array, O(n) copy and init
    - list.clear(): Keeps capacity, O(n) element zero-out (still faster)

    More aggressive reuse:
    for (int batch = 0; batch < 1000; batch++) {
      items.clear();
      items.addAll(loadItems());  // Reuse capacity
    }

  Garbage collection impact:
    1000 allocations: 1000 objects → GC every few iterations
    1 allocation, 1000 reuses: 1 object → GC rarely triggered

  Impact: 10-30% faster for batched operations; depends on batch size and item size.

---
id: java-perf-foreach-vs-for
language: java
severity: info
message: "Performance: For-each and traditional for-loop have equivalent performance (compiler optimizes both)"
tags:
  - performance
rule:
  kind: for_statement
note: |
  Modern Java compiler transforms for-each to traditional for-loop at compile time.
  No performance difference after compilation.
  Prefer for-each for readability; compiler handles optimization.

  Compiled equivalence:
    // For-each
    for (String item : list) {
      process(item);
    }

    // Compiled to equivalent:
    for (Iterator<String> it = list.iterator(); it.hasNext(); ) {
      String item = it.next();
      process(item);
    }

  When for-each is optimal:
    - ArrayList, Array: Optimized to index-based loop
    - LinkedList: Iterator-based (necessary to avoid O(n²))
    - Any Iterable: Uses iterator (appropriate)

  Performance:
    For-each: ~2 ns per iteration
    Traditional for-loop: ~2 ns per iteration
    Difference: < 1% (negligible)

  Guideline: Use for-each unless specific reason for traditional loop:
    Good: for (int item : array) { ... }
    Good: for (Item item : collection) { ... }

    When traditional loop needed:
    - Index manipulation: for (int i = 0; i < n; i += 2)
    - Reverse iteration: for (int i = n-1; i >= 0; i--)
    - Multiple collections: for (int i = 0; i < n; i++)

  Impact: No performance difference; choose based on readability.

---
id: java-perf-string-split-regex
language: java
severity: warning
message: "Performance: Avoid String.split() in loops - regex compilation is expensive"
tags:
  - performance
rule:
  kind: method_call
  pattern: "\\$str\\.split\\(.*\\)"
  in_loop: true
note: |
  String.split(regex) compiles regex each time (if pattern not cached).
  Regex compilation is expensive (~microseconds).
  In a loop with 1M iterations, adds milliseconds.

  Anti-pattern (repeated regex compilation):
    for (String line : lines) {
      String[] parts = line.split(",");  // Compiles "," regex each time
      process(parts);
    }
    // 1M splits × ~microseconds = milliseconds wasted

  Fix 1 - Cache Pattern:
    Pattern pattern = Pattern.compile(",");
    for (String line : lines) {
      String[] parts = pattern.split(line);  // Regex already compiled
      process(parts);
    }

  Fix 2 - Use Apache Commons Lang:
    for (String line : lines) {
      String[] parts = StringUtils.split(line, ',');  // Optimized, no regex
      process(parts);
    }

  Benchmark - 1M string splits on "," delimiter:
    String.split(): ~1000 ms (compilation overhead)
    Pattern.split(): ~50 ms (precompiled pattern)
    StringUtils.split(): ~10 ms (no regex, optimized)

    Speedup: 100x faster with Pattern, 1000x with StringUtils

  When split() is acceptable:
    - One-time use (not in loop)
    - Initialization (startup, not runtime)

  Performance tip: For simple delimiters, avoid regex:
    String[] parts = line.split(",");  // WRONG for loops
    String[] parts = line.split(",", -1);  // Keep empty trailing

    Faster alternative (no regex):
    String[] parts = StringUtils.split(line, ',');

  Impact: Cached Pattern: 10-100x faster; StringUtils: 100-1000x faster.

---
id: java-perf-string-operations-chain
language: java
severity: info
message: "Performance: Chain string operations to avoid intermediate String objects"
tags:
  - performance
  - string-handling
rule:
  kind: method_call
  pattern: "\\$str1\\.concat|\\$str1\\s*\\+.*|String\\.format"
  in_sequence: true
note: |
  Each string operation creates intermediate object.
  Chaining operations (split, substring, replace) creates multiple copies.
  Combine into single operation when possible.

  Anti-pattern (multiple intermediate objects):
    String result = input.replace("a", "A");  // Copy 1
    result = result.replace("b", "B");  // Copy 2
    result = result.substring(0, 10);  // Copy 3
    // 3 intermediate String objects for single logical operation

  Fix: Use appropriate method:
    // If possible, use a single operation
    String result = input
      .replace("a", "A")
      .replace("b", "B")
      .substring(0, 10);  // Still 3 copies, but chained

    Better: Process once:
    StringBuilder sb = new StringBuilder(input);
    replaceAll(sb, "a", "A");
    replaceAll(sb, "b", "B");
    String result = sb.substring(0, 10).toString();

  Case study - Processing CSV:
    Anti-pattern (many intermediate strings):
    String[] fields = line.split(",");
    fields[0] = fields[0].trim();
    fields[1] = fields[1].toUpperCase();
    process(fields);
    // Multiple String objects per field

    Better:
    String[] fields = line.split(",");
    for (int i = 0; i < fields.length; i++) {
      fields[i] = fields[i].trim().toUpperCase();
    }
    // Still multiple intermediates

    Best (use StringBuilder):
    String[] fields = line.split(",");
    for (int i = 0; i < fields.length; i++) {
      fields[i] = processField(fields[i]);
    }

    String processField(String field) {
      return field.trim().toUpperCase();  // JIT optimizes away
    }

  Impact: Depends on string length and operation count; typically 5-30% improvement.

---
id: java-perf-volatile-false-sharing
language: java
severity: warning
message: "Performance: Avoid volatile variables in tight loops sharing CPU cache line"
tags:
  - performance
  - synchronization
rule:
  kind: field_declaration
  modifier: "volatile"
  access_pattern: "frequent_loop"
note: |
  Volatile writes bypass CPU cache, directly accessing main memory.
  Main memory access: ~200 ns vs cache access: ~4 ns (50x slower).

  Additionally, volatile variable can cause false sharing:
  If volatile field shares cache line (64 bytes) with other fields,
  cache line invalidation on write affects all fields.

  Anti-pattern (volatile in tight loop):
    class Counter {
      volatile long count = 0;  // Volatile write = memory barrier

      void increment() {
        count++;  // 200+ ns per increment (no cache)
      }
    }

    for (int i = 0; i < 1_000_000; i++) {
      counter.increment();  // 1M × 200 ns = 200 ms
    }

  Fix 1 - Use AtomicLong for counter:
    AtomicLong counter = new AtomicLong();
    for (int i = 0; i < 1_000_000; i++) {
      counter.incrementAndGet();  // ~10-20 ns (CAS optimized)
    }
    // 1M × 15 ns = 15 ms (13x faster)

  Fix 2 - Local variable (no synchronization):
    long count = 0;
    for (int i = 0; i < 1_000_000; i++) {
      count++;  // ~1 ns (register, no memory access)
    }
    // 1M × 1 ns = 1 ms (200x faster than volatile)
    // Then atomically store result:
    counter.set(count);

  False sharing example:
    class Data {
      volatile long x;      // Cache line 1 (bytes 0-63)
      long y;               // Cache line 1 (bytes 0-63) - false sharing
      long z;               // Cache line 1 (bytes 0-63) - false sharing
    }

    Writing to 'x' invalidates entire cache line,
    affecting 'y' and 'z' even though they're not volatile.

  Fix: Padding to prevent false sharing:
    class Data {
      volatile long x;
      long p1, p2, p3, p4, p5, p6, p7;  // Padding to next cache line
      long y;  // Now in separate cache line
      long z;  // Now in separate cache line
    }

  Modern approach (Java 9+ @jdk.internal.vm.annotation.Contended):
    @jdk.internal.vm.annotation.Contended
    class Data {
      volatile long x;  // Automatically padded
      long y;
      long z;
    }

  When volatile is necessary:
    - Shared flags between threads
    - Initialization checks (double-checked locking)
    - Rarely accessed (not in tight loop)

  Impact: Volatile in tight loop: 50-200x SLOWER than non-volatile alternatives.

---
id: java-perf-contended-padding
language: java
severity: info
message: "Performance: Use @Contended annotation to avoid false sharing on fields accessed by different threads"
tags:
  - performance
  - synchronization
rule:
  kind: field_declaration
  accessed_by: "multiple_threads"
note: |
  False sharing occurs when two threads access different fields that share CPU cache line.
  Cache line: 64 bytes (modern CPUs).
  Writing to one field invalidates entire line, forcing reload for other field.

  Detection:
    Profile CPU cache misses (L1/L2 misses spike when false sharing occurs).
    Symptom: Performance degrades with more threads/cores.

  Example:
    class Pair {
      long x;  // 8 bytes, offset 0-7
      long y;  // 8 bytes, offset 8-15 (same cache line!)
    }

    Thread 1: Repeatedly writes x (cache line invalidation)
    Thread 2: Repeatedly reads y  (waits for cache line from L3/memory)

  Fix: Padding to separate cache lines:
    class PaddedPair {
      long x;                    // 8 bytes
      long p1, p2, p3, p4, p5, p6;  // 48 bytes padding
      long y;                    // 8 bytes (different cache line)
    }

  Annotation approach (Java 9+):
    @jdk.internal.vm.annotation.Contended
    class ContendedPair {
      long x;  // Automatically padded by JVM
      long y;  // Automatically padded by JVM
    }

    Requires: -XX:-RestrictContended flag to enable outside java.base module

  Benchmark (false sharing):
    2 threads, each incrementing own field in shared object:
      Without padding: ~100 million operations/second (cache line conflicts)
      With padding: ~2000 million operations/second (20x faster)

  When to apply:
    - Fields modified by different threads frequently
    - Performance profile shows cache misses
    - High-frequency counters, flags, state variables

  Impact: Proper contention handling: 10-50x performance improvement in multi-threaded scenarios.

---
id: java-perf-atomic-vs-synchronized
language: java
severity: info
message: "Performance: Prefer Atomic* classes to synchronized blocks for simple counters and flags"
tags:
  - performance
  - synchronization
rule:
  kind: synchronized_statement
note: |
  AtomicInteger, AtomicLong use Compare-And-Swap (CAS) operations.
  CAS is lock-free, faster than synchronized (which uses OS locks).

  Synchronized block:
    - Acquires monitor lock (OS kernel synchronization)
    - Other threads block waiting for lock
    - Serializes all operations
    - Context switch overhead: ~microseconds

  Atomic CAS:
    - Optimistic: Assume no conflict, update value
    - If conflict, retry (loop)
    - Lock-free: No blocking, threads progress independently
    - Typical cost: ~10-20 ns per operation (vs 100+ ns for synchronized)

  Anti-pattern (synchronized for simple counter):
    class Counter {
      private int count = 0;

      synchronized void increment() {
        count++;  // Lock/unlock overhead > actual work
      }

      synchronized int get() {
        return count;
      }
    }

  Fix: Use AtomicInteger:
    class Counter {
      private AtomicInteger count = new AtomicInteger();

      void increment() {
        count.incrementAndGet();  // Lock-free CAS
      }

      int get() {
        return count.get();  // Volatile read
      }
    }

  Benchmark (1M increments, single thread):
    Synchronized: ~200 ms
    AtomicInteger: ~15 ms
    Difference: 13x faster

  Benchmark (1M increments, 8 threads):
    Synchronized: ~5000 ms (severe contention)
    AtomicInteger: ~50 ms (lock-free, scales)
    Difference: 100x faster

  When Atomic is appropriate:
    - Simple counters, flags, references
    - Frequent updates
    - Low contention workloads

  When synchronized is better:
    - Protecting multiple fields (compound operations)
    - Complex logic requiring atomicity

  Atomic classes available:
    AtomicBoolean, AtomicInteger, AtomicLong, AtomicReference,
    AtomicIntegerArray, AtomicLongArray, AtomicReferenceArray,
    AtomicMarkableReference, AtomicStampedReference

  Impact: Atomic vs synchronized for simple operations: 10-100x faster depending on contention.

---
id: java-perf-immutable-objects-cache
language: java
severity: info
message: "Performance: Cache immutable objects (strings, constants) for memory and GC efficiency"
tags:
  - performance
  - memory-allocation
rule:
  kind: object_creation
  pattern: "new String\\(.*\\)|new Integer\\(.*\\)|new Long\\(.*\\)"
note: |
  Immutable objects are safe to share and cache.
  Reusing immutable objects instead of creating new saves memory and GC cost.

  String interning (JVM optimizes):
    String s1 = "hello";  // Cached in string pool
    String s2 = "hello";  // Reuses s1 (pool lookup)
    assert s1 == s2;      // true

  Integer caching (JVM optimizes):
    Integer a = 5;        // Cached (-128 to 127)
    Integer b = 5;        // Reuses cached instance
    assert a == b;        // true (same object)

    Integer c = 200;      // Not cached
    Integer d = 200;      // New object
    assert c == d;        // false (different objects!)

  Anti-pattern (unnecessary object creation):
    for (int i = 0; i < 1000; i++) {
      String s = new String("constant");  // 1000 String objects
      Integer num = Integer.valueOf(i);   // Cached (-128-127), new for >127
      process(s, num);
    }
    // 1000 String objects, GC pressure

  Fix: Cache immutable objects:
    private static final String CONSTANT = "constant";

    for (int i = 0; i < 1000; i++) {
      Integer num = Integer.valueOf(i);  // Cached, or use int primitive
      process(CONSTANT, num);
    }

  String interning best practices:
    Good: Constant strings, enum names
    Bad: Dynamic strings from user input (don't intern)

    // Good
    public static final String STATUS_OK = "OK";

    // Bad
    String s = userInput.intern();  // User input shouldn't be interned

  Integer boxing (use primitives):
    // Avoid boxing
    Integer sum = 0;
    for (int i = 1; i <= 1000; i++) {
      sum += i;  // Unbox + operation + rebox each iteration
    }

    // Use primitive
    int sum = 0;
    for (int i = 1; i <= 1000; i++) {
      sum += i;  // No boxing
    }

  Cache immutable objects:
    class Configuration {
      private static final Map<String, Config> CACHE =
        new ConcurrentHashMap<>();

      public static Config getConfig(String name) {
        return CACHE.computeIfAbsent(name, k -> loadConfig(k));
      }
      // Config is immutable, cached and reused
    }

  Impact: Caching immutable objects: 5-50% memory reduction, 10-30% GC improvement.

---
id: java-perf-unmodifiable-collections
language: java
severity: info
message: "Performance: Use Collections.unmodifiableList/Map/Set to prevent accidental modification and hint at optimization"
tags:
  - performance
  - memory-allocation
rule:
  kind: method_return
  pattern: "return\\s+new.*\\(.*\\)"
note: |
  Unmodifiable wrapper prevents accidental modification.
  Additionally, some optimizations can assume immutability.

  Example:
    public List<String> getItems() {
      return Collections.unmodifiableList(items);  // Can't be modified
    }

  Java 9+ alternatives:
    public List<String> getItems() {
      return List.of(items);  // Immutable, more efficient
    }

  Performance implications:
    Unmodifiable wrappers add one indirection (method call).
    Cost: negligible (< 1% performance impact).

    Benefit: Allows caller to cache and reuse safely.

  When to use:
    - Public API returning internal collections
    - Cache objects (prevent accidental corruption)
    - Documenting intent (immutable to callers)

  Impact: Negligible performance cost; documentation/safety benefit.

---
id: java-perf-hash-consistency
language: java
severity: warning
message: "Performance: Ensure hashCode() is consistent with equals() - inconsistency causes HashMap degradation"
tags:
  - performance
rule:
  kind: method_definition
  method_name: "equals|hashCode"
note: |
  If hashCode() and equals() are inconsistent, HashMap degrades to O(n) for all operations.

  Contract:
    - If obj1.equals(obj2) == true, then obj1.hashCode() == obj2.hashCode()
    - hashCode() should return consistent value for same object

  Anti-pattern (inconsistent hash/equals):
    public class Point {
      int x, y;

      @Override
      public boolean equals(Object obj) {
        if (!(obj instanceof Point)) return false;
        Point p = (Point) obj;
        return this.x == p.x && this.y == p.y;  // Based on x and y
      }

      @Override
      public int hashCode() {
        return 42;  // WRONG - constant hash, ignores x/y
      }
    }

    // Usage
    HashMap<Point, String> map = new HashMap<>();
    map.put(new Point(1, 2), "A");
    map.put(new Point(1, 2), "B");
    map.put(new Point(2, 3), "C");

    // All points hash to 42 (collision)
    // HashMap becomes linked list (O(n))
    map.get(new Point(1, 2));  // O(n) instead of O(1)

  Fix: Consistent hash/equals:
    @Override
    public int hashCode() {
      return Objects.hash(x, y);  // Based on same fields as equals()
    }

  Good hash function properties:
    1. Consistent: Same object always returns same hash
    2. Equal objects: Equal objects must have equal hash
    3. Distributed: Different objects should produce different hashes
    4. Immutable: Object fields shouldn't change after hashing

  Anti-pattern (mutable field in hash):
    public class Person {
      String name;

      @Override
      public int hashCode() {
        return name.hashCode();  // WRONG - name is mutable
      }
    }

    HashMap<Person, String> map = new HashMap<>();
    Person p = new Person("Alice");
    map.put(p, "Data");

    p.name = "Bob";  // Change field
    map.get(p);      // Can't find - hash changed!

  Fix: Use immutable fields or defensive copy:
    public class Person {
      private final String name;  // Immutable

      @Override
      public int hashCode() {
        return name.hashCode();  // Safe - name never changes
      }
    }

  Quality hash function:
    // Good
    @Override
    public int hashCode() {
      return Objects.hash(id, name, email);  // All relevant fields
    }

    // Better (more control)
    @Override
    public int hashCode() {
      int result = id;
      result = 31 * result + name.hashCode();
      result = 31 * result + email.hashCode();
      return result;
    }

  Impact: Inconsistent hash/equals: O(n) instead of O(1) for HashMap - 1000x SLOWER on large maps.
---
id: java-perf-early-termination
language: java
severity: info
message: "Performance: Use short-circuit operators and early termination to avoid unnecessary evaluation"
tags:
  - performance
rule:
  kind: logical_expression
note: |
  Short-circuit evaluation (&&, ||) stops evaluating when result determined.

  Examples:
    if (obj != null && obj.isValid()) {
      // If obj is null, isValid() not called (short-circuit)
    }

    if (list.isEmpty() || hasItems(list)) {
      // If list.isEmpty() is true, hasItems() not called
    }

  Anti-pattern (no short-circuit):
    if (isValid(obj) & obj != null) {  // & instead of &&
      // Both sides evaluated even if obj null
      // Second operand: obj.isValid() might throw NPE
    }

  Bitwise operators (&, |) don't short-circuit:
    Use && and || for logical operations
    Use & and | only for bitwise operations

  Stream short-circuit:
    list.stream()
      .anyMatch(x -> x > 100);  // Returns immediately on first true

    list.stream()
      .limit(10)  // Stop after 10 elements
      .collect(...);

  Order for short-circuit:
    Put likely-true condition first for OR:
      if (hasErrors() || isWarning()) {  // If hasErrors() likely true
      }

    Put likely-false condition first for AND:
      if (isEmpty() && hasData()) {  // If isEmpty() likely true
      }

  Impact: Proper short-circuiting: 5-50% faster for conditional branches with expensive operations.

---
id: java-perf-jvm-flags-optimization
language: java
severity: info
message: "Performance: Enable key JVM optimization flags for production: -XX:+UseStringDeduplication, -XX:+UseG1GC"
tags:
  - performance
  - jvm-tuning
rule:
  kind: configuration
note: |
  JVM flags enable optimizations; default settings are conservative.

  Critical flags for production:
    -server              # Server mode (better optimization than default client mode)
    -XX:+UseG1GC         # Generational GC (scales, low pause time)
    -XX:MaxGCPauseMillis=200  # Target GC pause (G1GC adaptive)
    -XX:+UseStringDeduplication  # Deduplicate identical strings (G1GC)
    -XX:+ParallelRefProcEnabled  # Parallel reference processing

  String deduplication:
    -XX:+UseStringDeduplication  # Finds duplicate strings, merges
    Benefit: 10-30% heap reduction for string-heavy apps
    Cost: Small CPU overhead (~5%)

  GC tuning:
    -Xms4g -Xmx4g        # Fixed heap (avoids resize)
    -XX:NewRatio=2       # Young gen : old gen = 1:2
    -XX:MaxTenuringThreshold=15  # Threshold to move to old gen

  Example production JVM:
    java -server \
      -Xms4g -Xmx4g \
      -XX:+UseG1GC \
      -XX:MaxGCPauseMillis=200 \
      -XX:+UseStringDeduplication \
      -XX:+ParallelRefProcEnabled \
      -XX:+PrintGCDetails \
      -XX:+PrintGCDateStamps \
      -Xloggc:gc.log \
      myapp.jar

  Monitoring:
    jstat -gc <pid>  # Check GC statistics
    jmap -heap <pid> # Heap summary
    jcmd <pid> GC.run  # Trigger GC

  Impact: Proper JVM tuning: 5-50% performance improvement; correct GC tuning can reduce GC pause time by 80%+.

---
id: java-perf-gc-young-gen-size
language: java
severity: info
message: "Performance: Right-size young generation heap to minimize GC frequency and pause time"
tags:
  - performance
  - gc-pressure
rule:
  kind: configuration
note: |
  Young generation (Eden + 2 Survivor spaces) size affects GC frequency.
  Too small: Frequent young GC, high CPU overhead
  Too large: Longer GC pauses, memory waste

  Typical sizing: 25-40% of total heap for most applications.

  Configuration:
    -XX:NewRatio=2              # Young:Old = 1:2 (33% young, 67% old)
    -XX:NewRatio=3              # Young:Old = 1:3 (25% young, 75% old)
    -XX:NewSize=512m            # Minimum young gen
    -XX:MaxNewSize=2g           # Maximum young gen

  Monitoring young gen behavior:
    jstat -gc <pid> 1000        # Monitor every 1 second
    Look for:
      - YGC: Young generation collection count
      - YGCT: Young generation collection time

  Signs of undersized young gen:
    - YGC column high (thousands per minute)
    - YGCT column high (> 10% of runtime)
    - CPU usage unexpectedly high with normal throughput

  Signs of oversized young gen:
    - Long pause times (full GC) when young gen gets too fragmented
    - Memory waste (unused capacity)

  Tuning strategy:
    1. Monitor baseline: jstat -gcutil <pid> 1000 (5+ minutes)
    2. Target: YGC frequency ~1-5 per minute for typical app
    3. Adjust NewRatio to achieve target
    4. Re-monitor and validate

  Example - monitoring healthy heap:
    S0C    S1C    S0U    S1U      EC       EU        OC         OU
    5120   5120   5120   0       41216   11215     111104      67301

    Young GC: ~once per minute is typical
    Survivor usage: One survivor should be nearly empty (current allocation space)

  Impact: Proper young gen sizing: 20-40% reduction in GC pause time; improved throughput.

---
id: java-perf-mixed-workload-gc
language: java
severity: info
message: "Performance: Choose GC based on workload: throughput vs latency trade-offs"
tags:
  - performance
  - gc-pressure
rule:
  kind: configuration
note: |
  Java GC options optimized for different workloads.

  G1GC (Garbage First): Default Java 9+, balanced latency/throughput
    -XX:+UseG1GC
    -XX:MaxGCPauseMillis=200       # Target pause time (ms)
    -XX:+ParallelRefProcEnabled     # Parallel reference processing
    Strengths:
      - Predictable pause times (200ms default, tunable)
      - Scales well to large heaps (8GB+)
      - Concurrent background collection
    Weaknesses:
      - Slightly higher overhead than Parallel GC
      - Not ideal for < 2GB heaps

  Parallel GC: Maximize throughput (batch/background processing)
    -XX:+UseParallelGC
    -XX:ParallelGCThreads=8         # Number of GC threads
    Strengths:
      - Highest throughput (~5-10% overhead)
      - Good for CPU-bound workloads
    Weaknesses:
      - Full GC pauses can be long (seconds)
      - Not suitable for latency-sensitive apps

  ZGC: Ultra-low latency (Java 15+)
    -XX:+UseZGC
    Strengths:
      - Sub-millisecond pauses (< 1ms)
      - Scales to TB heaps
    Weaknesses:
      - Only available on Linux
      - Memory overhead (15-20% more heap)
      - Not yet default/widely adopted

  Shenandoah: Low latency (OpenJDK, not HotSpot default)
    -XX:+UseShenandoahGC
    Similar to ZGC, concurrent marking with minimal pauses.

  Choosing GC:
    Throughput-focused (batch processing, analytics):
      -XX:+UseParallelGC
    Latency-sensitive (web services, games):
      -XX:+UseG1GC -XX:MaxGCPauseMillis=50
    Ultra-low latency (high-frequency trading):
      -XX:+UseZGC  (if Java 15+ and Linux)
    Large heaps (8GB+):
      -XX:+UseG1GC or -XX:+UseZGC

  Impact: Right GC choice: 30-60% improvement in P95/P99 latency or throughput.

---
id: java-perf-method-reference-static
language: java
severity: info
message: "Performance: Prefer static method references over instance method references for no binding"
tags:
  - performance
  - lambda-functional
rule:
  kind: method_reference
note: |
  Static method reference has no binding (no object reference to capture).
  Instance method reference binds 'this' (lightweight but still an object).

  Comparison:
    Static:    Math::abs          # No binding, single lambda instance
    Unbound:   String::length     # Unbound, takes receiver as parameter
    Bound:     obj::method        # Bound, captures obj reference

  Example:
    // Static - zero binding overhead
    Supplier<Date> supplier1 = Date::new;  // Static constructor ref

    // Instance - binds 'this'
    Supplier<Date> supplier2 = this::getDate;  // Binds this reference

  Memory impact:
    Static: 16-24 bytes per lambda instance (SAM wrapper)
    Bound: 24-32 bytes per lambda instance (wrapper + captured reference)

  When static available: Prefer for maximum efficiency.

  Impact: Negligible performance difference; static is marginally cleaner.

---
id: java-perf-stream-infinite-warning
language: java
severity: warning
message: "Performance: Infinite streams must have terminal short-circuit operation (limit, findFirst, anyMatch)"
tags:
  - performance
  - stream-api
rule:
  kind: method_call
  pattern: "Stream\\.generate|IntStream\\.iterate"
  terminal_operation: ["collect", "toArray", "forEach"]
note: |
  Infinite streams (Stream.generate, IntStream.iterate) never terminate.
  Terminal operations without limit cause infinite loops.

  Anti-pattern (infinite loop):
    Stream.generate(Math::random)
      .forEach(System.out::println);  // Prints forever

  Fix: Add limit or other short-circuit:
    Stream.generate(Math::random)
      .limit(100)
      .forEach(System.out::println);  // Stops after 100

    Stream.generate(Math::random)
      .anyMatch(x -> x > 0.99);  // Short-circuits on first match

  Infinite stream use cases:
    - Generating sequences: Stream.generate(() -> Random.nextInt())
    - Iterating with state: IntStream.iterate(0, i -> i + 1)
    - Fetching data: Stream<Data>.iterate(firstPage, page -> page.next())

  Always pair with:
    - limit(n): First n elements
    - takeWhile(predicate): While condition true
    - findFirst(): First element
    - anyMatch/allMatch: Short-circuit on match

  Impact: Missing limit on infinite stream: Application hang/crash.

---
id: java-perf-reduce-vs-collect
language: java
severity: info
message: "Performance: Use collect() for mutable reduction, reduce() for immutable"
tags:
  - performance
  - stream-api
rule:
  kind: method_call
  pattern: "stream\\.reduce\\(.*StringBuilder|stream\\.reduce\\(.*new|collect\\(Collectors\\.reducing"
note: |
  reduce(): Designed for immutable reduction (accumulation without mutation)
  collect(): Designed for mutable reduction (StringBuilder, List, etc.)

  Semantically:
    reduce() assumes immutable operations, simpler semantics
    collect() supports mutable operations, parallel-friendly

  Immutable reduction:
    int sum = stream.reduce(0, Integer::sum);  // Correct

  Mutable reduction - WRONG approach:
    String result = stream.reduce("", (a, b) -> a + b);  // Slow, creates strings
    StringBuilder sb = stream.reduce(
      new StringBuilder(),
      (sb, s) -> sb.append(s),  // Wrong - creates intermediate objects
      (sb1, sb2) -> sb1.append(sb2)
    );

  Mutable reduction - CORRECT approach:
    String result = stream.collect(
      Collector.of(
        StringBuilder::new,
        (sb, s) -> sb.append(s),
        (sb1, sb2) -> sb1.append(sb2),
        StringBuilder::toString
      )
    );

  Performance:
    reduce() with mutable objects: Creates intermediate copies
    collect() with Collector: Properly handles mutability

  Impact: reduce() for mutable reduction can be 2-3x SLOWER than collect().

---
id: java-perf-optional-orElseGet
language: java
severity: info
message: "Performance: Use Optional.orElseGet() for expensive defaults, not orElse()"
tags:
  - performance
rule:
  kind: method_call
  pattern: "Optional.*\\.orElse\\(.*\\(\\)|Optional.*\\.orElse\\(new.*\\(\\)"
note: |
  orElse(value): Evaluates argument immediately, even if Optional has value
  orElseGet(supplier): Lazy evaluation, only calls supplier if needed

  Anti-pattern (unnecessary computation):
    Optional<Data> opt = getData();
    Data result = opt.orElse(loadDefaultExpensive());  // Called even if opt has value

  Fix: Use orElseGet for expensive defaults:
    Data result = opt.orElseGet(this::loadDefaultExpensive);  // Lazy

  When orElse is acceptable:
    Cheap values (constants, primitives):
      Optional<String> name = getName();
      String result = name.orElse("unknown");  // Cheap, acceptable

  When orElseGet is necessary:
    Expensive operations:
      Optional<Config> config = loadConfig();
      Config result = config.orElseGet(this::buildDefaultConfig);  // Lazy

    Creating objects:
      Optional<Database> db = getConnection();
      Database result = db.orElseGet(() -> new Database());  // Lazy allocation

  Performance impact:
    Expensive default evaluated: 5-50% slowdown (depending on computation cost)
    orElseGet lazy evaluation: Zero cost when Optional has value

  Best practice:
    - orElse(null): Using null (antipattern, use orElse(null) if necessary)
    - orElse(value): Simple immutable value
    - orElseGet(supplier): Computation, object creation, or expensive operation

  Impact: Lazy evaluation with expensive defaults: 50-90% faster when Optional has value.

---
id: java-perf-arraylist-vs-hashset-lookup
language: java
severity: warning
message: "Performance: Use HashSet for contains() checks, not ArrayList with frequent lookups"
tags:
  - performance
  - collection-selection
rule:
  kind: method_call
  pattern: "\\$arrayList\\.contains\\(|\\$arrayList\\.indexOf\\("
  frequency: "high_in_loop"
note: |
  ArrayList.contains(): O(n) linear search
  HashSet.contains(): O(1) average case

  In a loop with n iterations checking m-element list:
    ArrayList: O(n*m) quadratic
    HashSet: O(n) linear

  Anti-pattern (linear search in loop):
    List<String> validIds = new ArrayList<>(loadIds());  // 1000 items

    for (String input : userInputs) {  // 1M items
      if (validIds.contains(input)) {  // O(n) = 1000 checks per iteration
        process(input);
      }
    }
    // 1M * 1000 = 1B operations = slow

  Fix: Use HashSet for O(1) lookup:
    Set<String> validIds = new HashSet<>(loadIds());  // Converted to set

    for (String input : userInputs) {  // 1M items
      if (validIds.contains(input)) {  // O(1) average
        process(input);
      }
    }
    // 1M + 1000 = ~1M operations = fast

  When ArrayList.contains() is acceptable:
    - List is small (< 20 items)
    - Lookup frequency is low (not in loops)
    - Order must be preserved (use LinkedHashSet if order needed)

  Trade-offs:
    ArrayList: Ordered, memory-efficient, slow contains()
    HashSet: Unordered, memory overhead, fast contains()

  If order matters:
    Use LinkedHashSet (maintains insertion order, O(1) contains())

  Impact: ArrayList vs HashSet for 1M contains() on 1000-item collection: 1000x SLOWER with ArrayList.

---
id: java-perf-for-each-array-vs-list
language: java
severity: info
message: "Performance: For-each on array is slightly faster than on List (no iterator allocation)"
tags:
  - performance
rule:
  kind: for_statement
  pattern: "for\\s*\\(.*:\\s*\\$array\\)|for\\s*\\(.*:\\s*\\$list\\)"
note: |
  For-each on array:
    for (int i = 0; i < array.length; i++) { ... }  // Compiled to traditional loop

  For-each on List:
    for (Iterator<T> it = list.iterator(); it.hasNext(); ) { ... }  // Iterator allocation

  Performance:
    Array: ~1-2 ns per iteration (direct access)
    List: ~2-3 ns per iteration (iterator method calls)

  The difference is negligible after JIT optimization.

  Practical impact:
    1M iterations on array: ~1-2 ms
    1M iterations on list: ~2-3 ms
    Difference: < 50% (negligible)

  Recommendation:
    Use for-each for both array and List (cleaner code).
    Iterator allocation overhead is negligible.
    Only micro-optimize if profiling shows iteration in hot path.

  Impact: Minimal performance difference (< 5%) between array and List iteration.

---
id: java-perf-double-vs-float
language: java
severity: info
message: "Performance: Use double instead of float - no performance difference on modern CPUs"
tags:
  - performance
rule:
  kind: variable_declaration
  type: "float"
note: |
  Legacy advice: float is faster than double (32-bit vs 64-bit).
  Modern reality: FPU (floating-point unit) processes both at same speed.

  64-bit CPU architecture: Native register size is 64-bit
  Modern SSE/AVX: SIMD units process float and double at same speed

  Performance:
    float: ~1 ns per operation (64-bit CPU, native)
    double: ~1 ns per operation (64-bit CPU, native)

  Memory:
    float: 4 bytes (saves memory if storing many floats)
    double: 8 bytes

  Guideline:
    Use double for calculations (same speed, better precision).
    Use float[] for large arrays (saves 50% memory, similar speed).

  Example - when float array helps:
    float[] image = new float[1920 * 1080];  // ~8 MB vs 16 MB
    // Processing speed: same as double[], but half memory

  Example - use double for calculations:
    double sum = 0.0;  // Accumulated precision
    for (float value : floatArray) {
      sum += value;  // Better accumulated precision with double
    }

  Note: Graphics/games often use float for GPU compatibility (not CPU performance).

  Impact: No performance difference between float and double on 64-bit CPU; choose based on memory/precision needs.

---
id: java-perf-generic-type-erasure
language: java
severity: info
message: "Performance: Generics are erased at compile-time - no runtime performance cost or benefit"
tags:
  - performance
rule:
  kind: generic_type
note: |
  Java generics are erased at compile time.
  Generic List<String> and raw List are identical at runtime.

  Compile-time:
    List<String> list = new ArrayList<String>();

  Runtime (bytecode):
    List list = new ArrayList();
    // Type parameter <String> is erased

  Performance implications:
    No difference between:
      List<String>: Type-safe, but erased at runtime
      List: Raw type, no compile-time checking

  Runtime behavior:
    Both compile to identical bytecode.
    Both have identical performance (zero overhead).

  Type erasure exists because Java generics were added in Java 5 (backward compatibility).

  Notes on type checking:
    Type parameters exist only in source code and .class bytecode signature.
    Cannot check instanceof with generic type:
      if (list instanceof List<String>) { }  // Compile error
      if (list instanceof List) { }           // OK, checks raw type

  Impact: No performance difference; generics provide compile-time type safety with zero runtime cost.

---
id: java-perf-enum-vs-constants
language: java
severity: info
message: "Performance: Enum is optimal for constants - use instead of static final int/String"
tags:
  - performance
rule:
  kind: variable_declaration
  type: "static final int|static final String"
  name_pattern: "[A-Z_]+"
note: |
  Enum provides type-safe constants with optimization benefits.

  Comparison:
    Static final int: Inlined by compiler (no lookup cost)
    Static final String: Pool lookup (fast)
    Enum: Singleton instances, fast comparisons

  Example:
    // Old style (unsafe)
    public static final int STATUS_OK = 0;
    public static final int STATUS_ERROR = 1;

    // Enum (type-safe)
    public enum Status {
      OK, ERROR;
    }

  Performance:
    static final int: Inlined, 0 cost per access
    Enum: Singleton lookup, ~1 ns per access
    Difference: Negligible, enum provides type safety

  Enum advantages:
    1. Type-safe (can't pass wrong value)
    2. Singleton per value (memory-efficient)
    3. Can have associated data
    4. Switch statement optimization

  Example - enum with data:
    public enum Status {
      OK(200, "OK"),
      ERROR(500, "Internal Error");

      private final int code;
      private final String message;

      Status(int code, String message) {
        this.code = code;
        this.message = message;
      }

      public int getCode() { return code; }
    }

  Performance with switch:
    Modern JVM optimizes enum switch to tableswitch (O(1)).
    static final int switch uses lookupswitch (O(log n)) or tableswitch.

  Impact: Enum vs static final: Negligible performance difference (~1% slowdown, massive type-safety benefit).

---
id: java-perf-array-initialization-size
language: java
severity: info
message: "Performance: Initialize arrays with exact size if known to avoid allocation on first write"
tags:
  - performance
  - memory-allocation
rule:
  kind: array_creation
  pattern: "new.*\\[\\s*\\]|new.*\\[0\\]"
note: |
  Arrays created with size 0 or without initial size must be resized on first write.
  Create with exact size if known.

  Anti-pattern (resize on first write):
    int[] arr = new int[0];       // Size 0
    // Adding element triggers resize
    arr = Arrays.copyOf(arr, 1);  // Allocation + copy

  Fix: Create with known size:
    int[] arr = new int[expectedSize];  // Allocate once

  If size is unknown:
    Use ArrayList (manages sizing internally):
    List<Integer> list = new ArrayList<>();  // Starts 10, doubles as needed
    // More efficient than manual array resizing

  Performance:
    Resizing array: O(n) copy operation
    Known size: O(1) allocation, one copy

  Array sizing in batch operations:
    int[] result = new int[inputSize];  // Know size upfront
    for (int i = 0; i < inputSize; i++) {
      result[i] = process(input[i]);
    }

  Impact: Array resize on write: 5-10% slowdown if resizing happens once; much worse if repeated.

---
id: java-perf-math-operations-intrinsics
language: java
severity: info
message: "Performance: JIT compiler intrinsifies Math operations (Math.abs, Math.sqrt) - use directly"
tags:
  - performance
  - jit-optimization
rule:
  kind: method_call
  pattern: "Math\\..*\\(.*\\)"
note: |
  JIT compiler has intrinsics for common Math operations.
  Intrinsic = directly compiled to CPU instruction (ultra-fast).

  Intrinsified operations:
    Math.abs(), Math.min(), Math.max()       # ~1 ns
    Math.sqrt(), Math.pow(), Math.log()      # ~10-50 ns (CPU dependent)
    Math.sin(), Math.cos(), Math.tan()       # ~100-500 ns

  Example - JIT intrinsic:
    Math.abs(-42) compiles to CPU abs instruction (~0.5 ns)

  Example - custom implementation (slower):
    public static int myAbs(int x) {
      return x < 0 ? -x : x;  // Not as optimized as CPU instruction
    }
    // ~2-3 ns (conditional branch)

  Guideline:
    Use Math.* for mathematical operations (compiler optimizes).
    Avoid reimplementing (slower, less readable).

  SIMD with Math:
    Modern JIT doesn't auto-vectorize scalar Math (yet).
    For bulk SIMD, use Vector API (Java 17+):
      FloatVector v1 = FloatVector.fromArray(...);
      FloatVector v2 = FloatVector.fromArray(...);
      FloatVector result = v1.mul(v2);  // Vectorized

  Performance:
    Math.abs() intrinsic: ~1 ns
    Custom implementation: ~2-3 ns
    Vector API SIMD: ~0.1 ns per element (16 elements in parallel)

  Impact: Use Math.* intrinsics - 2-3x faster than custom implementations.

---
id: java-perf-null-object-pattern
language: java
severity: info
message: "Performance: Null Object Pattern avoids null checks but adds method call overhead"
tags:
  - performance
rule:
  kind: class_definition
  implements: "some_interface"
note: |
  Null Object Pattern: Replace null with object that does nothing.
  Avoids null checks, but adds method call overhead.

  Traditional null check:
    if (logger != null) {
      logger.log(message);
    }

  Null Object Pattern:
    class NullLogger implements Logger {
      public void log(String msg) { }  // Do nothing
    }

    Logger logger = getLogger();  // Returns NullLogger instead of null
    logger.log(message);  // No null check needed

  Trade-off:
    Null check: Fast (branch prediction), one less method call
    Null Object: No condition, but method call to no-op

  Performance:
    Null check: ~1 ns (branch predicted, cache-friendly)
    Method call to no-op: ~2-3 ns
    Difference: ~2 ns, negligible

  When Null Object helps:
    Multiple null checks (reduces code clutter)
    Polymorphic behavior (subclasses handle differently)

  When null check is better:
    Single/rare checks (simpler)
    Performance-critical paths (branch prediction optimized)

  Example - Null Object in logger:
    public class NullLogger implements Logger {
      public void log(String msg) { }          // No-op
      public void debug(String msg) { }        // No-op
      public void error(String msg) { }        // No-op
    }

    Logger logger = isLoggingEnabled() ?
      new FileLogger() : new NullLogger();

    logger.log("message");  // No null check, polymorphic behavior

  Impact: Negligible performance difference; use for code clarity, not performance.

---
id: java-perf-lazy-initialization-pattern
language: java
severity: info
message: "Performance: Double-checked locking is error-prone - use volatile or class initialization for lazy initialization"
tags:
  - performance
  - synchronization
rule:
  kind: method_definition
  pattern: "synchronized.*if.*null|if.*null.*synchronized"
note: |
  Double-checked locking (DCL) is a common pattern for lazy initialization.
  Modern Java has better alternatives.

  Anti-pattern (broken DCL - not thread-safe):
    public class Singleton {
      private static Singleton instance;

      public static Singleton getInstance() {
        if (instance == null) {  // Check 1
          synchronized (Singleton.class) {
            if (instance == null) {  // Check 2
              instance = new Singleton();
            }
          }
        }
        return instance;
      }
    }
    // Broken due to memory visibility (needs volatile)

  Fix 1 - Make field volatile:
    private static volatile Singleton instance;  // Volatile for visibility

  Fix 2 - Eager initialization (simplest):
    public class Singleton {
      private static final Singleton instance = new Singleton();

      public static Singleton getInstance() {
        return instance;
      }
    }
    // Thread-safe, zero runtime cost

  Fix 3 - Holder pattern (best of both worlds):
    public class Singleton {
      private Singleton() { }

      public static Singleton getInstance() {
        return Holder.instance;  // Class loading is thread-safe
      }

      private static class Holder {
        static final Singleton instance = new Singleton();
      }
    }

  Fix 4 - Supplier (functional approach):
    private final Supplier<Singleton> supplier =
      Suppliers.memoize(() -> new Singleton());

    public Singleton getInstance() {
      return supplier.get();
    }

  Performance:
    Eager initialization: ~0 cost (initialization time)
    Lazy with DCL: ~10-20 ns first call, ~2-3 ns subsequent (cached)
    Holder pattern: Same as lazy, cleaner

  Recommendation:
    Simple singleton: Use eager initialization
    Need lazy loading: Use Holder pattern
    Avoid: Double-checked locking (error-prone, little benefit)

  Impact: Proper lazy initialization: Correct thread-safety with zero runtime cost.

---
id: java-perf-string-equals-vs-compareto
language: java
severity: info
message: "Performance: Use String.equals() for comparison, not compareTo() == 0"
tags:
  - performance
rule:
  kind: method_call
  pattern: "\\$str\\.compareTo\\(.*\\)\\s*==\\s*0|\\$str\\.compareTo\\(.*\\)\\s*!=\\s*0"
note: |
  String.equals(): Fast equality check, early exit on length/content mismatch
  String.compareTo(): Full comparison to determine order, slower for equality

  Performance:
    equals():      Early exit on length or first char mismatch
    compareTo(): Compares all characters (even if unequal)

  Example:
    String s1 = "hello";
    String s2 = "hi";

    s1.equals(s2)         // Returns false after 1 char, ~1-2 ns
    s1.compareTo(s2) == 0 // Compares all chars, ~3-5 ns (wasteful)

  When compareTo() is needed:
    Sorting: Arrays.sort(strings)  // Uses compareTo() correctly
    Ordering: Collections.sort()
    Comparisons: < > <= >=

  When equals() is appropriate:
    Checking equality (==, !=)

  Example - good usage:
    if (username.equals("admin")) {      // Correct - equals()
      ...
    }

    if (list.contains(targetString)) {   // Uses equals() internally
      ...
    }

  Performance impact:
    compareTo() for equality: 2-3x SLOWER than equals() for unequal strings.

  Impact: Use equals() for equality checks; compareTo() only for ordering.

---
id: kotlin-perf-sequence-over-list
language: kotlin
severity: warning
message: "Performance: Use asSequence() for chained collection operations on large collections"
tags:
  - performance
  - collections
note: |
  Sequences use lazy evaluation and avoid creating intermediate collections.
  For large datasets with multiple chained operations (3+ transformations),
  sequences can provide 20-40% performance improvement.
  Reference: https://chrisbanes.me/posts/use-sequence/

---
id: kotlin-perf-inline-large-function
language: kotlin
severity: warning
message: "Performance: Avoid inlining large functions; can cause code bloat"
tags:
  - performance
  - inlining
note: |
  Inlining copies function code to call sites. Large functions increase binary
  size and can negatively impact cache performance.
  Best practice: Only inline small functions (<10 lines) or critical functions.
  Reference: https://kotlinlang.org/docs/inline-functions.html

---
id: kotlin-perf-dispatcher-default-io
language: kotlin
severity: warning
message: "Performance: Use Dispatchers.IO for blocking I/O, Dispatchers.Default for CPU-bound"
tags:
  - performance
  - coroutines
note: |
  Incorrect dispatcher selection causes performance issues.
  Use Dispatchers.IO for file/network I/O, Dispatchers.Default for computation,
  Dispatchers.Main for UI updates.
  Reference: https://developer.android.com/kotlin/coroutines/coroutines-adv

---
id: kotlin-perf-global-scope-avoid
language: kotlin
severity: error
message: "Performance: Avoid GlobalScope; use structured concurrency scopes"
tags:
  - performance
  - coroutines
note: |
  GlobalScope bypasses structured concurrency, preventing lifecycle management.
  Causes memory leaks and uncancellable work.
  Use viewModelScope, lifecycleScope, or explicit coroutineScope instead.
  Reference: https://carrion.dev/en/posts/kotlin-coroutines-dispatchers-jobs/

---
id: kotlin-perf-intarray-over-array-int
language: kotlin
severity: warning
message: "Performance: Use IntArray, LongArray instead of Array<Int>, Array<Long>"
tags:
  - performance
  - jvm-interop
note: |
  Array<Int> allocates boxed integers (5x more memory than IntArray).
  For 1M integers: IntArray uses 4MB, Array<Int> uses 20MB.
  Calculations are ~25% faster with primitive arrays.
  Reference: https://www.baeldung.com/kotlin/intarray-vs-arrayint

---
id: kotlin-perf-flow-over-channel
language: kotlin
severity: warning
message: "Performance: Prefer Flow over ChannelFlow for sequential processing"
tags:
  - performance
  - coroutines
note: |
  Flow is 3x+ faster than ChannelFlow (0.012s vs 0.040s for 1000 emissions).
  Use Flow for sequential data streams.
  Use ChannelFlow only for concurrent producer/collector patterns.
  Reference: https://www.baeldung.com/kotlin/flows-vs-channels

---
id: kotlin-perf-string-concatenation-loop
language: kotlin
severity: error
message: "Performance: Avoid string concatenation (+=) in loops; use StringBuilder"
tags:
  - performance
  - allocation
note: |
  String += in loops allocates new String objects repeatedly.
  10K iterations = 10K intermediate strings. Use StringBuilder instead:
    val sb = StringBuilder()
    for (...) { sb.append(...) }
  StringBuilder is orders of magnitude faster.
  Reference: https://www.compilenrun.com/docs/language/kotlin/kotlin-best-practices/kotlin-performance-tips/

---
id: kotlin-perf-reflection-avoid
language: kotlin
severity: error
message: "Performance: Avoid excessive reflection; it has significant overhead"
tags:
  - performance
  - reflection
note: |
  Reflection is slow: ~1s warmup on first run, ~100-200ms on subsequent runs.
  Use compile-time code generation or data classes instead.
  Profile before optimizing if unsure of impact.
  Reference: https://www.baeldung.com/kotlin/deep-copy-data-class

---
id: kotlin-perf-lazy-expensive-property
language: kotlin
severity: warning
message: "Performance: Beware lazy delegate overhead for trivial properties"
tags:
  - performance
  - delegation
note: |
  Lazy delegates add bytecode, allocate objects, and add method-call overhead.
  Worthwhile only for expensive initialization (parsing, heavy computation).
  Avoid for simple constants or always-used properties.
  Reference: https://medium.com/@nirmale.ashwin9696/the-hidden-cost-of-kotlins-lazy-delegate-in-android-910166bb51fe

---
id: kotlin-perf-data-class-excessive-copy
language: kotlin
severity: warning
message: "Performance: Avoid excessive data class copy() calls in hot paths"
tags:
  - performance
  - allocation
note: |
  The copy() function creates new object allocations on the heap.
  Efficient for shallow copies but frequent copying in hot paths causes GC pressure.
  Consider mutable versions or builder patterns if allocation-heavy.
  Reference: https://www.baeldung.com/kotlin/deep-copy-data-class

---
id: kotlin-perf-nested-loops
language: kotlin
severity: warning
message: "Performance: Avoid nested loops; consider using Map/Set for O(1) lookup"
tags:
  - performance
  - algorithms
note: |
  Nested loops have O(n²) complexity. Consider:
  - Using Map/Set for O(1) lookup instead of nested iteration
  - Using flatMap() or asSequence() for chained processing
  - Reordering loops: put smallest iteration outer
  - Caching expensive computations outside inner loop
  Reference: https://www.compilenrun.com/docs/language/kotlin/kotlin-best-practices/kotlin-performance-tips/

---
id: kotlin-perf-regex-reuse
language: kotlin
severity: warning
message: "Performance: Compile regex patterns once; reuse across calls"
tags:
  - performance
  - regex
note: |
  Compiling regex on each call is expensive. Cache compiled patterns:
    companion object {
      private val PATTERN = Regex(pattern)
    }
  Reuse PATTERN instead of recompiling each time.
  Reference: https://www.compilenrun.com/docs/language/kotlin/kotlin-best-practices/kotlin-performance-tips/

---
id: kotlin-perf-extension-function-lambda
language: kotlin
severity: warning
message: "Performance: Mark extension functions with lambdas as inline"
tags:
  - performance
  - extension-functions
note: |
  Extension functions with lambdas should be inline to avoid lambda object allocation.
  Without inline, every invocation allocates a function object on the heap.
  Reference: https://medium.com/swlh/kotlin-dilemma-extension-or-member-38912d4c0989

---
id: kotlin-perf-collection-preallocation
language: kotlin
severity: info
message: "Performance: Preallocate collections with expected capacity"
tags:
  - performance
  - collections
note: |
  Dynamic resizing causes repeated allocations and copying.
  If known final size, preallocate: ArrayList<T>(expectedSize)
  Reference: https://www.compilenrun.com/docs/language/kotlin/kotlin-best-practices/kotlin-performance-tips/

---
id: kotlin-perf-apply-initialization
language: kotlin
severity: info
message: "Performance: Use apply() for fluent object initialization"
tags:
  - performance
  - scope-functions
note: |
  apply() returns the object, ideal for fluent initialization patterns.
  Inline with minimal overhead.
  Reference: https://kotlinlang.org/docs/scope-functions.html

---
id: kotlin-perf-let-null-safety
language: kotlin
severity: info
message: "Performance: Use let() for null-safe operations"
tags:
  - performance
  - scope-functions
note: |
  let() with safe call ensures block runs only if value is non-null.
  Use for null-safe transformations. Returns lambda result.
  Reference: https://kotlinlang.org/docs/scope-functions.html

---
id: kotlin-perf-also-side-effects
language: kotlin
severity: info
message: "Performance: Use also() for side effects"
tags:
  - performance
  - scope-functions
note: |
  also() returns receiver object, perfect for side effects without breaking chains.
  Example: val result = process().also { log.info(it) }.next()
  Reference: https://kotlinlang.org/docs/scope-functions.html

---
id: kotlin-perf-flow-lazy
language: kotlin
severity: info
message: "Performance: Flow is cold/lazy"
tags:
  - performance
  - coroutines
note: |
  Flows are cold streams. Emission doesn't begin until collection.
  Allows efficient resource management and late binding.
  Reference: https://elizarov.medium.com/kotlin-flows-and-coroutines-256260fb3bdb

---
id: kotlin-perf-channel-communication
language: kotlin
severity: info
message: "Performance: Use Channel for inter-coroutine communication"
tags:
  - performance
  - coroutines
note: |
  Channels are hot streams for communication between concurrent coroutines.
  Use when producers/consumers run independently.
  Reference: https://kotlinlang.org/docs/channels.html

---
id: kotlin-perf-flow-backpressure
language: kotlin
severity: info
message: "Performance: Flow handles backpressure natively"
tags:
  - performance
  - coroutines
note: |
  Flow automatically suspends slow collectors. No buffering overhead unless explicit.
  Channels require manual buffer management.
  Reference: https://elizarov.medium.com/kotlin-flows-and-coroutines-256260fb3bdb

---
id: kotlin-perf-primitive-arrays
language: kotlin
severity: warning
message: "Performance: Use primitive arrays for numeric collections"
tags:
  - performance
  - jvm-interop
note: |
  List<Int> has 5x memory overhead. Use IntArray, FloatArray, DoubleArray, etc.
  Calculations are ~25% faster with primitive arrays.
  Reference: https://kt.academy/article/ek-arrays

---
id: kotlin-perf-boxing-interop
language: kotlin
severity: warning
message: "Performance: Beware autoboxing in Java interop"
tags:
  - performance
  - jvm-interop
note: |
  Passing Kotlin primitives to Java generics causes autoboxing wrapper objects.
  For critical interop: use primitive arrays or @JvmExposeBoxed.
  Reference: https://typealias.com/guides/inline-classes-and-autoboxing/

---
id: kotlin-perf-lazy-thread-safety
language: kotlin
severity: info
message: "Performance: Consider LazyThreadSafetyMode for performance"
tags:
  - performance
  - delegation
note: |
  Default lazy uses synchronized initialization. For single-threaded contexts:
  val prop by lazy(LazyThreadSafetyMode.PUBLICATION)
  Reference: https://kotlinlang.org/docs/delegated-properties.html

---
id: kotlin-perf-crossinline
language: kotlin
severity: info
message: "Performance: Use crossinline for nested lambda contexts"
tags:
  - performance
  - inlining
note: |
  crossinline maintains inlining while disallowing non-local returns.
  Better than noinline for nested contexts needing inlining benefits.
  Reference: https://www.dhiwise.com/post/how-kotlin-crossinline-can-make-your-code-better

---
id: kotlin-perf-noinline
language: kotlin
severity: info
message: "Performance: Use noinline for stored/passed lambdas"
tags:
  - performance
  - inlining
note: |
  Mark lambdas as noinline when storing or passing them.
  Disables inlining but allows lambda storage and manipulation.
  Reference: https://www.baeldung.com/kotlin/crossinline-vs-noinline

---
id: kotlin-perf-context-switching
language: kotlin
severity: info
message: "Performance: Minimize withContext() overhead"
tags:
  - performance
  - coroutines
note: |
  Each withContext call incurs context switching. Batch operations on same dispatcher.
  Profile to measure impact on your workload.
  Reference: https://medium.com/@riztech.dev/understanding-coroutine-dispatchers-in-kotlin-impact-on-coroutine-threads-c30f57729a21

---
id: kotlin-perf-fine-grained-coroutines
language: kotlin
severity: info
message: "Performance: Avoid spawning thousands of tiny coroutines"
tags:
  - performance
  - coroutines
note: |
  Each coroutine adds overhead. Thousands of tiny coroutines negate benefits.
  Batch operations or use Flow for efficient processing.
  Reference: https://carrion.dev/en/posts/kotlin-coroutines-dispatchers-jobs/

---
id: kotlin-perf-companion-lazy
language: kotlin
severity: info
message: "Performance: Use lazy for expensive companion properties"
tags:
  - performance
  - allocation
note: |
  Companion properties are eagerly initialized at class load time.
  For expensive resources, use lazy with LazyThreadSafetyMode.PUBLICATION.
  Reference: https://medium.com/@nirmale.ashwin9696/the-hidden-cost-of-kotlins-lazy-delegate-in-android-910166bb51fe

---
id: kotlin-perf-elvis-operator-safe
language: kotlin
severity: info
message: "Performance: Elvis operator (?:) efficiently provides defaults"
tags:
  - performance
  - null-handling
note: |
  Evaluates right side only if left is null. Ensure default is not expensive.
  Good for providing defaults without unnecessary overhead.
  Reference: https://www.dhiwise.com/post/kotlin-elvis-operator-the-key-to-safer-null-proof-code

---
id: kotlin-perf-safe-call-operator
language: kotlin
severity: info
message: "Performance: Safe call (?.) has minimal overhead"
tags:
  - performance
  - null-handling
note: |
  Null-safe with minimal overhead compared to explicit null checks.
  Preferred over !! when nullability is possible.
  Reference: https://agrawalsuneet.github.io/blogs/safe-calls-vs-null-checks-in-kotlin/

---
id: kotlin-perf-lazy-vs-eager-chains
language: kotlin
severity: info
message: "Performance: Use asSequence() for filter+map+chains"
tags:
  - performance
  - collections
note: |
  Without asSequence(): multiple intermediate collections created.
  With asSequence(): fused element-by-element processing, no intermediates.
  Use: list.asSequence().filter(...).map(...).toList()
  Reference: https://kt.academy/article/ek-sequence

---
id: kotlin-perf-sorted-list-exception
language: kotlin
severity: warning
message: "Performance: sorted() is slower on Sequence"
tags:
  - performance
  - collections
note: |
  sorted() must materialize entire sequence, negating lazy benefits.
  Use on List directly: list.sorted() vs list.asSequence().sorted()
  Reference: https://chrisbanes.me/posts/use-sequence/

---
id: kotlin-perf-data-class-equality-override
language: kotlin
severity: info
message: "Performance: Override equals() for key-field comparisons"
tags:
  - performance
  - allocation
note: |
  Data class equals() compares all properties by default.
  If only comparing specific fields (e.g., ID), override equals().
  Reference: https://www.baeldung.com/kotlin/deep-copy-data-class

---
id: kotlin-perf-when-vs-if-else
language: kotlin
severity: info
message: "Performance: Use when for efficient switch compilation"
tags:
  - performance
  - control-flow
note: |
  when expressions compile to efficient switch statements for enums/integers.
  More efficient and readable than if/else chains.
  Reference: https://medium.com/@debuggingisfun/kotlin-when-anti-pattern-26902baf7362

---
id: kotlin-perf-enum-memory
language: kotlin
severity: info
message: "Performance: Enums more memory-efficient than sealed classes"
tags:
  - performance
  - allocation
note: |
  Sealed classes allocate per subclass. Enums allocate singletons.
  Use enums for fixed sets (Status, Direction, Color).
  Reference: https://medium.com/@a.poplawski96/donts-in-modern-android-kotlin-development-bad-practices-anti-patterns-chapter-i-d38cba2f5f7d

---
id: kotlin-perf-not-null-safety
language: kotlin
severity: warning
message: "Performance: Use safe call (?.) instead of !!"
tags:
  - performance
  - null-handling
note: |
  !! throws exception on null, crashing the app.
  Use !! only when absolutely certain of non-nullability.
  Safer: nullable?.property or nullable?.property ?: default
  Reference: https://agrawalsuneet.github.io/blogs/safe-calls-vs-null-checks-in-kotlin/

---
id: kotlin-perf-inline-value-classes
language: kotlin
severity: info
message: "Performance: Use inline value classes for type safety"
tags:
  - performance
  - allocation
note: |
  Inline value classes avoid boxing while providing type safety.
  Wrapper erased at compile-time (no allocation).
  Example: value class UserId(val value: Int)
  Reference: https://typealias.com/guides/inline-classes-and-autoboxing/

---
id: kotlin-perf-shared-state
language: kotlin
severity: error
message: "Performance: Avoid mutable shared state between coroutines"
tags:
  - performance
  - concurrency
note: |
  Unsynchronized state causes race conditions. Use: val immutability,
  Atomic types, Synchronized collections, or coroutine channels.
  Immutability is safest and often fastest approach.
  Reference: https://medium.com/@a.poplawski96/donts-in-modern-android-kotlin-development-bad-practices-anti-patterns-chapter-i-d38cba2f5f7d

---
id: kotlin-perf-profile-first
language: kotlin
severity: info
message: "Performance: Profile before optimizing"
tags:
  - performance
  - best-practice
note: |
  Never speculate about bottlenecks. Always profile with realistic data:
  1. Measure baseline 2. Identify hotspots 3. Optimize 4. Re-measure
  Focus on algorithm optimization before micro-optimizations.
  Reference: https://medium.com/@thejufo/enhancing-kotlin-performance-mastering-5-best-practices-4ff69be13e0c

---
id: kotlin-perf-large-data-classes
language: kotlin
severity: info
message: "Performance: Split large data classes"
tags:
  - performance
  - allocation
note: |
  Data classes with 10+ properties add overhead to equals/hashCode/toString.
  Split into smaller classes or override equals() if comparing key fields only.
  Reference: https://www.baeldung.com/kotlin/deep-copy-data-class

---
id: kotlin-perf-early-exit-loops
language: kotlin
severity: info
message: "Performance: Use first(), find() for early loop exit"
tags:
  - performance
  - collections
note: |
  list.first { condition } stops after match.
  list.filter { condition }.first() filters entire list.
  Early exit prevents unnecessary iterations.
  Reference: https://kt.academy/article/ek-sequence

---
id: kotlin-perf-short-circuit
language: kotlin
severity: info
message: "Performance: Use any(), all(), none() for short-circuit"
tags:
  - performance
  - collections
note: |
  list.any { condition } stops at first true.
  list.filter { condition }.isNotEmpty() filters entire list.
  Short-circuit evaluation improves performance on large collections.
  Reference: https://kt.academy/article/ek-sequence

---
id: kotlin-perf-direct-map-building
language: kotlin
severity: info
message: "Performance: Use associateBy() for direct map building"
tags:
  - performance
  - collections
note: |
  list.map { it to it.property }.toMap() creates intermediate Pair list.
  list.associateBy({ it }, { it.property }) builds map directly.
  Direct building avoids intermediate collections.
  Reference: https://kt.academy/article/ek-sequence

---
id: kotlin-perf-delegates-latinit
language: kotlin
severity: warning
message: "Performance: Prefer lateinit over Delegates.notNull()"
tags:
  - performance
  - delegation
note: |
  Delegates.notNull() has higher overhead than lateinit.
  Use lateinit for mutable properties when possible.
  Be aware of performance cost with notNull() delegate.
  Reference: https://medium.com/@ramadan123sayed/understanding-lazy-lateinit-and-delegates-in-kotlin-a-comprehensive-guide-638bbdc4ccd4

---
id: kotlin-perf-extension-zero-cost-abstraction
language: kotlin
severity: info
message: "Performance: Extension functions are zero-cost abstractions"
tags:
  - performance
  - extension-functions
note: |
  Extension functions compile to static methods. No runtime overhead.
  Consider inlining for higher-order extensions with lambda parameters.
  Reference: https://www.baeldung.com/kotlin/inline-functions
# PowerShell Performance Best Practices - Comprehensive Rules Collection
# Generated: 2026-01-03
# Purpose: 50-60 performance rules covering common anti-patterns and optimization techniques
# Based on: Microsoft Learn, Office365itpros.com, PowerShell community best practices

---
id: powershell-perf-array-concatenation-antipattern
language: powershell
severity: warning
message: "Performance: Avoid array concatenation with += in loops; causes O(n²) complexity"
tags:
  - performance
  - arrays
  - anti-pattern
rule:
  kind: foreach_statement
  pattern: |
    foreach ($_ in $collection) {
      $array += $_
    }
note: |
  ANTI-PATTERN: Array += operator creates new array for each addition, copies all elements.

  IMPACT: With 10,240 items: 3,424x slower. With 102,400 items: 18,067x slower.

  FIX 1 - Use List<T>:
  $list = [System.Collections.Generic.List[Object]]::new()
  foreach ($item in $collection) {
    $list.Add($item)
  }

  FIX 2 - Use pipeline array:
  $array = @(
    foreach ($item in $collection) {
      $item
    }
  )

  FIX 3 - Pre-allocate array:
  $array = New-Object object[] $collection.Count
  for ($i = 0; $i -lt $collection.Count; $i++) {
    $array[$i] = $collection[$i]
  }

  NOTE: PowerShell 7.5+ optimized array addition, but recommendation applies to 5.1+.

---
id: powershell-perf-string-concatenation-loop
language: powershell
severity: warning
message: "Performance: Avoid string += concatenation in loops; use -join or StringBuilder"
tags:
  - performance
  - strings
  - anti-pattern
rule:
  kind: foreach_statement
  pattern: |
    foreach ($item in $collection) {
      $string += $value
    }
note: |
  ANTI-PATTERN: String += creates new string, copies content each iteration.

  IMPACT: 100,240 iterations:
  - += operator: 67,640.79 ms
  - StringBuilder: 499.12 ms (135x faster)
  - -join operator: 85.62 ms (790x faster)

  FIX 1 - Use -join (FASTEST):
  $string = @(
    foreach ($item in $collection) {
      "Item: $item"
    }
  ) -join "`n"

  FIX 2 - Use StringBuilder:
  $sb = [System.Text.StringBuilder]::new()
  foreach ($item in $collection) {
    $sb.AppendLine("Item: $item") | Out-Null
  }
  $result = $sb.ToString()

  FIX 3 - Collect values then join:
  $values = foreach ($item in $collection) { "Item: $item" }
  $string = $values -join "`n"

---
id: powershell-perf-output-suppression-methods
language: powershell
severity: info
message: "Performance: Choose output suppression method carefully; Out-Null slowest"
tags:
  - performance
  - output
  - best-practice
rule:
  kind: assignment_statement
  pattern: |
    $_ | Out-Null
note: |
  OUTPUT SUPPRESSION PERFORMANCE RANKING (fastest to slowest):

  FAST (nearly identical):
  1. $null = $object.Method()           # Fastest
  2. $object.Method() > $null           # 1.52x slower
  3. [void] $object.Method()            # 1.71x slower

  SLOW (avoid in tight loops):
  4. $object.Method() | Out-Null        # 2.22x slower - significant overhead

  BEST PRACTICE:
  - Use "$null =" in tight loops
  - Use "> $null" for readability when performance is not critical
  - AVOID "| Out-Null" in performance-sensitive code

  Example - Efficient ArrayList usage:
  $arraylist = [System.Collections.ArrayList]::new()
  for ($i = 0; $i -lt 1000; $i++) {
    $null = $arraylist.Add($i)  # Suppress output efficiently
  }

---
id: powershell-perf-pipeline-output-in-loop
language: powershell
severity: warning
message: "Performance: Move heavy cmdlets outside loops to avoid repeated initialization"
tags:
  - performance
  - pipeline
  - anti-pattern
rule:
  kind: foreach_statement
  pattern: |
    Import-Csv $file | ForEach-Object {
      [pscustomobject]@{...} | Export-Csv $output -Append
    }
note: |
  ANTI-PATTERN: Cmdlet (Export-Csv) runs once per iteration, initializing overhead.

  IMPACT: Export-Csv running 2,100 times:
  - SLOW: 15,968.78 ms
  - FAST: 42.92 ms (372x faster)

  FIX - Move cmdlet outside loop:
  $data = Import-Csv $file | ForEach-Object {
    [pscustomobject]@{
      Id   = $_.id
      Name = $_.name
    }
  }
  $data | Export-Csv $output

  OTHER EXAMPLES:
  - Move Export-Json outside loop
  - Move Export-Clixml outside loop
  - Move ConvertTo-Json outside loop
  - Move Out-String outside loop

  APPLIES TO: Any heavy cmdlet with initialization overhead.

---
id: powershell-perf-where-object-vs-method
language: powershell
severity: warning
message: "Performance: Use .Where() method instead of Where-Object cmdlet for filtering"
tags:
  - performance
  - collections
  - best-practice
rule:
  kind: pipeline_statement
  pattern: |
    $collection | Where-Object { $_.Property -eq 'value' }
note: |
  PERFORMANCE: .Where() method is 12-15% faster than Where-Object.

  WHY: Avoids pipeline overhead, direct collection method.

  FIX:
  $filtered = $collection.Where({ $_.Property -eq 'value' })

  WITH -First OPTIMIZATION (stop after first match):
  # SLOW: Enumerates all 1,048,576 items
  if ($Collection -like '*1*') { 'Found' }  # 633.37 ms

  # FAST: Stops at first match
  if ($Collection.Where({ $_ -like '*1*' }, 'first')) { 'Found' }  # 2.61 ms

  SYNTAX VARIANTS:
  $collection.Where({ $_ -gt 5 })                    # All matches
  $collection.Where({ $_ -gt 5 }, 'first')          # First match only
  $collection.Where({ $_ -gt 5 }, 'last')           # Last match only
  $collection.Where({ $_ -gt 5 }, 'skipuntil')      # Skip until condition

  IMPORTANT: Where-Object returns $null if no match, Where returns [object[]]
  - Adjust comparisons accordingly: if ($result) vs if ($result.Count -gt 0)

---
id: powershell-perf-select-object-calculation
language: powershell
severity: info
message: "Performance: Use calculated properties efficiently; avoid complex expressions"
tags:
  - performance
  - collections
  - best-practice
rule:
  kind: method_call
  pattern: |
    Select-Object @{Name='Property'; Expression={...}}
note: |
  CALCULATED PROPERTIES: Legitimate use, but expensive with complex expressions.

  BEST PRACTICE:
  Evaluate complexity: Is it necessary in Select-Object or can it be pre-computed?

  FAST - Simple property mapping:
  Get-Process | Select-Object -Property Name, @{N='MemoryMB'; E={$_.WorkingSet / 1MB}}

  SLOW - Complex calculations in Select-Object:
  Get-Process | Select-Object -Property Name, @{
    N='ComputedValue'
    E={
      $val = $_.WorkingSet / 1MB
      if ($val -gt 100) { $val * 2 }
      else { $val }
    }
  }

  BETTER - Pre-compute in loop:
  $processes = Get-Process
  foreach ($proc in $processes) {
    $mem = $proc.WorkingSet / 1MB
    $computed = if ($mem -gt 100) { $mem * 2 } else { $mem }

    [pscustomobject]@{
      Name = $proc.Name
      ComputedValue = $computed
    }
  }

  RECOMMENDATION:
  - Use Select-Object for simple property renames/selections
  - Use explicit loops for complex calculations
  - Avoid nested expressions in calculated properties

---
id: powershell-perf-net-method-vs-cmdlet
language: powershell
severity: info
message: "Performance: .NET methods often faster than cmdlets; choose based on performance needs"
tags:
  - performance
  - best-practice
rule:
  kind: comparison
  pattern: "Get-Content vs [System.IO.File]::ReadAllLines"
note: |
  GENERAL PRINCIPLE: .NET methods >> PowerShell cmdlets (usually)

  TRADEOFF: Performance vs. Readability/Consistency
  - Use cmdlets for normal operations (better for output streams, consistent behavior)
  - Use .NET APIs in performance-critical sections

  EXAMPLE - File I/O:

  SLOW - Get-Content with pipeline:
  Get-Content $path | Where-Object Length -GT 10

  FAST - StreamReader (.NET):
  $reader = [System.IO.StreamReader]::new($path)
  try {
    while (-not $reader.EndOfStream) {
      $line = $reader.ReadLine()
      if ($line.Length -gt 10) { $line }
    }
  } finally {
    $reader.Dispose()
  }

  MIDDLE GROUND - File.ReadLines:
  foreach ($line in [System.IO.File]::ReadLines($path)) {
    if ($line.Length -gt 10) { $line }
  }

  OBJECT CREATION PERFORMANCE (100,000 objects):
  | Method | Time | Relative Speed |
  | [pscustomobject] type accelerator | 0.48s | 1x (FASTEST) |
  | .new() static method | 0.59s | 1.2x |
  | New-Object cmdlet | 3.37s | 7x (SLOWEST) |

  RECOMMENDATION:
  - Use [pscustomobject] not New-Object for object creation
  - Use .NET APIs in tight loops (> 1000 iterations)
  - Document performance rationale in comments

---
id: powershell-perf-module-import-overhead
language: powershell
severity: info
message: "Performance: Import all modules at start; avoid scattered imports in logic"
tags:
  - performance
  - modules
  - best-practice
rule:
  kind: function_body
  pattern: |
    function MyFunction {
      Import-Module SomeModule
      # function logic
    }
note: |
  ANTI-PATTERN: Importing modules in function bodies or middle of scripts.

  PERFORMANCE IMPACT:
  - Module loading is expensive (parsing, compilation, manifest processing)
  - Each import initializes module state
  - Scattered imports cause repeated overhead

  BEST PRACTICE:
  Import all dependencies at script start:

  #Requires -Modules Module1, Module2

  # Or explicit imports at top
  Import-Module -Name Module1, Module2 -Force

  function MyFunction {
    # Use module cmdlets
  }

  ADDITIONAL OPTIMIZATION:
  1. Specify module versions to avoid re-scanning:
     Import-Module ActiveDirectory -MinimumVersion 1.0

  2. Load only needed modules:
     # AVOID
     Get-Module -ListAvailable  # Scans all modules

     # PREFER
     Get-Module -Name SpecificModule

  3. Unload unused modules (if not needed for session):
     Remove-Module ModuleName -Force

  4. Consider module prefix to avoid namespace pollution:
     Import-Module ModuleName -Prefix MM
     Get-MMData vs Get-Data

---
id: powershell-perf-compiled-regex-performance
language: powershell
severity: warning
message: "Performance: Use precompiled [regex] for pattern matching in loops"
tags:
  - performance
  - regex
  - best-practice
rule:
  kind: method_call
  pattern: |
    foreach ($item in $collection) {
      if ($item -match $pattern) { }
    }
note: |
  REGEX COMPILATION PERFORMANCE:

  Test: Matching against 1 million items
  | Method | Time |
  | String literal regex with -match | 4.31s |
  | Precompiled regex with -match | 3.70s |
  | Precompiled regex with .Match() | 0.49s (8.8x faster) |

  FIX 1 - Use precompiled regex for -match:
  $pattern = [regex]::new('^test.*', [System.Text.RegularExpressions.RegexOptions]::IgnoreCase)
  foreach ($item in $collection) {
    if ($pattern.IsMatch($item)) {
      # process match
    }
  }

  FIX 2 - Use static [regex]::Match() method:
  $regex = [regex]::new('^test.*')
  foreach ($item in $collection) {
    $match = $regex.Match($item)
    if ($match.Success) {
      $match.Value
    }
  }

  IMPORTANT NOTE - $Matches OVERHEAD:
  The $Matches automatic variable construction is the major slowdown.
  Avoid $Matches in tight loops if not needed.

  Use precompiled regex with explicit caching:
  [regex]::new('^pattern', [RegexOptions]::Compiled)

  BEST FOR: Large datasets, tight loops, performance-critical code
  USE CMDLETS: -match, -replace for readability when performance acceptable

---
id: powershell-perf-runspace-pool-sizing
language: powershell
severity: info
message: "Performance: Properly size runspace pools; too many/few cause performance degradation"
tags:
  - performance
  - parallel
  - best-practice
rule:
  kind: method_call
  pattern: |
    [RunspaceFactory]::CreateRunspacePool(1, 5)
note: |
  RUNSPACE POOL SIZING: Critical decision for parallel performance.

  PRINCIPLE: Balance utilization vs. contention
  - Too few: Underutilizes CPU cores, slow performance
  - Too many: Context switching overhead, contention on shared resources

  RECOMMENDED APPROACH:
  $threadCount = [Environment]::ProcessorCount
  $pool = [RunspaceFactory]::CreateRunspacePool(1, $threadCount)

  POWERSHELL 7.1+ ForEach-Object -Parallel:
  Default runspace pool size: 5
  Override with ThrottleLimit parameter:

  $data | ForEach-Object -Parallel {
    # process $_
  } -ThrottleLimit 10  # Creates 10 runspaces

  OPTIMIZATION STRATEGIES:

  1. Module loading (one-time cost):
     Load modules into runspaces sequentially to avoid contention:

     $iss = [System.Management.Automation.Runspaces.InitialSessionState]::CreateDefault()
     $iss.ImportPSModule(@('Module1', 'Module2'))
     $pool = [RunspaceFactory]::CreateRunspacePool(1, $threadCount, $iss)

  2. Shared variables via session state:
     $iss.Variables.Add([System.Management.Automation.Runspaces.SessionStateVariableEntry]::new('shared', $value, $null))

  3. Memory management:
     Always call Dispose() on runspaces and pools:

     $pool.Close()
     $pool.Dispose()
     foreach ($ps in $powerShellInstances) {
       $ps.Dispose()
     }

  WHEN TO USE PARALLEL:
  ✓ I/O-bound operations (network, file I/O)
  ✓ CPU-intensive operations on multi-core systems
  ✗ Simple data filtering
  ✗ Operations with high serialization overhead

  DANGER: Serialization overhead can negate parallel benefits
  Keep objects passed between runspaces lightweight.

---
id: powershell-perf-foreach-object-parallel
language: powershell
severity: info
message: "Performance: ForEach-Object -Parallel in PS7.1+ reuses runspace pools efficiently"
tags:
  - performance
  - parallel
  - best-practice
rule:
  kind: method_call
  pattern: |
    ForEach-Object -Parallel { }
note: |
  MODERN PARALLEL EXECUTION (PowerShell 7.1+):

  ForEach-Object -Parallel automatically manages runspace pools.

  BASIC USAGE:
  $data | ForEach-Object -Parallel {
    # Process $_ in parallel
    $_
  } -ThrottleLimit 5  # 5 concurrent runspaces

  THREADJOB ALTERNATIVE:
  ThreadJob module provides lighter-weight job management:

  $results = @()
  $data | ForEach-Object {
    $_ | Start-ThreadJob -ScriptBlock { $_ }
    $results += $_
  }

  Wait-Job | Receive-Job -NoRecurse

  VS. START-JOB (SLOWER):
  Start-Job: Separate process, serialization/deserialization
  Start-ThreadJob: Same process, runspace-based, faster
  ForEach-Object -Parallel: Managed runspace pool (most convenient)

  PERFORMANCE NOTES:
  - Serialization/deserialization overhead is significant
  - In many cases, a linear loop is faster for small datasets
  - Parallel shines for I/O-bound or compute-intensive tasks
  - Avoid passing large objects between runspaces

  BEST PRACTICE:
  1. Measure before parallelizing
  2. Keep scriptblock logic simple (avoid closure variables)
  3. Use ThrottleLimit equal to CPU count for CPU-bound
  4. Use ThrottleLimit = 5+ for I/O-bound operations
  5. Monitor memory usage (each runspace consumes ~30-50MB)

---
id: powershell-perf-hash-table-vs-pscustomobject
language: powershell
severity: info
message: "Performance: Hash tables for data manipulation; PSCustomObjects for output"
tags:
  - performance
  - objects
  - best-practice
rule:
  kind: assignment_statement
  pattern: "[pscustomobject]@{...}"
note: |
  USE CASES:

  HASH TABLES @{} - WHEN:
  ✓ Lookup operations (O(1) access)
  ✓ Data aggregation/manipulation
  ✓ Key-value pair storage
  ✓ Configuration storage
  ✓ Parameter splatting (@splat)

  PSCUSTOMOBJECT [pscustomobject]@{} - WHEN:
  ✓ Output objects from functions
  ✓ Need ordered properties
  ✓ Consistent property access (.Property syntax)
  ✓ Pipeline processing
  ✓ Type conversion consistency

  PERFORMANCE EXAMPLE - Collection Lookup:

  SLOW - Repeated filtering (O(n) per lookup):
  $results = foreach ($emp in $employees) {
    $account = $accounts | Where-Object { $_.Name -eq $emp.Name }
    [pscustomobject]@{
      Id = $emp.Id
      Email = $account.Email
    }
  }
  # Time: Minutes for 10k employees * 5k accounts

  FAST - Hash table lookup (O(1)):
  $lookup = @{}
  foreach ($acc in $accounts) {
    $lookup[$acc.Name] = $acc
  }

  $results = foreach ($emp in $employees) {
    [pscustomobject]@{
      Id = $emp.Id
      Email = $lookup[$emp.Name].Email
    }
  }
  # Time: < 1 second

  ORDERED PRESERVATION:
  Use [ordered] to maintain insertion order:

  $obj = [ordered]@{
    Name = 'Value1'
    Path = 'Value2'
  }
  [pscustomobject] $obj

  NOT ORDERED:
  [pscustomobject]@{ ... }  # May vary between PS versions

  ALTERNATIVE - Use [ordered] cast:
  $ordered = @{
    b = 2
    a = 1
  }
  [pscustomobject][ordered]$ordered  # Properties in insertion order

---
id: powershell-perf-add-member-in-loop
language: powershell
severity: warning
message: "Performance: Avoid Add-Member in loops; use [pscustomobject] or hashtable instead"
tags:
  - performance
  - objects
  - anti-pattern
rule:
  kind: foreach_statement
  pattern: |
    foreach ($item in $collection) {
      $obj | Add-Member -MemberType NoteProperty -Name $prop -Value $val
    }
note: |
  ANTI-PATTERN: Add-Member is expensive for bulk object creation.

  PERFORMANCE (100,000 objects with 5 properties):
  | Method | Time | Relative Speed |
  | [ordered] cast | 639.83 ms | 1x (FASTEST) |
  | [pscustomobject] | 1,200 ms | 1.9x |
  | Add-Member | 23,496 ms | 36.7x (SLOWEST) |

  FIX 1 - Use [pscustomobject] directly (RECOMMENDED):
  $results = foreach ($row in $data) {
    [pscustomobject]@{
      Column1 = $row[0]
      Column2 = $row[1]
      Column3 = $row[2]
    }
  }

  FIX 2 - Use [ordered] cast:
  $results = foreach ($row in $data) {
    $obj = [ordered]@{
      Column1 = $row[0]
      Column2 = $row[1]
      Column3 = $row[2]
    }
    [pscustomobject] $obj
  }

  FIX 3 - Use Add-Member OUTSIDE loops only:
  $obj = [pscustomobject]@{ Name = 'Test' }
  $obj | Add-Member -MemberType NoteProperty -Name Dynamic -Value $value

  ONLY WHEN NECESSARY:
  Add-Member is appropriate for:
  - Dynamic property addition after object creation
  - One-off property additions (not in loops)
  - Adding script methods or scriptblock properties

---
id: powershell-perf-splatting-parameters
language: powershell
severity: info
message: "Performance: Use splatting for cleaner, more maintainable parameter passing"
tags:
  - performance
  - parameters
  - best-practice
rule:
  kind: method_call
  pattern: "New-Object -TypeName 'System.Type' -ArgumentList @(...)"
note: |
  SPLATTING: Passing parameters via hash table (or array).

  BENEFITS:
  ✓ Cleaner, more readable code
  ✓ Easier to maintain and modify parameters
  ✓ Reduces syntax errors in long parameter lists
  ✓ Reusable parameter sets
  ✓ Works with $PSBoundParameters for proxy functions

  NAMED SPLATTING (@hash):

  Traditional (verbose):
  Get-ChildItem -Path 'C:\Logs' -Filter '*.log' -Recurse -Force -ErrorAction Stop

  Splatted (clean):
  $params = @{
    Path = 'C:\Logs'
    Filter = '*.log'
    Recurse = $true
    Force = $true
    ErrorAction = 'Stop'
  }
  Get-ChildItem @params

  ARRAY SPLATTING (@array):

  $args = @(
    'C:\file.txt',
    'ascii',
    $true
  )
  [System.IO.File]::ReadAllText($args[0], [System.Text.Encoding]::$args[1])

  USEFUL WITH $PSBoundParameters:

  function MyFunction {
    param(
      [string] $ComputerName,
      [string] $Path,
      [int] $Timeout
    )

    # Forward parameters to another function
    Get-RemoteFile @PSBoundParameters
  }

  OVERRIDE SPLAT PARAMETERS (PowerShell 7.1+):

  $params = @{
    Path = 'C:\temp'
    Filter = '*'
  }
  # Override Filter while keeping other params
  Get-ChildItem @params -Filter '*.txt'  # Explicit param overrides splat

  PERFORMANCE NOTE:
  Splatting doesn't improve performance; it improves readability/maintainability.
  Use for all functions where readability benefits from cleaner parameter passing.

---
id: powershell-perf-foreach-vs-foreach-object
language: powershell
severity: info
message: "Performance: foreach loop faster than ForEach-Object cmdlet; use appropriately"
tags:
  - performance
  - loops
  - best-practice
rule:
  kind: comparison
  pattern: "ForEach-Object vs foreach"
note: |
  PERFORMANCE COMPARISON:

  FOREACH LOOP (faster, no pipeline):
  foreach ($item in $collection) {
    # Process $item
  }

  ForEach-Object CMDLET (slower, pipeline):
  $collection | ForEach-Object { $_ }

  PERFORMANCE:
  - foreach: ~1x baseline
  - ForEach-Object: ~1.5-2x slower (pipeline overhead)

  WHEN TO USE FOREACH LOOP:
  ✓ Processing collections
  ✓ Simple iterations
  ✓ Performance-critical code
  ✓ Not piping output

  WHEN TO USE ForEach-Object:
  ✓ In pipeline chains
  ✓ Using $PSItem from pipeline
  ✓ Need cleaner code style
  ✓ Performance not critical
  ✓ -Parallel execution (PS7.1+)

  EXAMPLE PIPELINE CHAIN:
  Get-Process |
    Where-Object WorkingSet -GT 100MB |
    ForEach-Object { $_.Name } |
    Sort-Object

  SAME IN foreach (less readable):
  foreach ($proc in Get-Process) {
    if ($proc.WorkingSet -gt 100MB) {
      $proc.Name
    }
  } | Sort-Object

  RECOMMENDATION:
  - Use foreach for simple loops
  - Use ForEach-Object in pipelines
  - ForEach-Object -Parallel only way to parallelize in PowerShell

---
id: powershell-perf-function-overhead-in-loop
language: powershell
severity: warning
message: "Performance: Move loops inside functions; avoid function calls in loops"
tags:
  - performance
  - functions
  - anti-pattern
rule:
  kind: foreach_statement
  pattern: |
    for ($i = 0; $i -lt $count; $i++) {
      $result = MyFunction
    }
note: |
  ANTI-PATTERN: Function call overhead multiplies with iterations.

  PERFORMANCE (100,000 iterations):
  | Approach | Time | Relative Speed |
  | Function called 100k times | 6.45x slower |
  | Loop inside function | 1x baseline |

  SLOW - Function called in loop:
  for ($i = 0; $i -lt 100000; $i++) {
    $null = Get-RandomNumber
  }

  function Get-RandomNumber {
    $RNG.Next()
  }

  FAST - Loop inside function:
  Get-Random -Count 100000 | foreach { $_ }

  Or create dedicated function:
  Get-RandomNumbers -Count 100000

  function Get-RandomNumbers {
    param($Count)
    for ($i = 0; $i -lt $Count; $i++) {
      $RNG.Next()
    }
  }

  PRINCIPLE:
  Function call overhead is significant (~1-2 microseconds per call).
  With 100k iterations: 100-200ms wasted on function overhead alone.

  OPTIMIZATION RULE:
  Move loop inside function, not function inside loop.

  EXCEPTION:
  When loop body calls external cmdlets (Get-ChildItem, etc.),
  the cmdlet overhead dwarfs function overhead, so trade-off doesn't apply.

---
id: powershell-perf-collection-comparison-in-if
language: powershell
severity: warning
message: "Performance: Use .Where() method for boolean checks on collections"
tags:
  - performance
  - collections
  - best-practice
rule:
  kind: conditional_statement
  pattern: |
    if ($collection -like '*pattern*') { }
note: |
  ANTI-PATTERN: PowerShell enumerates entire collection for comparison.

  PERFORMANCE (1,048,576 items):
  | Method | Time |
  | $collection -like '*1*' | 633.37 ms |
  | $collection.Where({...}, 'first') | 2.61 ms (243x faster) |

  SLOW - Compares all items:
  if ($collection -like '*pattern*') {
    'Found'
  }

  FAST - Stop at first match:
  if ($collection.Where({ $_ -like '*pattern*' }, 'first')) {
    'Found'
  }

  MORE EXAMPLES:

  SLOW:
  if ($list -contains $value) { }
  if ($list -eq $value) { }
  if ($array -match $pattern) { }

  FAST:
  if ($list.Contains($value)) { }
  if ($list.IndexOf($value) -ge 0) { }
  if ($array.Where({ $_ -match $pattern }, 'first')) { }

  OPTIMAL FOR DIFFERENT TYPES:

  Hash table lookup:
  if ($hash.ContainsKey('key')) { }

  Generic List:
  if ($genericList.Contains($item)) { }

  Array:
  if ($array.Where({ $_ -eq $value }, 'first')) { }
  if ($array -contains $value) { # OK if small }

  PRINCIPLE:
  Always use specialized collection methods for existence checks.
  .Where() with 'first' option stops at first match.
  .Contains() method uses hash-based lookup (O(1)).

---
id: powershell-perf-jit-compilation-awareness
language: powershell
severity: info
message: "Performance: Understand JIT compilation; small loops JIT after 16 iterations"
tags:
  - performance
  - optimization
  - educational
rule:
  kind: foreach_statement
  pattern: |
    for ($i = 0; $i -lt $iterations; $i++) {
      # Loop body
    }
note: |
  JIT COMPILATION FACTS:

  1. Loop JIT Compilation Threshold:
     - Loops execute 16 times before JIT compilation triggers
     - PowerShell then compiles to native code in background
     - Native code is 5-10x faster than interpreted

  2. Loop Size Limit:
     - Loops with < 300 instructions are JIT candidates
     - Loops > 300 instructions too expensive to compile
     - Complex/large loops stay interpreted

  3. IMPLICATION:
     For small loops (< 300 instructions):
     - Warmup: First 16 iterations interpreted
     - Then: JIT compilation kicks in
     - Subsequent iterations: Native speed

     For large loops:
     - All iterations interpreted
     - No JIT compilation
     - Performance-critical code should minimize loop complexity

  OPTIMIZATION RULE:
  Keep hot loops (frequently executed) under 300 instructions.
  Refactor complex loop bodies into helper functions.

  EXAMPLE - Complex loop (DO NOT JIT):
  for ($i = 0; $i -lt $count; $i++) {
    # 500 lines of logic
    # This won't JIT; too many instructions
  }

  BETTER:
  for ($i = 0; $i -lt $count; $i++) {
    Process-Item $_  # Extracted to function
  }

  MEASUREMENT TIP:
  Use Measure-Object to see performance differences:

  Measure-Command {
    for ($i = 0; $i -lt 100; $i++) {
      # Small body: JIT after iteration 16
    }
  }

---
id: powershell-perf-new-object-vs-type-accelerator
language: powershell
severity: warning
message: "Performance: Use type accelerators [pscustomobject] not New-Object cmdlet"
tags:
  - performance
  - objects
  - anti-pattern
rule:
  kind: method_call
  pattern: |
    New-Object -TypeName PSCustomObject -Property @{}
note: |
  PERFORMANCE (100,000 objects):
  | Method | Time | Relative Speed |
  | [pscustomobject] type accelerator | 0.48s | 1x (FASTEST) |
  | .new() static method | 0.59s | 1.2x |
  | New-Object cmdlet | 3.37s | 7x (SLOWEST) |

  FIX 1 - Use type accelerator (RECOMMENDED):
  $obj = [pscustomobject]@{
    Name = 'Test'
    Value = 42
  }

  FIX 2 - Use .new() method:
  $obj = [PSCustomObject]::new()
  $obj | Add-Member -NotePropertyName Name -NotePropertyValue 'Test'

  AVOID - New-Object cmdlet:
  $obj = New-Object -TypeName PSCustomObject -Property @{
    Name = 'Test'
  }

  WHY: New-Object has parameter binding and validation overhead.
  Type accelerators are direct instantiation (no overhead).

  APPLIES TO: All type instantiation when performance matters.

  FAST TYPES:
  [pscustomobject], [hashtable], [ordered], [int], [string], etc.

  SLOW METHOD:
  New-Object -TypeName 'System.Collections.Generic.List[int]'

  FAST METHOD:
  [System.Collections.Generic.List[int]]::new()

---
id: powershell-perf-output-stream-suppression
language: powershell
severity: info
message: "Performance: Suppress unnecessary output; avoid pollution of output stream"
tags:
  - performance
  - output
  - best-practice
rule:
  kind: statement
  pattern: |
    Write-Host "message"
    $result = $object.Method()
note: |
  OUTPUT STREAM MANAGEMENT:

  PROBLEM: Unwanted output in data stream causes issues.

  ANTI-PATTERN 1 - Write-Host for status:
  Write-Host "Processing..."
  $data | Export-Csv out.csv
  # Output is mixed; downstream cannot distinguish data from status

  BETTER - Use Write-Verbose/Write-Debug:
  Write-Verbose "Processing..."
  $data | Export-Csv out.csv
  # Status hidden; run with -Verbose to see

  ANTI-PATTERN 2 - Method output not suppressed:
  $object.Method()
  $result = $object.Method()  # Output also captured in $result

  FIX:
  $null = $object.Method()

  STREAM USAGE:
  Write-Output (or implicit): Data stream (default)
  Write-Verbose: Verbose stream (shown with -Verbose)
  Write-Debug: Debug stream (shown with -Debug)
  Write-Warning: Warning stream
  Write-Error: Error stream
  [Console]::WriteLine(): Host only (not transcribable)

  PERFORMANCE ADVANTAGE:
  Proper stream usage allows consumers to:
  - Redirect output to files (> file.txt)
  - Filter output (Where-Object)
  - Suppress verbosity (-Verbose:$false)
  - Capture in transcripts (Start-Transcript)

  AVOID Write-Host EXCEPTION:
  [Console]::WriteLine() is faster than Write-Host,
  but output won't be captured by Start-Transcript.
  Use Write-Host for console-only formatted output only.

---
id: powershell-perf-switch-statement-vs-if-chain
language: powershell
severity: info
message: "Performance: switch faster than if-elseif chain; use for multiple conditions"
tags:
  - performance
  - control-flow
  - best-practice
rule:
  kind: conditional_statement
  pattern: |
    if ($x -eq 1) { }
    elseif ($x -eq 2) { }
    elseif ($x -eq 3) { }
note: |
  PERFORMANCE: switch statement optimized for multiple comparisons.

  SLOW - if-elseif chain (evaluates all conditions sequentially):
  if ($value -eq 'red') { ... }
  elseif ($value -eq 'green') { ... }
  elseif ($value -eq 'blue') { ... }
  elseif ($value -eq 'yellow') { ... }

  FAST - switch statement (hash-based):
  switch ($value) {
    'red' { ... }
    'green' { ... }
    'blue' { ... }
    'yellow' { ... }
  }

  WITH -REGEX:
  switch -Regex ($text) {
    '^test' { 'Matches test prefix' }
    'end$' { 'Matches end suffix' }
    default { 'No match' }
  }

  WITH -WILDCARD:
  switch -Wildcard ($name) {
    'test*' { 'Matches wildcard' }
    '*log' { 'Log file' }
    default { 'Other' }
  }

  PERFORMANCE NOTE:
  For 2-3 conditions: if-elseif is fine
  For 4+ conditions: switch is significantly faster
  Regex: Both similar, use for readability

  IMPORTANT: switch statement falls through!
  Each matching case executes; add break to prevent:

  switch ($value) {
    'red' { 'Is red'; break }  # break prevents fallthrough
    'blue' { 'Is blue'; break }
  }

---
id: powershell-perf-string-interpolation-vs-concatenation
language: powershell
severity: info
message: "Performance: String interpolation vs concatenation; use appropriately"
tags:
  - performance
  - strings
  - best-practice
rule:
  kind: assignment_statement
  pattern: |
    $string = "Value: $var"
    $string = "Value: " + $var
note: |
  STRING CONSTRUCTION METHODS:

  METHOD 1 - String interpolation (PREFERRED):
  $name = 'World'
  $greeting = "Hello, $name!"

  PROS:
  ✓ Readable
  ✓ Evaluates expressions: "Total: $($array.Count)"
  ✓ Handles escaping automatically

  METHOD 2 - String concatenation with +:
  $greeting = "Hello, " + $name + "!"

  CONS: (Remember += is O(n²) in loops)
  ✗ Less readable
  ✗ More verbose
  ✗ No automatic expression evaluation

  METHOD 3 - SubString/Format:
  $greeting = [string]::Format("Hello, {0}!", $name)

  PROS:
  ✓ Culture-specific formatting
  ✓ Explicit type formatting

  CONS:
  ✗ More verbose
  ✗ Less readable for simple cases

  BEST PRACTICE:
  - Use string interpolation for most cases
  - Use -join for building strings in loops (see separate rule)
  - Use StringBuilder for append operations
  - Avoid += in loops

  COMPLEX EXPRESSIONS IN INTERPOLATION:
  $message = "Count: $($array.Count) items"
  $message = "Path: $($PSScriptRoot)\config.json"
  $message = "Value: $(Get-Content $file)"

  PERFORMANCE: Negligible difference in most cases;
  worry about it only in tight loops or large string building.

---
id: powershell-perf-lazy-evaluation-consideration
language: powershell
severity: info
message: "Performance: Understand lazy vs eager evaluation in pipelines"
tags:
  - performance
  - pipelines
  - best-practice
rule:
  kind: pipeline_statement
  pattern: |
    Get-ChildItem | Where-Object { }
note: |
  LAZY EVALUATION (pipelines):

  CONCEPT: Data flows through pipeline one item at a time.

  $files = Get-ChildItem -Recurse -Filter '*.log' |
    Where-Object Length -GT 1MB |
    Select-Object -First 10

  BEHAVIOR:
  - Get-ChildItem yields one file
  - Where-Object filters it
  - Select-Object checks if 10 found
  - Stop enumeration when limit reached

  ADVANTAGE: Early termination with -First

  EAGER EVALUATION (array operations):

  $files = @(Get-ChildItem -Recurse -Filter '*.log') |
    Where-Object Length -GT 1MB
  $largeFiles = $files[0..9]

  BEHAVIOR:
  - Get-ChildItem enumerates ALL files
  - Where-Object filters entire array
  - Select-Object array slice

  DISADVANTAGE: No early termination

  RECOMMENDATION:
  Use Where-Object | Select-Object -First for filtering with limits.
  Array slicing appropriate when you need random access.

  PIPELINES ENABLE:
  ✓ Streaming large datasets
  ✓ Early termination with -First/-Last
  ✓ Memory-efficient processing
  ✓ Responsive UI (results appear as they stream)

  ARRAYS GOOD FOR:
  ✓ Random access ($array[5])
  ✓ Multiple passes
  ✓ Sorting/grouping entire dataset

---
id: powershell-perf-pscustomobject-property-order
language: powershell
severity: info
message: "Performance: Use [ordered] or explicit property order for consistent output"
tags:
  - performance
  - objects
  - best-practice
rule:
  kind: assignment_statement
  pattern: |
    [pscustomobject]@{ b = 2; a = 1 }
note: |
  PROPERTY ORDER IN PSCustomObjects:

  PROBLEM: Hash tables don't guarantee order in older PowerShell.

  SOLUTION 1 - Use [ordered]:
  $obj = [pscustomobject][ordered]@{
    Name = 'Value'
    Path = 'Value2'
    Count = 42
  }
  # Properties in insertion order

  SOLUTION 2 - Cast [ordered] first:
  $props = @{
    Count = 42
    Name = 'Value'
    Path = 'Value2'
  }
  $obj = [pscustomobject][ordered]$props

  SOLUTION 3 - PowerShell 7 preserves insertion order:
  # Automatic in PS7+
  $obj = [pscustomobject]@{
    Name = 'Value'
    Path = 'Value2'
  }

  WHY IT MATTERS:
  - CSV export uses property order (first property = first column)
  - Display formatting depends on order
  - Predictability for downstream processing

  BEST PRACTICE:
  Always use [ordered] in functions/production code:

  function Get-Data {
    [pscustomobject][ordered]@{
      Id = $item.Id
      Name = $item.Name
      Status = $item.Status
    }
  }

---
id: powershell-perf-array-pre-sizing
language: powershell
severity: info
message: "Performance: Pre-size arrays when count is known; avoid dynamic sizing"
tags:
  - performance
  - arrays
  - best-practice
rule:
  kind: assignment_statement
  pattern: |
    $array = @()
    for ($i = 0; $i -lt $count; $i++) {
      $array += $_
    }
note: |
  ARRAY SIZING STRATEGIES:

  STRATEGY 1 - Pre-allocate (FASTEST):
  $count = 1000
  $array = New-Object object[] $count
  for ($i = 0; $i -lt $count; $i++) {
    $array[$i] = Get-Item $_
  }

  ADVANTAGES:
  ✓ Fixed memory allocation
  ✓ No resizing overhead
  ✓ O(1) assignment

  STRATEGY 2 - Use List<T>:
  $list = [System.Collections.Generic.List[Object]]::new()
  foreach ($item in $collection) {
    $list.Add($item)
  }

  STRATEGY 3 - Pipeline (automatic collection):
  $array = @(
    foreach ($item in $collection) {
      $item
    }
  )

  PERFORMANCE (100,000 items):
  | Method | Time |
  | Pre-allocated array | Fast (baseline) |
  | List<T> | ~1.1x baseline |
  | += loop | 3424x slower! |

  WHEN TO USE:
  Pre-allocation: Count known upfront, performance critical
  List<T>: Count unknown, flexibility needed
  Pipeline: Automated by PowerShell, convenient
  += loop: NEVER (except small datasets < 100 items)

  EDGE CASE - Unknown count:
  # Can't use pre-allocation
  $list = [System.Collections.Generic.List[Object]]::new()
  foreach ($item in $source) {
    if ($item.Valid) {
      $list.Add($item)
    }
  }

---
id: powershell-perf-where-method-multiple-criteria
language: powershell
severity: info
message: "Performance: Chain .Where() for multiple filters; faster than ForEach-Object"
tags:
  - performance
  - collections
  - best-practice
rule:
  kind: method_call
  pattern: |
    $collection.Where({ $_ -gt 10 }).Where({ $_ -lt 100 })
note: |
  MULTIPLE FILTERING CHAINS:

  APPROACH 1 - Chained .Where():
  $results = $collection
    .Where({ $_.Type -eq 'Critical' })
    .Where({ $_.Date -gt $cutoff })
    .Where({ $_.Status -eq 'Active' })

  APPROACH 2 - Single .Where() with complex condition:
  $results = $collection.Where({
    $_.Type -eq 'Critical' -and
    $_.Date -gt $cutoff -and
    $_.Status -eq 'Active'
  })

  APPROACH 3 - ForEach-Object (SLOWER):
  $results = $collection |
    Where-Object { $_.Type -eq 'Critical' } |
    Where-Object { $_.Date -gt $cutoff } |
    Where-Object { $_.Status -eq 'Active' }

  PERFORMANCE:
  Chained .Where() and single .Where() similar (fast)
  ForEach-Object chain slower (pipeline overhead per filter)

  READABILITY vs PERFORMANCE:
  - Single complex condition: Hardest to read, fastest
  - Chained single conditions: More readable, slightly slower
  - ForEach-Object chain: Most readable, slowest

  RECOMMENDATION:
  Use single .Where() with compound conditions for performance.
  Use chained .Where() for readability when performance acceptable.
  Avoid ForEach-Object chains; use .Where() instead.

---
id: powershell-perf-groupby-hashset-vs-group-object
language: powershell
severity: info
message: "Performance: For simple grouping, use hashtable; for complex, Group-Object"
tags:
  - performance
  - collections
  - best-practice
rule:
  kind: method_call
  pattern: |
    $grouped = $collection | Group-Object -Property Category
note: |
  GROUPING APPROACHES:

  APPROACH 1 - Manual hashtable (FAST for simple grouping):
  $grouped = @{}
  foreach ($item in $collection) {
    $key = $item.Category
    if (-not $grouped.ContainsKey($key)) {
      $grouped[$key] = [System.Collections.Generic.List[Object]]::new()
    }
    $grouped[$key].Add($item)
  }

  ACCESS:
  $grouped['Category1']  # Get all items with Category1

  APPROACH 2 - Group-Object (SLOWER, more flexible):
  $grouped = $collection | Group-Object -Property Category

  STRUCTURE:
  $grouped | Select-Object -Property Name, Count
  $grouped[0].Group  # Access items

  PERFORMANCE:
  - Hashtable: Faster (O(1) lookup)
  - Group-Object: Creates extra metadata (count, name properties)

  WHEN TO USE:
  Hashtable: Simple grouping, high performance needed
  Group-Object: Complex grouping, reporting output needed

  HASHTABLE WITH MULTIPLE KEYS:
  $grouped = @{}
  foreach ($item in $collection) {
    $key = "$($item.Category)-$($item.Type)"
    if (-not $grouped.ContainsKey($key)) {
      $grouped[$key] = @()
    }
    $grouped[$key] += $item
  }

  RECOMMENDATION:
  Simple grouping: Manual hashtable for performance
  Complex/reporting: Group-Object for convenience
  Output pipeline: Group-Object includes Count, Name properties

---
id: powershell-perf-contains-vs-indexof-lookup
language: powershell
severity: info
message: "Performance: Use appropriate method for collection lookups"
tags:
  - performance
  - collections
  - best-practice
rule:
  kind: method_call
  pattern: |
    $collection.Contains($value)
    $collection -contains $value
note: |
  COLLECTION LOOKUP METHODS:

  HASH TABLE:
  if ($hashtable.ContainsKey('key')) { }       # Best: O(1)
  if ($hashtable['key'] -ne $null) { }        # Also O(1)

  GENERIC LIST:
  if ($list.Contains($item)) { }                # Good: O(n) but optimized
  if ($list.IndexOf($item) -ge 0) { }          # Also O(n)

  ARRAY WITH OPERATOR:
  if ($array -contains $value) { }              # Works: O(n)
  if ($array.Contains($value)) { }              # Same: O(n)

  ARRAY WITH WHERE (FIRST MATCH):
  if ($array.Where({ $_ -eq $value }, 'first')) { }  # Good: O(n) but stops early

  SORTED ARRAY (BINARY SEARCH):
  [System.Collections.Generic.List[int]] $sorted = @(1, 2, 3, 4, 5)
  if ($sorted.BinarySearch($value) -ge 0) { }  # O(log n)

  ORDERED DICTIONARY:
  $od = [System.Collections.Specialized.OrderedDictionary]::new()
  if ($od.Contains('key')) { }                  # O(1)

  PERFORMANCE RANKING:
  | Method | Complexity | When to Use |
  | Hash/Dict lookup | O(1) | Frequent lookups, many items |
  | Binary search | O(log n) | Sorted array, many searches |
  | .Contains() on List | O(n) optimized | Generic list, few items |
  | .Where() first | O(n) | Array, need first match |
  | -contains operator | O(n) | Array, simple usage |

  RECOMMENDATION:
  - Hash table: Default for lookups (10k+ items)
  - List.Contains(): Generic list, linear search acceptable
  - .Where() first: Array with condition, stop early
  - Binary search: Only if array pre-sorted

---
id: powershell-perf-lazy-pipeline-truncation
language: powershell
severity: info
message: "Performance: Use -First with pipelines to stop enumeration early"
tags:
  - performance
  - pipelines
  - best-practice
rule:
  kind: method_call
  pattern: |
    Get-Process | Select-Object -First 10
note: |
  PIPELINE EARLY TERMINATION:

  CONCEPT: -First/-Last with Get-* cmdlets stops enumeration early.

  SLOW - Enumerate all processes, select first 10:
  $procs = Get-Process
  $first = $procs[0..9]

  FAST - Stop after 10 processes:
  $first = Get-Process | Select-Object -First 10

  SAME WITH FILTERING:
  $large = Get-Process |
    Where-Object WorkingSet -GT 100MB |
    Select-Object -First 5
  # Stops after finding 5 matching processes

  ADVANTAGES:
  ✓ Reduced memory usage
  ✓ Faster execution (stops early)
  ✓ Responsive for large datasets

  CMDLET SUPPORT:
  Get-ChildItem -File | Select-Object -First 100
  Get-Process | Select-Object -Last 5
  1..10000000 | Where-Object { $_ -gt 5 } | Select-Object -First 1

  NOT SUPPORTED:
  @(1..1000000) | Select-Object -First 10  # Array already materialized
  $results | Select-Object -First 10        # If $results is already array

  IMPORTANT: Works only with cmdlets that support streaming enumeration.
  Most Get-* cmdlets support this.

  RECOMMENDATION:
  Always use -First/-Last in production when possible.
  Especially important for large datasets (files, processes, registry).

---
id: powershell-perf-psobject-property-access-speed
language: powershell
severity: info
message: "Performance: Direct property access faster than GetMember or dynamic access"
tags:
  - performance
  - objects
  - best-practice
rule:
  kind: member_access
  pattern: |
    $obj.GetType().GetProperty('Name').GetValue($obj)
note: |
  PROPERTY ACCESS PERFORMANCE:

  FASTEST - Direct property access:
  $value = $object.Name

  SLOWER - Reflection:
  $prop = $object.GetType().GetProperty('Name')
  $value = $prop.GetValue($object)

  WITH DYNAMIC PROPERTY NAMES:
  $name = 'PropertyName'

  OPTION 1 - Use -Property parameter:
  Select-Object -Property $name

  OPTION 2 - Use double bracket syntax (often):
  $value = $object[$name]        # If PSCustomObject
  $value = $object.($name)       # If object property

  OPTION 3 - Use GetMember (if necessary):
  $member = $object | Get-Member -Name $name -MemberType Properties
  $value = $member.Value

  PERFORMANCE (1 million accesses):
  Direct access: ~100ms
  Reflection: ~10-50x slower
  Dynamic bracket: ~1.5-2x slower

  BEST PRACTICE:
  - Use direct access in performance loops
  - Use dynamic access only when necessary
  - Cache property info for repeated access

  CACHING EXAMPLE:
  $property = $objects[0].GetType().GetProperty('Name')
  foreach ($obj in $objects) {
    $value = $property.GetValue($obj)  # Reuse property info
  }

---
id: powershell-perf-avoid-explicit-type-conversion
language: powershell
severity: info
message: "Performance: Let PowerShell handle type conversion; explicit conversion slower"
tags:
  - performance
  - types
  - best-practice
rule:
  kind: type_cast
  pattern: |
    [int] $value
    [string] $text
note: |
  TYPE CONVERSION METHODS:

  IMPLICIT (FAST):
  $number = 42
  $text = "The answer is $number"  # Automatic conversion

  EXPLICIT (SLOWER):
  $number = [int] "42"             # Slower
  $text = [string] 42              # Slower

  PERFORMANCE DIFFERENCE:
  Negligible in most cases (microseconds per conversion)
  Matters in loops with millions of conversions

  WHEN EXPLICIT CONVERSION NEEDED:
  ✓ Type constraints in function parameters
  ✓ PSCustomObject property type hints
  ✓ Forcing specific behavior (-as operator)

  CAST VS -as OPERATOR:
  [int] "abc"     # Throws error if conversion fails
  "abc" -as [int] # Returns $null if conversion fails

  PERFORMANCE IN LOOPS:
  SLOW - Explicit conversion every iteration:
  foreach ($item in $items) {
    $num = [int] $item
  }

  FASTER - Use -as or let PowerShell decide:
  foreach ($item in $items) {
    $num = $item + 0  # Implicit conversion
  }

  RECOMMENDATION:
  - Let PowerShell handle implicit conversion
  - Use explicit cast only when type safety needed
  - In performance-critical code, pre-convert before loop

---
id: powershell-perf-avoid-format-string-operator
language: powershell
severity: info
message: "Performance: Use string interpolation, not -f operator in tight loops"
tags:
  - performance
  - strings
  - best-practice
rule:
  kind: operator
  pattern: |
    'Value: {0}' -f $value
note: |
  STRING FORMATTING METHODS:

  METHOD 1 - String interpolation (PREFERRED):
  "Value: $value"
  "Count: $($array.Count)"

  METHOD 2 - -f operator:
  'Value: {0}' -f $value
  'X:{0}, Y:{1}' -f $x, $y

  METHOD 3 - [string]::Format:
  [string]::Format('Value: {0}', $value)

  PERFORMANCE:
  - Interpolation: Fastest
  - -f operator: Slower (method call)
  - [string]::Format: Slowest (explicit type call)

  WHEN TO USE:
  Interpolation: Most cases (simpler, faster)
  -f operator: Culture-specific formatting, readability preference
  [string]::Format: Complex formatting, explicit type requirements

  CULTURE-AWARE FORMATTING:
  $format = 'Total: {0:C}'  # Currency formatting
  $format -f 1000.50

  RECOMMENDATION:
  - Use interpolation by default
  - Use -f only when formatting features needed
  - Avoid in tight loops if performance critical

---
id: powershell-perf-method-call-signature
language: powershell
severity: info
message: "Performance: Choose method signature carefully; parameterized versions faster"
tags:
  - performance
  - methods
  - best-practice
rule:
  kind: method_call
  pattern: |
    [System.IO.File]::ReadAllText($path)
note: |
  METHOD SIGNATURE SELECTION:

  EXAMPLE - Different overloads of same method:

  Read all text:
  [System.IO.File]::ReadAllText($path)                    # Default encoding
  [System.IO.File]::ReadAllText($path, $encoding)         # Specific encoding
  [System.IO.File]::ReadAllLines($path)                   # Returns array

  STREAM OPERATIONS:
  $reader = [System.IO.StreamReader]::new($path)
  $line = $reader.ReadLine()
  $reader.Dispose()

  PERFORMANCE CONSIDERATIONS:
  - Direct methods: Faster (fewer parameters to resolve)
  - With encoding: May be slower but correct for non-UTF-8
  - Stream-based: Better for large files (streaming)
  - Array-based: Entire file in memory

  RECOMMENDATION:
  Choose based on:
  1. Performance need (streaming vs. all-at-once)
  2. Memory constraints (large vs. small files)
  3. Encoding requirements (ASCII, UTF-16, etc.)

  For loops with many small method calls:
  Cache method references or use simpler overloads without parameters.

---
id: powershell-perf-avoid-string-operations-in-regex
language: powershell
severity: warning
message: "Performance: Pre-compile regex for repeated pattern matching"
tags:
  - performance
  - regex
  - best-practice
rule:
  kind: operator
  pattern: |
    if ($string -match $pattern) { }
note: |
  REGEX COMPILATION:

  ANTI-PATTERN - Recompile regex each iteration:
  foreach ($item in $largeCollection) {
    if ($item -match '^ERROR:')  { ... }  # Regex recompiled each time
  }

  FIX - Pre-compile regex once:
  $regex = [regex]::new('^ERROR:', [System.Text.RegularExpressions.RegexOptions]::Compiled)
  foreach ($item in $largeCollection) {
    if ($regex.IsMatch($item)) { ... }
  }

  PERFORMANCE IMPROVEMENT:
  First pass: Compilation overhead
  Subsequent passes: Direct pattern matching (no recompilation)

  WITH [RegexOptions]:
  [System.Text.RegularExpressions.RegexOptions]::Compiled  # Compile to IL
  [System.Text.RegularExpressions.RegexOptions]::IgnoreCase # Case insensitive
  [System.Text.RegularExpressions.RegexOptions]::Multiline  # ^ $ match lines
  [System.Text.RegularExpressions.RegexOptions]::Singleline # . matches newline

  EXPLICIT MATCH:
  $match = $regex.Match($text)
  if ($match.Success) {
    $match.Value
    $match.Groups[1].Value
  }

  RECOMMENDATION:
  - Pre-compile in performance-critical sections
  - -match operator acceptable for one-off patterns
  - Pre-compiled regex for loops > 100 iterations

---
id: powershell-perf-avoid-recursive-function-depth
language: powershell
severity: warning
message: "Performance: Avoid deep recursion; limit recursion depth or convert to iteration"
tags:
  - performance
  - functions
  - anti-pattern
rule:
  kind: function_definition
  pattern: |
    function Recursive {
      param($input)
      if ($input -gt 0) {
        Recursive ($input - 1)
      }
    }
note: |
  RECURSION PROBLEMS:

  1. Stack overhead: Each call adds to call stack
  2. Stack limit: PowerShell has ~1000 depth limit
  3. Slow function calls: ~1-2 microseconds per call

  DEEP RECURSION (SLOW):
  function Sum {
    param($arr, $index = 0)
    if ($index -ge $arr.Count) { return 0 }
    return $arr[$index] + (Sum $arr ($index + 1))
  }

  ITERATION (FAST):
  function Sum {
    param($arr)
    $total = 0
    foreach ($item in $arr) {
      $total += $item
    }
    return $total
  }

  WHEN RECURSION NECESSARY:
  ✓ Tree walking (directory traversal)
  ✓ Parsing nested structures
  ✓ Divide-and-conquer algorithms

  OPTIMIZATION:
  1. Add max depth checks
  2. Consider tail-call optimization (if algorithm allows)
  3. Use iteration when possible

  EXAMPLE - File system recursion (appropriate):
  function Get-FilesRecursive {
    param($path)
    Get-ChildItem -Path $path -File
    foreach ($dir in Get-ChildItem -Path $path -Directory) {
      Get-FilesRecursive -Path $dir.FullName
    }
  }

  DEPTH LIMIT EXAMPLE:
  if ($depth -gt 100) {
    throw 'Recursion depth exceeded'
  }

---
id: powershell-perf-avoid-duplicate-variable-assignment
language: powershell
severity: info
message: "Performance: Avoid redundant variable assignments; reuse values"
tags:
  - performance
  - variables
  - best-practice
rule:
  kind: assignment_statement
  pattern: |
    $value1 = $object.Property
    $value2 = $object.Property
note: |
  VARIABLE ASSIGNMENT EFFICIENCY:

  REDUNDANT ACCESS (AVOID):
  foreach ($item in $collection) {
    $name = $item.Name
    $path = $item.Path
    $name = $item.Name  # Redundant!
    $path = $item.Path  # Redundant!
  }

  OPTIMIZED:
  foreach ($item in $collection) {
    $name = $item.Name
    $path = $item.Path
    # Reuse $name and $path
  }

  CACHING PATTERN:
  # Cache expensive operations
  $count = $collection.Count  # Call once
  for ($i = 0; $i -lt $count; $i++) {
    # Use $count in loop
  }

  BEST PRACTICE:
  - Assign once, reuse many times
  - Cache property lookups in loops
  - Use local variables instead of re-evaluating expressions

  EXAMPLE - Optimized loop:
  $parentPath = $item.Parent.FullName  # Cache once
  foreach ($file in $files) {
    $fullPath = "$parentPath\$($file.Name)"
  }

  VS UNOPTIMIZED:
  foreach ($file in $files) {
    $fullPath = "$($item.Parent.FullName)\$($file.Name)"  # Evaluated each iteration
  }

---
id: powershell-perf-compression-before-serialization
language: powershell
severity: info
message: "Performance: Compress data before serialization for network/storage transfer"
tags:
  - performance
  - serialization
  - best-practice
rule:
  kind: method_call
  pattern: |
    ConvertTo-Json | [System.Text.Encoding]::UTF8.GetBytes
note: |
  SERIALIZATION FOR TRANSFER:

  WORKFLOW:
  1. Serialize (ConvertTo-Json, Export-Clixml, etc.)
  2. Compress (GZip, BZip2, Deflate)
  3. Encrypt (optional)
  4. Transfer (network/storage)
  5. Decompress
  6. Deserialize

  EXAMPLE - JSON with compression:

  SERIALIZE + COMPRESS:
  $data = Get-Process | Select-Object Name, Id, WorkingSet
  $json = ConvertTo-Json $data
  $bytes = [System.Text.Encoding]::UTF8.GetBytes($json)

  # Compress
  $ms = [System.IO.MemoryStream]::new()
  $gz = [System.IO.Compression.GZipStream]::new($ms, [System.IO.Compression.CompressionMode]::Compress)
  $gz.Write($bytes, 0, $bytes.Length)
  $gz.Close()

  # Transfer
  $compressed = $ms.ToArray()

  DECOMPRESS + DESERIALIZE:
  $ms = [System.IO.MemoryStream]::new($compressed)
  $gz = [System.IO.Compression.GZipStream]::new($ms, [System.IO.Compression.CompressionMode]::Decompress)
  $sr = [System.IO.StreamReader]::new($gz)
  $json = $sr.ReadToEnd()
  $data = ConvertFrom-Json $json

  COMPRESSION RATIOS:
  - JSON: 50-70% reduction typical
  - XML: 60-80% reduction typical
  - Text: Variable (depends on content)

  USE COMPRESSION WHEN:
  ✓ Network transfer (bandwidth limited)
  ✓ Storage (disk space limited)
  ✓ Large datasets (> 10MB)

  SKIP COMPRESSION WHEN:
  ✗ Local operations (no transfer)
  ✗ Already compressed format (zip, gz, etc.)
  ✗ Small datasets (< 1MB)

---
id: powershell-perf-enum-casting-vs-validation
language: powershell
severity: info
message: "Performance: Validate enum membership before parsing; avoid exceptions"
tags:
  - performance
  - types
  - best-practice
rule:
  kind: method_call
  pattern: |
    [System.Enum]::Parse([type], $value)
note: |
  ENUM PARSING:

  APPROACH 1 - Direct parsing (risky):
  [System.IO.FileMode]::Open = 3
  try {
    $mode = [System.IO.FileMode] "InvalidValue"  # Throws
  } catch {
    $mode = [System.IO.FileMode]::Open
  }

  APPROACH 2 - Try-Parse (safer, faster):
  $value = "Open"
  if ([System.Enum]::TryParse([System.IO.FileMode], $value, [ref]$result)) {
    # $result now contains parsed enum
  } else {
    $result = [System.IO.FileMode]::Open
  }

  APPROACH 3 - Validate against values:
  $valid = [System.Enum]::GetValues([System.IO.FileMode])
  if ($value -in $valid) {
    $mode = [System.IO.FileMode] $value
  }

  PERFORMANCE NOTE:
  - TryParse: Efficient (no exception thrown)
  - Direct parse: Slow if invalid (exception overhead)

  BEST PRACTICE:
  Use TryParse in loops or performance-critical code:

  foreach ($item in $items) {
    if ([System.Enum]::TryParse([MyEnum], $item, [ref]$parsed)) {
      Process $parsed
    }
  }

---
id: powershell-perf-avoid-write-host-in-production
language: powershell
severity: warning
message: "Performance: Avoid Write-Host in production functions; use output streams"
tags:
  - performance
  - output
  - best-practice
rule:
  kind: method_call
  pattern: |
    Write-Host "message"
note: |
  WRITE-HOST PERFORMANCE ISSUE:

  PROBLEM:
  - Write-Host can be 10-100x slower than other output methods
  - Blocks output until console updates
  - Cannot be redirected to files (bypasses piping)
  - Pollutes output stream (mixes data with formatting)

  ANTI-PATTERN:
  function Get-Data {
    Write-Host "Processing..."
    $data = @(...)
    Write-Host "Done"
    return $data
  }

  ISSUES:
  1. Status messages mixed with data output
  2. Slow in loops
  3. Cannot suppress with -Verbose
  4. Not captured by Start-Transcript

  FIX - Use Write-Verbose:
  function Get-Data {
    Write-Verbose "Processing..."
    $data = @(...)
    Write-Verbose "Done"
    return $data
  }

  # Run with -Verbose to see messages
  Get-Data -Verbose

  PROPER STREAM USAGE:
  Write-Output      # Data (default output stream)
  Write-Host        # Formatted console output ONLY
  Write-Verbose     # Verbose information (-Verbose)
  Write-Debug       # Debug information (-Debug)
  Write-Warning     # Warnings
  Write-Error       # Errors
  [Console]::WriteLine()  # Host only (fast, non-transcriptable)

  CONSOLE ALTERNATIVE (FAST):
  [Console]::WriteLine("message")  # Faster than Write-Host
  # Note: Not captured by Start-Transcript

  BENCHMARK:
  1 million iterations:
  - Write-Host: 50+ seconds
  - Write-Verbose: 0.1 seconds (with -Verbose flag off)
  - [Console]::WriteLine: 5 seconds

  RECOMMENDATION:
  - NEVER use Write-Host for status/progress in functions
  - Use Write-Verbose for debug messages
  - Use Write-Host only for formatted console-only output
  - Use [Console]::WriteLine for performance-critical loops if console output needed

---
id: powershell-perf-avoid-expansion-in-switch
language: powershell
severity: info
message: "Performance: Be aware of expression expansion in switch statements"
tags:
  - performance
  - control-flow
  - best-practice
rule:
  kind: switch_statement
  pattern: |
    switch -Regex ($value) {
      '.*pattern.*' { }
    }
note: |
  SWITCH STATEMENT VARIATIONS:

  SIMPLE COMPARISON (FAST):
  switch ($value) {
    'value1' { ... }
    'value2' { ... }
  }

  REGEX MATCHING:
  switch -Regex ($value) {
    '^test' { ... }
    'end$' { ... }
  }

  WILDCARD MATCHING:
  switch -Wildcard ($value) {
    'test*' { ... }
    '*log' { ... }
  }

  PERFORMANCE:
  Simple comparison: O(1) hash lookup
  Regex: Compiles each pattern per iteration
  Wildcard: Also compiles per iteration

  OPTIMIZATION:
  For repeated regex checks, pre-compile:

  $errorPattern = [regex]::new('^ERROR:', [System.Text.RegularExpressions.RegexOptions]::Compiled)
  foreach ($line in $lines) {
    if ($errorPattern.IsMatch($line)) { ... }
  }

  Rather than:
  switch -Regex $line {
    '^ERROR:' { ... }  # Recompiles each iteration
  }

  BEST PRACTICE:
  - Use simple switch for fixed values
  - Use if-elseif for complex conditions
  - Pre-compile regex for repeated patterns
  - Avoid complex expressions in switch cases

---
id: powershell-perf-minimize-object-allocation
language: powershell
severity: info
message: "Performance: Minimize object allocations in tight loops"
tags:
  - performance
  - memory
  - best-practice
rule:
  kind: foreach_statement
  pattern: |
    for ($i = 0; $i -lt 100000; $i++) {
      $obj = [pscustomobject]@{ ... }
    }
note: |
  OBJECT ALLOCATION OVERHEAD:

  PROBLEM: Creating objects in loops is expensive.

  SLOW - Create object each iteration:
  $results = @()
  for ($i = 0; $i -lt 100000; $i++) {
    $obj = [pscustomobject]@{
      Index = $i
      Value = $i * 2
    }
    $results += $obj
  }

  FAST - Collect values, create objects once:
  $results = @(
    for ($i = 0; $i -lt 100000; $i++) {
      # Return values
      [pscustomobject]@{
        Index = $i
        Value = $i * 2
      }
    }
  )

  OR - Use iterator in pipeline:
  $results = 0..99999 | ForEach-Object {
    [pscustomobject]@{
      Index = $_
      Value = $_ * 2
    }
  }

  MEMORY IMPACT:
  - Each [pscustomobject]: ~100-500 bytes
  - 100,000 objects: 10-50MB memory
  - Garbage collection overhead

  OPTIMIZATION STRATEGIES:
  1. Pre-allocate array if count known
  2. Use pipeline (automatic collection)
  3. Reuse objects if structure permits
  4. Collect then create (collect then @() wrap)

  REUSE PATTERN (if modifying single object):
  $template = [pscustomobject]@{ Name = ''; Value = 0 }
  $results = @()
  foreach ($item in $source) {
    $template.Name = $item.Name
    $template.Value = $item.Value
    # Warning: Store reference or Clone
    $results += $template.PSObject.Copy()  # Clone to avoid reference issues
  }

---
id: powershell-perf-datetime-operations
language: powershell
severity: info
message: "Performance: Cache DateTime conversions; avoid repeated parsing"
tags:
  - performance
  - types
  - best-practice
rule:
  kind: method_call
  pattern: |
    [DateTime]::Parse($string)
note: |
  DATETIME PARSING PERFORMANCE:

  SLOW - Parse in loop:
  foreach ($item in $items) {
    $date = [DateTime]::Parse($item.DateString)
    if ($date -gt $cutoff) { ... }
  }

  FAST - Parse once:
  $cutoff = [DateTime]::Parse('2024-01-01')
  foreach ($item in $items) {
    $date = [DateTime]::Parse($item.DateString)
    if ($date -gt $cutoff) { ... }
  }

  FASTER - Cache format string:
  $format = 'yyyy-MM-dd'
  $cutoff = [DateTime]::ParseExact('2024-01-01', $format, $null)
  foreach ($item in $items) {
    if ([DateTime]::TryParseExact($item.DateString, $format, $null, [System.Globalization.DateTimeStyles]::None, [ref]$date)) {
      if ($date -gt $cutoff) { ... }
    }
  }

  BEST PRACTICE:
  - Cache cutoff/reference dates
  - Use TryParseExact to avoid exceptions
  - Specify format string (faster than parsing format)

  PERFORMANCE COMPARISON:
  Parse (flexible): Slower, handles multiple formats
  ParseExact (format specified): Faster, requires exact format

  RECOMMENDATION:
  Use ParseExact in performance loops with known format.

---
id: powershell-perf-json-serialization-size
language: powershell
severity: info
message: "Performance: ConvertTo-Json can produce large output; consider alternatives"
tags:
  - performance
  - serialization
  - best-practice
rule:
  kind: method_call
  pattern: |
    ConvertTo-Json $object
note: |
  JSON SERIALIZATION SIZE:

  ISSUE: ConvertTo-Json produces verbose output.

  EXAMPLE:
  $obj = [pscustomobject]@{
    Name = 'Test'
    Value = 42
    Items = @(1, 2, 3)
  }

  JSON output:
  {
    "Name": "Test",
    "Value": 42,
    "Items": [1, 2, 3]
  }
  # Relatively compact

  WITH NESTED OBJECTS:
  Size grows with nesting depth and number of properties.

  ALTERNATIVES:

  1. CSV (tabular data):
  $data | Export-Csv -NoTypeInformation
  # Much more compact for table-like data

  2. Binary (compression):
  $bytes = [System.IO.MemoryStream]::new()
  $compress = [System.IO.Compression.GZipStream]::new($bytes, [System.IO.Compression.CompressionMode]::Compress)
  $json = ConvertTo-Json $object
  $compress.Write([System.Text.Encoding]::UTF8.GetBytes($json), 0, $json.Length)
  $compressed = $bytes.ToArray()

  3. MessagePack (third-party, compact):
  # Requires external library, but very compact

  PERFORMANCE TRADE-OFFS:
  - JSON: Human readable, larger size, slower to parse
  - CSV: Very compact for tables, loses type info
  - Binary: Smallest, fastest, less portable

  RECOMMENDATION:
  - JSON: Default for APIs, configuration, structured data
  - CSV: Tables/reports, data export
  - Binary: Internal transfer, performance critical
  - Compress: Always for network transfer of large objects

---
id: powershell-perf-avoid-nested-loops-with-operators
language: powershell
severity: warning
message: "Performance: Nested loops with -contains worse than hash lookup"
tags:
  - performance
  - loops
  - anti-pattern
rule:
  kind: foreach_statement
  pattern: |
    foreach ($item in $collection1) {
      if ($collection2 -contains $item) { }
    }
note: |
  NESTED LOOP PERFORMANCE:

  ANTI-PATTERN - O(n²) or O(n*m):
  $common = @()
  foreach ($item in $set1) {
    if ($set2 -contains $item) {  # O(n) search each iteration
      $common += $item
    }
  }
  # Total: O(n*m) where n=#items, m=$set2.Count

  EXAMPLE: 1000 items × 5000 items = 5 million comparisons!

  FIX 1 - Use hash table:
  $hash = @{}
  foreach ($item in $set2) {
    $hash[$item] = $true
  }

  $common = @()
  foreach ($item in $set1) {
    if ($hash.ContainsKey($item)) {  # O(1) lookup
      $common += $item
    }
  }
  # Total: O(n+m) - linear!

  FIX 2 - Use .Where() method:
  $set2Hash = @{}
  foreach ($item in $set2) {
    $set2Hash[$item] = $true
  }

  $common = $set1.Where({ $set2Hash.ContainsKey($_) })

  PERFORMANCE:
  Nested -contains: 5M comparisons, ~500ms+
  Hash lookup: ~50ms (10x faster)

  APPLIES TO:
  ✗ if ($array -contains $value)  # O(n) each time
  ✓ if ($hash.ContainsKey($value))  # O(1)
  ✓ if ($array.Contains($value))  # O(n) but faster

  RULE: Always convert to hash for repeated lookups.

---
id: powershell-perf-pipeline-cmdlet-chaining
language: powershell
severity: info
message: "Performance: Balance pipeline chaining with loop efficiency"
tags:
  - performance
  - pipelines
  - best-practice
rule:
  kind: pipeline_statement
  pattern: |
    Get-ChildItem | Where-Object | ForEach-Object | Select-Object
note: |
  PIPELINE CHAINING TRADE-OFFS:

  HIGHLY CHAINED (readable but slower):
  Get-ChildItem -Recurse |
    Where-Object { $_.Extension -eq '.log' } |
    Where-Object { $_.Length -gt 1MB } |
    ForEach-Object { $_.Directory.Name + ': ' + $_.Name } |
    Sort-Object |
    Get-Unique

  FEWER CHAINS (faster but less readable):
  $files = Get-ChildItem -Recurse -Filter '*.log'
  foreach ($file in $files) {
    if ($file.Length -gt 1MB) {
      $file.Directory.Name + ': ' + $file.Name
    }
  } | Sort-Object | Get-Unique

  PERFORMANCE IMPACT:
  - Each pipe operator adds overhead
  - 10+ pipes: Noticeable slowdown
  - Cmdlet initialization per pipe
  - Memory: Each stage buffers data

  OPTIMIZATION STRATEGIES:
  1. Filter early (move Where-Object first)
  2. Combine filters (single Where-Object)
  3. Use -Filter on Get-ChildItem
  4. Move expensive operations outside pipeline

  EXAMPLE - Optimized:
  Get-ChildItem -Recurse -Filter '*.log' |
    Where-Object { $_.Length -gt 1MB } |
    ForEach-Object { $_.Directory.Name + ': ' + $_.Name } |
    Sort-Object -Unique

  RECOMMENDATION:
  - 3-4 pipes: Fine
  - 5-7 pipes: Acceptable for most operations
  - 8+ pipes: Consider refactoring to loop
  - Profile actual performance; don't optimize prematurely

---
id: powershell-perf-avoid-splat-parameter-copy
language: powershell
severity: info
message: "Performance: Be aware of parameter copying in splatted calls"
tags:
  - performance
  - parameters
  - educational
rule:
  kind: method_call
  pattern: |
    FunctionCall @splatParameters
note: |
  SPLATTING MECHANICS:

  When you splat parameters, PowerShell:
  1. Evaluates each key-value pair
  2. Binds parameters
  3. Calls function

  PERFORMANCE:
  ✓ Cleaner code
  ✓ No performance penalty for normal cases
  ✓ Large hash tables: Negligible difference

  OPTIMIZATION:
  For performance, splatting vs traditional makes little difference.
  Use splatting for readability/maintainability.

  REUSE SPLAT:
  $commonParams = @{
    Force = $true
    ErrorAction = 'Stop'
    Verbose = $true
  }

  Get-Item @commonParams -Path $path
  Remove-Item @commonParams -Path $path
  # Reuse common parameters across calls

  DYNAMIC SPLAT BUILDING:
  $params = @{}
  if ($condition1) { $params['Recurse'] = $true }
  if ($condition2) { $params['Force'] = $true }

  Get-ChildItem @params

  PERFORMANCE TIP:
  Building hash dynamically is negligible cost.
  Clarity of code outweighs micro-optimizations.

  RECOMMENDATION:
  Use splatting freely; it's the recommended pattern.
  Performance impact is negligible vs benefit to readability.

---
id: powershell-perf-cmdlet-filtering-parameter
language: powershell
severity: info
message: "Performance: Use -Filter parameter on cmdlets instead of Where-Object"
tags:
  - performance
  - cmdlets
  - best-practice
rule:
  kind: method_call
  pattern: |
    Get-ChildItem -Path $path | Where-Object { $_.Name -like '*.log' }
note: |
  FILTERING ON CMDLET VS PIPELINE:

  SLOWER - Filter after retrieval:
  Get-ChildItem -Path $path | Where-Object { $_.Name -like '*.log' }

  FASTER - Use -Filter parameter:
  Get-ChildItem -Path $path -Filter '*.log'

  PERFORMANCE:
  - -Filter: Only returns matching items (reduces data flow)
  - Where-Object: Returns all items, filters in pipeline (wasteful)

  IMPACT:
  Directory with 10,000 files, 100 match filter:
  - Where-Object: Retrieves/processes all 10,000 items
  - -Filter: Retrieves/processes only 100 items

  CMDLETS WITH -FILTER:
  Get-ChildItem -Filter '*.log'
  Get-Process -Name 'powershell*'
  Get-Service -Name 'wmi*'
  Get-ADUser -Filter 'Name -like "*smith*"'
  Get-EventLog -LogName Security -InstanceId 4688

  WHEN TO USE:
  ✓ -Filter available on cmdlet: Use it
  ✗ -Filter not available: Use Where-Object
  ✓ Complex filtering: Combine -Filter with Where-Object

  EXAMPLE:
  # GOOD - Filter at source
  Get-ChildItem -Path C:\Logs -Filter '*.log' |
    Where-Object { $_.Length -gt 1MB }

  # NOT AS GOOD
  Get-ChildItem -Path C:\Logs |
    Where-Object { $_.Name -like '*.log' } |
    Where-Object { $_.Length -gt 1MB }

---
id: powershell-perf-avoid-select-object-wildcard
language: powershell
severity: warning
message: "Performance: Avoid Select-Object -ExcludeProperty with many properties"
tags:
  - performance
  - cmdlets
  - anti-pattern
rule:
  kind: method_call
  pattern: |
    Select-Object -ExcludeProperty Prop1, Prop2, Prop3
note: |
  PROPERTY SELECTION PERFORMANCE:

  APPROACH 1 - Include specific properties (FAST):
  $process | Select-Object -Property Name, Id, WorkingSet, Handles

  APPROACH 2 - Exclude unwanted properties (SLOWER):
  $process | Select-Object -ExcludeProperty SystemPageFileUsage, UserPageFileUsage, VirtualMemory

  PERFORMANCE:
  Include: Retrieves only specified properties
  Exclude: Retrieves all properties, removes specified ones

  WHEN INCLUDE IS BETTER:
  ✓ Need only few properties of many
  ✓ Complex objects with 50+ properties
  ✓ Remote objects (PSRemoting) - less data transfer

  WHEN EXCLUDE IS OK:
  ✓ Need all but 2-3 properties
  ✓ Need flexibility (keep new properties added later)
  ✓ Property list known dynamically

  EXAMPLE - Local optimization:
  # SLOW - Excludes 20 properties
  Get-Process |
    Select-Object -ExcludeProperty
      @{ list of 20 unwanted properties }

  # FAST - Includes 5 needed properties
  Get-Process |
    Select-Object -Property Name, Id, WorkingSet, Handles, ProcessName

  REMOTE OPTIMIZATION (big impact):
  # Without -Property: Transfers all properties (100KB+ per process)
  Invoke-Command -ComputerName Server {
    Get-Process | Select-Object -ExcludeProperty *Memory*
  }

  # With -Property: Transfers only needed properties (2KB)
  Invoke-Command -ComputerName Server {
    Get-Process | Select-Object -Property Name, Id, Handles
  }

  RECOMMENDATION:
  - Always use -Property (Include) when possible
  - Use -ExcludeProperty only when necessary
  - Significant impact on remote operations

---
id: powershell-perf-use-get-error-instead-catch
language: powershell
severity: info
message: "Performance: Prefer $Error automatic variable for simple error checking"
tags:
  - performance
  - error-handling
  - best-practice
rule:
  kind: try_catch_statement
  pattern: |
    try { } catch { $err = $_ }
note: |
  ERROR HANDLING PATTERNS:

  PATTERN 1 - Traditional try-catch:
  try {
    Get-Item $path -ErrorAction Stop
  } catch {
    Write-Host "Error: $_"
    $err = $_
  }

  PATTERN 2 - ErrorAction Ignore with $Error:
  Get-Item $path -ErrorAction Ignore
  if ($Error.Count -gt 0) {
    $err = $Error[0]
  }

  PATTERN 3 - Check $? (last command succeeded):
  Get-Item $path
  if (-not $?) {
    Write-Host "Item not found"
  }

  PERFORMANCE:
  try-catch: Overhead of exception handling
  -ErrorAction Ignore: Lightweight, $Error capture
  $? check: Very fast boolean check

  WHEN TO USE:
  try-catch: Expected exceptions, complex error handling
  -ErrorAction Ignore: Checking success of single command
  $?: Quick success/failure check

  USE CASE:
  SLOW - Nested try-catch:
  try {
    try { Get-Item $path1 } catch { ... }
    try { Get-Item $path2 } catch { ... }
  } catch { ... }

  FASTER - ErrorAction with checks:
  Get-Item $path1 -ErrorAction Ignore
  if ($Error[0]) { ... $Error.Clear() }
  Get-Item $path2 -ErrorAction Ignore
  if ($Error[0]) { ... $Error.Clear() }

  RECOMMENDATION:
  - Use try-catch for exception handling logic
  - Use $Error for command success checking
  - Use -ErrorAction SilentlyContinue for simple suppression

---
id: powershell-perf-measure-object-alternative
language: powershell
severity: info
message: "Performance: Measure-Object can be slow; use .Count for simple counting"
tags:
  - performance
  - cmdlets
  - best-practice
rule:
  kind: method_call
  pattern: |
    Get-ChildItem | Measure-Object
note: |
  COUNTING PERFORMANCE:

  METHOD 1 - Measure-Object (SLOWEST):
  $count = (Get-ChildItem | Measure-Object).Count

  METHOD 2 - .Count property:
  $count = @(Get-ChildItem).Count

  METHOD 3 - Collection.Count:
  $files = Get-ChildItem
  $count = $files.Count  # Direct property

  METHOD 4 - @().Count:
  $count = @(Get-ChildItem).Count

  PERFORMANCE (100,000 items):
  Measure-Object: Slower (full object analysis)
  .Count: Much faster (direct property)

  CAUTION - Single item returns 0:
  $single = Get-Item $path
  $single.Count  # Returns $null!

  FIX:
  $count = @($single).Count  # Always wraps in array

  WHEN TO USE:
  Measure-Object: Statistics (Sum, Average, Min, Max)
  .Count: Simple count only
  @().Count: Safe counting (handles single item)

  EXAMPLE:
  SLOW:
  $stats = Get-ChildItem | Measure-Object -Property Length -Sum -Average
  $count = $stats.Count
  $total = $stats.Sum
  $avg = $stats.Average

  FAST (for count only):
  $count = @(Get-ChildItem).Count

  RECOMMENDATION:
  - Use .Count for simple counting
  - Use Measure-Object for statistics (Sum, Average, etc.)
  - Use @() wrapper for safe single-item handling

---
id: powershell-perf-export-format-choice
language: powershell
severity: info
message: "Performance: Choose export format based on use case; CSV fastest for tables"
tags:
  - performance
  - serialization
  - best-practice
rule:
  kind: method_call
  pattern: |
    Export-Clixml $path
    Export-Csv $path
note: |
  EXPORT FORMAT PERFORMANCE:

  CSV (FASTEST for tables):
  $data | Export-Csv -Path output.csv -NoTypeInformation
  - Compact, fast, human-readable
  - Loses type information
  - Ideal for spreadsheets, reports

  JSON (FLEXIBLE):
  $data | ConvertTo-Json | Set-Content output.json
  - Portable, preserves structure
  - Larger than CSV
  - Re-importable with ConvertFrom-Json

  CLIXML (TYPE PRESERVATION):
  $data | Export-Clixml -Path output.xml
  - Preserves PowerShell types
  - Larger file size
  - Full round-trip fidelity
  - Slower than CSV/JSON

  BINARY (FASTEST):
  $bytes = [System.Text.Encoding]::UTF8.GetBytes(
    ConvertTo-Json $data
  )
  [System.IO.File]::WriteAllBytes(output.bin, $bytes)
  - Very fast I/O
  - Requires custom deserialization

  PERFORMANCE COMPARISON (10,000 objects):
  | Format | Size | Write Time | Read Time |
  | CSV | ~200KB | Fast | Fast |
  | JSON | ~300KB | Medium | Medium |
  | CLIXML | ~500KB | Slow | Slow |
  | Binary | ~100KB | Very fast | N/A |

  RECOMMENDATIONS:
  - Tables/reports: Export-Csv (fastest)
  - Structured data: ConvertTo-Json
  - Type fidelity: Export-Clixml
  - Performance critical: Binary format with custom handling

---
id: powershell-perf-limit-scope-variable-lookup
language: powershell
severity: info
message: "Performance: Minimize variable scope depth; use local variables in functions"
tags:
  - performance
  - scoping
  - best-practice
rule:
  kind: variable_assignment
  pattern: |
    $script:variable
    $global:variable
note: |
  VARIABLE SCOPE PERFORMANCE:

  SCOPES (lookup order):
  1. Local (function scope) - FASTEST
  2. Script (script scope)
  3. Global (session scope)
  4. Private (function-private)

  LOOKUP TIME:
  Local variable: ~microseconds
  Script variable: ~10x slower
  Global variable: ~10-20x slower

  ANTI-PATTERN - Global variables:
  $global:counter = 0
  function Increment {
    $global:counter++
  }

  BETTER - Local with return:
  function Increment {
    param([int]$counter)
    return $counter + 1
  }
  $counter = Increment $counter

  BEST - Return from function:
  function Get-Counter {
    return $script:counter
  }

  PERFORMANCE IN LOOPS:
  SLOW - Global lookup 100k times:
  for ($i = 0; $i -lt 100000; $i++) {
    $global:count++
  }

  FAST - Local variable:
  for ($i = 0; $i -lt 100000; $i++) {
    $count++
  }

  WHEN SCOPE NEEDED:
  ✓ Global: Module initialization, configuration cache
  ✗ Global: Function implementation details
  ✓ Script: Shared state within script
  ✗ Script: Frequent access in loops

  BEST PRACTICE:
  - Use local variables in functions
  - Pass values as parameters
  - Return results rather than storing globally
  - Use script scope sparingly
  - Cache global lookups in local variables for loops

---
id: powershell-perf-batch-remote-operations
language: powershell
severity: warning
message: "Performance: Batch remote operations; avoid single-item remoting"
tags:
  - performance
  - remoting
  - anti-pattern
rule:
  kind: method_call
  pattern: |
    foreach ($computer in $computers) {
      Invoke-Command -ComputerName $computer -ScriptBlock { }
    }
note: |
  REMOTE OPERATION BATCHING:

  SLOW - Individual invocations:
  foreach ($computer in $servers) {
    Invoke-Command -ComputerName $computer {
      Get-Service
    }
  }
  # New session/authentication for each computer
  # Serialization overhead per item

  FAST - Batch invocation:
  Invoke-Command -ComputerName $servers {
    Get-Service
  }
  # Single session per computer
  # Bulk data transfer

  FASTER - Array input:
  $servers | ForEach-Object {
    Invoke-Command -ComputerName $_ {
      Get-Service
    }
  } -ThrottleLimit 5  # Parallel execution

  PERFORMANCE IMPACT:
  100 servers, 1000ms connection time each:
  - Sequential: 100,000ms
  - Parallel (ThrottleLimit 10): 10,000ms (10x faster)

  OPTIMIZATION STRATEGIES:

  1. Use -ComputerName with multiple servers:
  Invoke-Command -ComputerName Server1, Server2, Server3 {
    Get-Service
  }

  2. Use -ThrottleLimit for parallelization:
  $servers | ForEach-Object -Parallel {
    Invoke-Command -ComputerName $_ { ... }
  } -ThrottleLimit 20

  3. Use -AsJob for background execution:
  $jobs = Invoke-Command -ComputerName $servers {
    Get-Service
  } -AsJob
  $results = $jobs | Receive-Job

  4. Use PSSessions for multiple commands:
  $session = New-PSSession -ComputerName Server1
  Invoke-Command -Session $session { Get-Service }
  Invoke-Command -Session $session { Get-Process }
  Remove-PSSession $session

  RECOMMENDATION:
  - Batch commands where possible
  - Use -ThrottleLimit for parallel execution
  - Use PSSessions for multiple commands to same server
  - Minimize serialization (filter before return)

---
id: powershell-perf-avoid-calculated-properties-complexity
language: powershell
severity: info
message: "Performance: Keep calculated properties simple; avoid nested expressions"
tags:
  - performance
  - collections
  - best-practice
rule:
  kind: calculated_property
  pattern: |
    Select-Object @{N='Complex'; E={complex nested expression}}
note: |
  CALCULATED PROPERTY COMPLEXITY:

  SIMPLE (FAST):
  Get-Process | Select-Object -Property Name, @{
    N='MemoryMB'
    E={$_.WorkingSet / 1MB}
  }

  COMPLEX (SLOW):
  Get-Process | Select-Object -Property Name, @{
    N='Status'
    E={
      if ($_.WorkingSet -gt 100MB) {
        if ($_.Handles -gt 500) {
          'High Risk'
        } else {
          'Medium'
        }
      } else {
        'Normal'
      }
    }
  }

  PERFORMANCE IMPACT:
  - Simple: Minimal overhead
  - Complex: Significant per-item cost
  - 10,000 processes × 50ms each = 500 seconds!

  OPTIMIZATION:

  FIX 1 - Move to loop:
  $results = foreach ($proc in Get-Process) {
    $mem = $proc.WorkingSet / 1MB
    $status = if ($mem -gt 100MB) {
      if ($proc.Handles -gt 500) { 'High' } else { 'Medium' }
    } else { 'Normal' }

    [pscustomobject]@{
      Name = $proc.Name
      Status = $status
    }
  }

  FIX 2 - Pre-calculate in pipeline:
  Get-Process |
    Where-Object { $_.WorkingSet -gt 100MB } |
    Select-Object -Property Name, @{
      N='MemoryMB'
      E={$_.WorkingSet / 1MB}
    }

  BEST PRACTICE:
  - Keep calculated properties to single operation (simple math)
  - Use complex logic in loop
  - Filter first (Where-Object), then calculate
  - Consider pre-processing before Select-Object

---
id: typescript-security-any-type-abuse
message: 'Quality: Prevent any type abuse for injection and security bypasses'
tags:
- security
severity: error
rule:
  pattern: :\s*any\s*[=;,\)]
note: '|'
language: typescript

---
id: typescript-security-type-assertion-bypass
message: 'Quality: Prevent type assertions (as/angle-bracket) that bypass security
  checks'
tags:
- security
severity: error
rule:
  pattern: (as\s+\w+|<\w+>.*as\w+)
note: '|'
language: typescript

---
id: typescript-security-ts-ignore-hiding
message: 'Quality: Prohibit @ts-ignore without security audit and comment'
tags:
- security
severity: error
rule:
  pattern: '@ts-ignore\s*(?![\s\S]*?SECURITY|[\s\S]*?AUDIT)'
note: '|'
language: typescript

---
id: typescript-security-module-augmentation
message: 'Quality: Validate module augmentation to prevent scope pollution and monkey-patching'
tags:
- security
severity: error
rule:
  pattern: declare\s+module\s+["\']
note: '|'
language: typescript

---
id: typescript-security-unsafe-json-parse
message: 'Quality: Validate JSON.parse results with schema validation'
tags:
- security
severity: error
rule:
  pattern: JSON\.parse\s*\([^)]*\)\s*(?!\.)
note: '|'
language: typescript

---
id: typescript-security-eval-dynamic-code
message: 'Quality: Prohibit eval, Function constructor, and dynamic code execution'
tags:
- security
severity: error
rule:
  pattern: (eval\s*\(|new\s+Function\s*\(|Function\s*\()
note: '|'
language: typescript

---
id: typescript-security-nostore-unsafe-global
message: 'Quality: Prevent global state modification without isolation'
tags:
- security
severity: error
rule:
  pattern: (globalThis\.|global\.|Object\.defineProperty\s*\(\s*Object\.prototype)
note: '|'
language: typescript

---
id: typescript-security-object-freeze-enforcement
message: 'Quality: Use Object.freeze on security-critical immutable data'
tags:
- security
severity: warning
rule:
  pattern: const\s+(\w+)\s*=\s*\{\s*[^}]*(?:password|secret|apiKey|token)[^}]*\}
note: '|'
language: typescript

---
id: typescript-security-sql-injection-template
message: 'Quality: Use parameterized queries, never string interpolation for SQL'
tags:
- security
severity: error
rule:
  pattern: (sql`|query\s*\(.*\$\{|`.*SELECT.*\$\{)
note: '|'
language: typescript

---
id: typescript-security-xss-innerHTML
message: 'Quality: Never use innerHTML with untrusted data'
tags:
- security
severity: error
rule:
  pattern: innerHTML\s*=\s*[^;]*(?!DOMPurify|sanitize)
note: '|'
language: typescript

---
id: typescript-performance-excessive-type-complexity
message: 'Quality: Avoid excessively complex generic/conditional types'
tags:
- performance
severity: warning
rule:
  pattern: (?:extends.*\?.*:.*extends.*\?)
note: '|'
language: typescript

---
id: typescript-performance-const-enum-removal
message: 'Quality: Use const enum for compile-time elimination'
tags:
- performance
severity: info
rule:
  pattern: enum\s+\w+\s*\{
note: '|'
language: typescript

---
id: typescript-performance-namespace-overhead
message: 'Quality: Prefer modules over namespaces for runtime efficiency'
tags:
- performance
severity: info
rule:
  pattern: namespace\s+\w+
note: '|'
language: typescript

---
id: typescript-performance-declaration-merging
message: 'Quality: Avoid declaration merging that increases module size'
tags:
- performance
severity: warning
rule:
  pattern: declare\s+(class|function|interface)\s+\w+|interface\s+\w+\s*\{.*\}\s*interface\s+\w+\s*\{
note: '|'
language: typescript

---
id: typescript-performance-tsconfig-module-resolution
message: 'Quality: Optimize tsconfig moduleResolution and noResolve'
tags:
- performance
severity: info
rule:
  pattern: '"moduleResolution":\s*"(classic|node)"'
note: '|'
language: typescript

---
id: typescript-performance-excessive-overloads
message: 'Quality: Limit function overload count (max 3-5 signatures)'
tags:
- performance
severity: warning
rule:
  pattern: function\s+\w+.*\{[^}]*function\s+\w+.*\{[^}]*function\s+\w+
note: '|'
language: typescript

---
id: typescript-performance-large-union-types
message: 'Quality: Avoid unions with 20+ members; use discriminated unions instead'
tags:
- performance
severity: warning
rule:
  pattern: type\s+\w+\s*=\s*.*\|.*\|.*\|.*\|.*\|.*\|.*\|.*\|.*\|.*\|
note: '|'
language: typescript

---
id: typescript-performance-nocompile-without-incremental
message: 'Quality: Enable incremental compilation for large projects'
tags:
- performance
severity: info
rule:
  pattern: '"incremental":\s*false|"noEmit":\s*true'
note: '|'
language: typescript

---
id: typescript-quality-strict-mode-off
message: 'Quality: Enable strict mode in tsconfig.json'
tags:
- quality
severity: error
rule:
  pattern: '"strict":\s*false|"noImplicitAny":\s*false'
note: '|'
language: typescript

---
id: typescript-quality-implicit-any
message: 'Quality: Eliminate implicit any types; add explicit types'
tags:
- quality
severity: error
rule:
  pattern: (?<!:\s)[,\)]\s*(?!.*:\s)
note: '|'
language: typescript

---
id: typescript-quality-missing-return-types
message: 'Quality: Add explicit return type annotations to all functions'
tags:
- quality
severity: error
rule:
  pattern: (?:function|=>|catch)\s+\w+\s*\([^)]*\)(?!\s*:\s*(?:void|Promise|boolean|string|number|unknown|[\w\.]+))
note: '|'
language: typescript

---
id: typescript-quality-unknown-vs-any
message: 'Quality: Use unknown instead of any; forces type narrowing'
tags:
- quality
severity: error
rule:
  pattern: :\s*any(?!\s*\[|\s*\()
note: '|'
language: typescript

---
id: typescript-quality-interface-vs-type
message: 'Quality: Use interface for object shapes; type for unions/aliases'
tags:
- quality
severity: warning
rule:
  pattern: type\s+\w+\s*=\s*\{[^}]*\}
note: '|'
language: typescript

---
id: typescript-quality-generic-constraints
message: 'Quality: Add constraints to generics; avoid unconstrained generics'
tags:
- quality
severity: warning
rule:
  pattern: <\w+\s*(?![^>]*extends)(?:>|,)
note: '|'
language: typescript

---
id: typescript-quality-utility-types
message: 'Quality: Use utility types (Pick, Omit, Partial, Required, Readonly)'
tags:
- quality
severity: warning
rule:
  pattern: interface\s+\w+\s*\{(?!.*Omit|.*Pick|.*Partial|.*Required)[^}]*\}
note: '|'
language: typescript

---
id: typescript-quality-discriminated-unions
message: 'Quality: Use discriminated unions for safe polymorphism'
tags:
- quality
severity: error
rule:
  pattern: type\s+\w+\s*=\s*.*\|\s*.*(?!type\s*:|kind\s*:|kind:|type:|status\s*:|status:)
note: '|'
language: typescript

---
id: typescript-quality-readonly-properties
message: 'Quality: Mark immutable properties as readonly'
tags:
- quality
severity: warning
rule:
  pattern: interface\s+\w+\s*\{(?!.*readonly)[^}]*\s+\w+\s*:[^}]*\}
note: '|'
language: typescript

---
id: typescript-quality-exhaustiveness-checking
message: 'Quality: Use exhaustiveness checking in switch/if-else chains'
tags:
- quality
severity: error
rule:
  pattern: switch\s*\([^)]*\)\s*\{(?!.*default\s*:|_:\s*)
note: '|'
language: typescript

---
id: typescript-quality-avoid-boolean-params
message: 'Quality: Avoid boolean function parameters; use objects or enums'
tags:
- quality
severity: warning
rule:
  pattern: function\s+\w+\s*\([^)]*:\s*boolean
note: '|'
language: typescript

---
id: typescript-quality-nullish-coalescing
message: 'Quality: Use nullish coalescing (??) instead of || for defaults'
tags:
- quality
severity: warning
rule:
  pattern: '([^|]|[^|]\|) \|\| '
note: '|'
language: typescript

---
id: typescript-quality-optional-chaining
message: 'Quality: Use optional chaining (?.) for safe property access'
tags:
- quality
severity: warning
rule:
  pattern: (?!.*\?\.).*\.[a-zA-Z_]\w*\s*(?:\?|\.)
note: '|'
language: typescript

---
id: typescript-quality-avoid-vague-types
message: 'Quality: Avoid vague types like object, {} (empty object)'
tags:
- quality
severity: warning
rule:
  pattern: :\s*(?:object|{}|Object)(?![<\[])
note: '|'
language: typescript

---
id: typescript-quality-enums-const-or-union
message: 'Quality: Prefer const enum or string union over regular enum'
tags:
- quality
severity: info
rule:
  pattern: enum\s+\w+\s*\{[^}]*=[^}]*\}
note: '|'
language: typescript

---
id: typescript-quality-null-checking-order
message: 'Quality: Check null/undefined before type narrowing'
tags:
- quality
severity: warning
rule:
  pattern: (?!if.*null)if.*typeof
note: '|'
language: typescript

---
id: typescript-quality-class-access-modifiers
message: 'Quality: Use access modifiers (public, private, protected) in classes'
tags:
- quality
severity: warning
rule:
  pattern: class\s+\w+\s*\{(?!.*(?:public|private|protected))[^}]*\s+\w+\s*[=;:]
note: '|'
language: typescript

---
id: typescript-quality-unused-variables
message: 'Quality: Remove or prefix unused variables with underscore'
tags:
- quality
severity: info
rule:
  pattern: (?<!_)\b[a-zA-Z_]\w*\b(?!.*=)\s*[:=]
note: '|'
language: typescript

---
id: php-sec-001
language: php
severity: error
message: SQL Injection - Unparameterized Queries
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'SQL Injection - Unparameterized Queries

  '

---
id: php-sec-002
language: php
severity: error
message: SQL Injection - Concatenated WHERE Clauses
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'SQL Injection - Concatenated WHERE Clauses

  '

---
id: php-sec-003
language: php
severity: error
message: XSS - Unescaped Output in HTML Context
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'XSS - Unescaped Output in HTML Context

  '

---
id: php-sec-004
language: php
severity: error
message: XSS - Unescaped Output in JavaScript Context
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'XSS - Unescaped Output in JavaScript Context

  '

---
id: php-sec-005
language: php
severity: error
message: XSS - Unescaped Output in URL Attribute
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'XSS - Unescaped Output in URL Attribute

  '

---
id: php-sec-006
language: php
severity: error
message: XSS - DOM-based XSS via JavaScript Variable Injection
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'XSS - DOM-based XSS via JavaScript Variable Injection

  '

---
id: php-sec-007
language: php
severity: error
message: Command Injection - Unescaped Shell Commands
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Command Injection - Unescaped Shell Commands

  '

---
id: php-sec-008
language: php
severity: error
message: Command Injection - shell_exec with String Concatenation
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Command Injection - shell_exec with String Concatenation

  '

---
id: php-sec-010
language: php
severity: error
message: Remote File Inclusion (RFI) - External URL Inclusion
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Remote File Inclusion (RFI) - External URL Inclusion

  '

---
id: php-sec-011
language: php
severity: error
message: File Inclusion - Path Traversal via Directory Traversal
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'File Inclusion - Path Traversal via Directory Traversal

  '

---
id: php-sec-013
language: php
severity: error
message: Insecure Deserialization - Magic Methods Abuse
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Insecure Deserialization - Magic Methods Abuse

  '

---
id: php-sec-014
language: php
severity: error
message: Session Fixation - Static Session ID
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Session Fixation - Static Session ID

  '

---
id: php-sec-015
language: php
severity: error
message: Session Security - Missing HttpOnly Flag
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Session Security - Missing HttpOnly Flag

  '

---
id: php-sec-016
language: php
severity: error
message: Session Security - Missing SameSite Attribute
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Session Security - Missing SameSite Attribute

  '

---
id: php-sec-017
language: php
severity: error
message: Weak Hashing - MD5 or SHA1 for Passwords
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Weak Hashing - MD5 or SHA1 for Passwords

  '

---
id: php-sec-018
language: php
severity: error
message: Weak Hashing - Hash Function Without Salt
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Weak Hashing - Hash Function Without Salt

  '

---
id: php-sec-019
language: php
severity: error
message: Weak Encryption - Hardcoded Encryption Keys
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Weak Encryption - Hardcoded Encryption Keys

  '

---
id: php-sec-021
language: php
severity: error
message: Authentication - Hardcoded Credentials
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Authentication - Hardcoded Credentials

  '

---
id: php-sec-022
language: php
severity: error
message: Authentication - Weak Login Check
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Authentication - Weak Login Check

  '

---
id: php-sec-023
language: php
severity: error
message: Authorization - Missing Access Control Checks
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Authorization - Missing Access Control Checks

  '

---
id: php-sec-024
language: php
severity: error
message: Unsafe Functions - eval() Usage
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Unsafe Functions - eval() Usage

  '

---
id: php-sec-025
language: php
severity: error
message: Unsafe Functions - assert() with Dynamic Code
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Unsafe Functions - assert() with Dynamic Code

  '

---
id: php-sec-026
language: php
severity: warning
message: Unsafe Functions - create_function() Deprecated
tags:
- security
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Unsafe Functions - create_function() Deprecated

  '

---
id: php-perf-001
language: php
severity: error
message: String Concatenation in Loops
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'String Concatenation in Loops

  '

---
id: php-perf-003
language: php
severity: warning
message: Repeated str_replace() Calls
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Repeated str_replace() Calls

  '

---
id: php-perf-004
language: php
severity: warning
message: Inefficient Regular Expressions
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Inefficient Regular Expressions

  '

---
id: php-perf-005
language: php
severity: error
message: Inefficient Array Searches
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Inefficient Array Searches

  '

---
id: php-perf-006
language: php
severity: error
message: Multiple array_merge() in Loops
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Multiple array_merge() in Loops

  '

---
id: php-perf-007
language: php
severity: warning
message: Inefficient Array Filtering
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Inefficient Array Filtering

  '

---
id: php-perf-009
language: php
severity: error
message: N+1 Query Problem
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'N+1 Query Problem

  '

---
id: php-perf-010
language: php
severity: warning
message: SELECT * in Queries
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'SELECT * in Queries

  '

---
id: php-perf-012
language: php
severity: error
message: Unbounded Query Results
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Unbounded Query Results

  '

---
id: php-perf-013
language: php
severity: warning
message: Redundant require/include Statements
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Redundant require/include Statements

  '

---
id: php-perf-015
language: php
severity: error
message: OPcache Not Enabled or Configured
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'OPcache Not Enabled or Configured

  '

---
id: php-perf-017
language: php
severity: error
message: Processing Large Files in Memory
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Processing Large Files in Memory

  '

---
id: php-perf-018
language: php
severity: warning
message: Unnecessary Object Creation in Loops
tags:
- performance
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Unnecessary Object Creation in Loops

  '

---
id: php-qual-005
language: php
severity: info
message: Incorrect Naming Convention - Constants
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Incorrect Naming Convention - Constants

  '

---
id: php-qual-006
language: php
severity: error
message: Missing Parameter Type Hints
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Missing Parameter Type Hints

  '

---
id: php-qual-007
language: php
severity: error
message: Missing Return Type Declarations
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Missing Return Type Declarations

  '

---
id: php-qual-008
language: php
severity: warning
message: Missing Property Type Declarations
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Missing Property Type Declarations

  '

---
id: php-qual-009
language: php
severity: warning
message: Using Weak Type Juggling
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Using Weak Type Juggling

  '

---
id: php-qual-010
language: php
severity: error
message: Silent Error Suppression (@)
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Silent Error Suppression (@)

  '

---
id: php-qual-012
language: php
severity: warning
message: Bare Exception Catching
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Bare Exception Catching

  '

---
id: php-qual-013
language: php
severity: warning
message: Uninitialized Variables
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Uninitialized Variables

  '

---
id: php-qual-014
language: php
severity: warning
message: Missing PHPDoc Comments - Functions
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Missing PHPDoc Comments - Functions

  '

---
id: php-qual-015
language: php
severity: info
message: Missing PHPDoc Comments - Classes
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Missing PHPDoc Comments - Classes

  '

---
id: php-qual-016
language: php
severity: info
message: Incorrect PHPDoc Type Annotations
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Incorrect PHPDoc Type Annotations

  '

---
id: php-qual-017
language: php
severity: error
message: Missing Unit Tests
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Missing Unit Tests

  '

---
id: php-qual-019
language: php
severity: warning
message: Test Code Coverage Below Threshold
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Test Code Coverage Below Threshold

  '

---
id: php-qual-020
language: php
severity: warning
message: Cyclomatic Complexity Too High
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Cyclomatic Complexity Too High

  '

---
id: php-qual-021
language: php
severity: warning
message: Function Length Exceeded
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Function Length Exceeded

  '

---
id: php-qual-022
language: php
severity: warning
message: Too Many Parameters
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Too Many Parameters

  '

---
id: php-qual-023
language: php
severity: warning
message: Duplicate Code - Copy-Paste
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Duplicate Code - Copy-Paste

  '

---
id: php-qual-025
language: php
severity: error
message: Global Variables Usage
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Global Variables Usage

  '

---
id: php-qual-026
language: php
severity: info
message: Dead Code - Unused Functions/Variables
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Dead Code - Unused Functions/Variables

  '

---
id: php-qual-027
language: php
severity: warning
message: Large Classes - God Object
tags:
- quality
- php
rule:
  kind: call
  pattern: '[pattern]'
note: 'Large Classes - God Object

  '

---
id: swift-sec-force-unwrap
tags:
- Security
severity: error
message: 'Quality: Avoid force unwrapping optionals'
note: '|'
language: swift

---
id: swift-sec-keychain-hardcoding
tags:
- Security
severity: error
message: 'Quality: Never hardcode keychain passwords or tokens'
note: '|'
language: swift

---
id: swift-sec-insecure-storage
tags:
- Security
severity: error
message: 'Quality: Avoid insecure storage of sensitive data'
note: '|'
language: swift

---
id: swift-sec-url-schemes
tags:
- Security
severity: error
message: 'Quality: Validate custom URL schemes and deep links'
note: '|'
language: swift

---
id: swift-sec-app-transport-security
tags:
- Security
severity: error
message: 'Quality: Enforce App Transport Security (ATS)'
note: '|'
language: swift

---
id: swift-sec-biometric-auth
tags:
- Security
severity: error
message: 'Quality: Validate biometric authentication properly'
note: '|'
language: swift

---
id: swift-sec-ssl-pinning
tags:
- Security
severity: error
message: 'Quality: Implement SSL/TLS certificate pinning for sensitive APIs'
note: '|'
language: swift

---
id: swift-sec-injection-attacks
tags:
- Security
severity: error
message: 'Quality: Prevent injection attacks (SQL, code, command)'
note: '|'
language: swift

---
id: swift-sec-memory-safety
tags:
- Security
severity: error
message: 'Quality: Maintain memory safety with proper pointer usage'
note: '|'
language: swift

---
id: swift-perf-value-vs-reference
tags:
- Performance
severity: warning
message: 'Quality: Choose appropriate value vs reference types'
note: '|'
language: swift

---
id: swift-perf-copy-on-write
tags:
- Performance
severity: warning
message: 'Quality: Leverage copy-on-write semantics for performance'
note: '|'
language: swift

---
id: swift-perf-arc-overhead
tags:
- Performance
severity: warning
message: 'Quality: Minimize Automatic Reference Counting (ARC) overhead'
note: '|'
language: swift

---
id: swift-perf-collection-operations
tags:
- Performance
severity: warning
message: 'Quality: Optimize collection operations for performance'
note: '|'
language: swift

---
id: swift-perf-lazy-properties
tags:
- Performance
severity: info
message: 'Quality: Use lazy properties for expensive initialization'
note: '|'
language: swift

---
id: swift-perf-inline-closures
tags:
- Performance
severity: info
message: 'Quality: Avoid capturing heavy objects in inline closures'
note: '|'
language: swift

---
id: swift-perf-avoid-string-concat
tags:
- Performance
severity: warning
message: 'Quality: Avoid string concatenation in loops'
note: '|'
language: swift

---
id: swift-perf-async-await-deadlock
tags:
- Performance
severity: error
message: 'Quality: Avoid deadlocks with async/await'
note: '|'
language: swift

---
id: swift-qual-naming-conventions
tags:
- Quality
severity: warning
message: 'Quality: Follow Swift naming conventions (PascalCase, camelCase)'
note: '|'
language: swift

---
id: swift-qual-optionals-usage
tags:
- Quality
severity: error
message: 'Quality: Use optionals correctly and safely'
note: '|'
language: swift

---
id: swift-qual-protocol-design
tags:
- Quality
severity: warning
message: 'Quality: Design protocols that are clear and maintainable'
note: '|'
language: swift

---
id: swift-qual-error-handling
tags:
- Quality
severity: error
message: 'Quality: Use proper error handling with typed errors'
note: '|'
language: swift

---
id: swift-qual-access-control
tags:
- Quality
severity: warning
message: 'Quality: Apply appropriate access control modifiers'
note: '|'
language: swift

---
id: swift-qual-documentation
tags:
- Quality
severity: warning
message: 'Quality: Document public APIs with DocC or inline comments'
note: '|'
language: swift

---
id: swift-qual-dependency-injection
tags:
- Quality
severity: warning
message: 'Quality: Use dependency injection to improve testability'
note: '|'
language: swift

---
id: swift-qual-testability
tags:
- Quality
severity: warning
message: 'Quality: Write code that is testable'
note: '|'
language: swift

---
id: swift-qual-type-safety
tags:
- Quality
severity: warning
message: 'Quality: Leverage Swift''s type safety'
note: '|'
language: swift

---
id: swift-qual-avoid-force-unwrap-general
tags:
- Quality
severity: error
message: 'Quality: Eliminate all force unwraps except in tests'
note: '"Comprehensive Swift security, performance, and quality rules"'
language: swift

---
id: objc-sec-001
name: Format String Vulnerability Detection
category: Security
severity: Critical
description: Detect format string vulnerabilities in NSString and NSLog calls
pattern: "(NSLog|NSString|stringWithFormat|stringByAppendingFormat|appendFormat|\n\
  \ initWithFormat|localizedStringWithFormat|stringWithCString|\n stringWithUTF8String).*%[a-zA-Z0-9@$]*\n"
check_type: format_string
vulnerable_patterns:
- NSLog(userInput)
- NSLog([dict description])
- '[NSString stringWithFormat:variable]'
- '[NSString localizedStringWithFormat:userString]'
safe_patterns:
- NSLog(@"%@", variable)
- '[NSString stringWithFormat:@"%@", variable]'
- 'NSLog(@"Static format: %@", value)'
recommendation: 'Always use format strings as literal constants, never from user input
  or variables.

  Pass user data as parameters to format specifiers:

  - NSLog(@"%@", userString) NOT NSLog(userString)

  - [NSString stringWithFormat:@"%@", value] NOT [NSString stringWithFormat:value]

  '
cwe:
- CWE-134
mitigation: Use %@ for objects, %d for integers, %f for floats. Validate all format
  strings.

---
id: objc-sec-002
name: Buffer Overflow Detection
category: Security
severity: Critical
description: Detect potential buffer overflow in C functions and char arrays
pattern: '(strcpy|strcat|sprintf|scanf|gets|fgets|memcpy|memmove|strncpy|strncat).*

  '
check_type: buffer_safety
vulnerable_patterns:
- strcpy(buffer, input)
- char buf[256]; strcpy(buf, userInput)
- sprintf(buffer, format, arg)
- char array[FIXED_SIZE]; memcpy(array, data, size)
- gets(buffer)
safe_patterns:
- strlcpy(buffer, input, sizeof(buffer))
- strncat(buffer, input, remaining)
- snprintf(buffer, sizeof(buffer), format, args)
- memset(buffer, 0, size); memcpy_s(buffer, size, data, min(size, dataLen))
recommendation: 'Use bounds-checked alternatives for string/memory operations:

  - Replace strcpy → strlcpy with explicit buffer size

  - Replace strcat → strlcat with explicit buffer size

  - Replace sprintf → snprintf with buffer size

  - Replace memcpy → use NS* equivalents or bounds checking

  - NEVER use gets() - always use fgets() with size limit

  - Use NSString for string handling when possible

  '
cwe:
- CWE-120
- CWE-119
mitigation: Validate buffer sizes, use safe variants, use NSString/NSData.

---
id: objc-sec-003
name: Keychain Security Issues
category: Security
severity: High
description: Detect insecure keychain usage and sensitive data handling
pattern: "(kSecClass|kSecAttrAccessible|kSecUseDataProtection|\n SecItemAdd|SecItemDelete|SecItemUpdate|SecItemCopyMatching|\n\
  \ SecKeyCreateSignature|SecKeyCreateDecryptedData).*\n"
check_type: keychain_security
vulnerable_patterns:
- 'kSecAttrAccessible: kSecAttrAccessibleAlways'
- 'kSecAttrAccessible: kSecAttrAccessibleWhenUnlocked'
- '[keychain storePassword:pwd user:u]'
- NSString *password = [NSString stringWithContentsOfFile:...]
- char password[256]; memcpy(password, data, len)
safe_patterns:
- 'kSecAttrAccessible: kSecAttrAccessibleWhenUnlockedThisDeviceOnly'
- 'kSecAttrAccessible: kSecAttrAccessibleAfterFirstUnlockThisDeviceOnly'
- 'kSecUseDataProtection: @YES'
- 'kSecAttrSynchronizable: @NO'
- volatile char *sensitiveData; memset(sensitiveData, 0, len)
recommendation: 'Follow Apple Keychain best practices:

  - Use kSecAttrAccessibleWhenUnlockedThisDeviceOnly for maximum security

  - Set kSecAttrSynchronizable: @NO for passwords (disable iCloud Keychain)

  - Use kSecUseDataProtection: @YES for encrypted storage at rest

  - Use volatile for sensitive buffers and zero-fill on release

  - Never log passwords or sensitive data

  - Validate keychain error codes and handle failures gracefully

  - Use SecItemDelete to remove sensitive items when no longer needed

  '
cwe:
- CWE-922
- CWE-316
mitigation: Use ThisDeviceOnly, disable iCloud sync, zero-fill buffers, handle errors.

---
id: objc-sec-004
name: Insecure Data Storage Detection
category: Security
severity: High
description: Detect hardcoded credentials, insecure file storage, plaintext data
pattern: "(@\"password|@\"secret|@\"apikey|@\"token|@\"credential|\n NSUserDefaults|preferences|defaults|NSPropertyListSerialization|\n\
  \ [NSString writeToFile|[NSData writeToFile|NSSearchPathForDirectoriesInDomains|\n\
  \ NSDocumentDirectory|NSCachesDirectory|NSApplicationSupportDirectory).*\n"
check_type: data_storage
vulnerable_patterns:
- NSString *password = @"myPassword123"
- NSString *apiKey = @"sk-xyz123"
- '[defaults setObject:password forKey:@"pwd"]'
- '[NSKeyedArchiver archiveRootObject:userData toFile:path]'
- NSString *filePath = [docs stringByAppendingPathComponent:@"secrets.plist"]
- '[userData writeToFile:path atomically:YES]'
safe_patterns:
- // Credentials from environment or secure configuration
- '[keychain storePassword:pwd forService:@"app" account:user]'
- '[keychain retrievePasswordForService:@"app" account:user]'
- NSDataWritingFileProtectionComplete
- '[data writeToFile:path options:NSDataWritingFileProtectionComplete error:nil]'
recommendation: 'Never store sensitive data in code or NSUserDefaults:

  - Use Keychain for passwords, tokens, and credentials

  - Use Data Protection (NSDataWritingFileProtectionComplete) for files

  - Remove hardcoded secrets and use secure configuration

  - Encrypt sensitive files with CommonCrypto or CryptoKit

  - Use NSFileProtectionComplete for file permissions

  - Never log sensitive data

  - Clear sensitive data from memory (memset, zero-fill)

  '
cwe:
- CWE-798
- CWE-312
- CWE-315
mitigation: Use Keychain, Data Protection, encrypt files, remove hardcoded secrets.

---
id: objc-sec-005
name: SSL/TLS Configuration Issues
category: Security
severity: High
description: Detect weak SSL/TLS configuration and certificate validation
pattern: "(NSURLSessionConfiguration|URLSessionConfiguration|\n serverTrustPolicy|validateServerTrust|allowInvalidCertificates|\n\
  \ kCFStreamSSLValidatesCertificateChain|kCFStreamSSLLevel|\n SFSafariViewController|WKWebView|UIWebView).*\n"
check_type: ssl_tls
vulnerable_patterns:
- config.allowInvalidCertificates = YES
- challenge.sender useCredential:[NSURLCredential credentialForTrust:trust]
- 'kCFStreamSSLValidatesCertificateChain: NO'
- serverTrustPolicy = [AFNoSSLPinningServerTrustPolicy new]
- UIWebView loadRequest:request
safe_patterns:
- NSURLSessionConfiguration.default
- '[challenge.sender performDefaultHandlingForAuthenticationChallenge:challenge]'
- '[challenge.sender rejectProtectionSpaceAndContinueWithChallenge:challenge]'
- WKWebViewConfiguration with certificate pinning
- SFSafariViewController for external URLs
recommendation: 'Implement proper SSL/TLS security:

  - Use NSURLSession with default configuration

  - Implement certificate pinning for sensitive connections

  - Validate certificate chains and hostnames

  - Reject invalid certificates (do NOT set allowInvalidCertificates=YES)

  - Use WKWebView instead of deprecated UIWebView

  - Use SFSafariViewController for external URLs

  - Disable weak SSL versions (SSLv3, TLS 1.0, 1.1)

  - Test with network security configuration (networksecurity.xml on Android)

  '
cwe:
- CWE-295
mitigation: Enable certificate validation, implement pinning, reject invalid certs.

---
id: objc-sec-006
name: Memory Corruption Detection
category: Security
severity: Critical
description: Detect use-after-free, double-free, and memory safety issues
pattern: "(__weak|__strong|__autoreleasing|__unsafe_unretained|\n dealloc|release|autorelease|retain|copy|strong|weak|assign|\n\
  \ unsafe_unretained|NSZombieEnabled|MallocStackLogging).*\n"
check_type: memory_safety
vulnerable_patterns:
- Object *obj = ...; [obj release]; [obj method]
- __unsafe_unretained Object *obj = ...
- assign properties without __weak
- dealloc { [observer removeObserver:self] } // after object freed
- '[_array addObject:obj]; [obj release]'
safe_patterns:
- Object *obj = ... // ARC manages release
- '@property (weak, nonatomic) Object *obj'
- '@property (strong, nonatomic) Object *obj'
- '- (void)dealloc { [[NSNotificationCenter defaultCenter] removeObserver:self] }'
recommendation: 'Follow ARC and memory management best practices:

  - Use ARC - only manually manage in specific legacy code

  - Use __weak for delegates and observers to break retain cycles

  - Use __strong (default) for ownership

  - Implement dealloc to remove observers and clean up resources

  - Test with Zombie Objects and MallocStackLogging

  - Avoid __unsafe_unretained except in performance-critical code

  - Use tools: Instruments (Allocations, Leaks), AddressSanitizer

  '
cwe:
- CWE-416
- CWE-415
mitigation: Use ARC, __weak for delegates, implement dealloc, test with Instruments.

---
id: objc-sec-007
name: Injection Attack Detection
category: Security
severity: High
description: Detect SQL injection, predicate injection, and code injection
pattern: "(NSFetchRequest|predicate|predicateWithFormat|SQLiteDatabase|\n sqlite3_prepare|sqlite3_bind|executeQuery|NSPredicate|\n\
  \ NSCompoundPredicate|stringByAppendingString|stringByAppendingFormat).*\n"
check_type: injection
vulnerable_patterns:
- '[NSPredicate predicateWithFormat:userInput]'
- NSString *query = [NSString stringWithFormat:@"SELECT * FROM users WHERE id = %d",
  userId]
- NSFetchRequest *request = [NSFetchRequest fetchRequestWithEntityName:@"Entity"];
- request.predicate = [NSPredicate predicateWithFormat:@"name = %@", userInput]
- sqlite3_prepare_v2(db, query, -1, &stmt, NULL)
safe_patterns:
- '[NSPredicate predicateWithFormat:@"name = %@", userInput]'
- '[NSPredicate predicateWithFormat:@"age > %d", userAge]'
- sqlite3_prepare_v2(db, @"SELECT * FROM users WHERE id = ?", -1, &stmt, NULL)
- sqlite3_bind_int(stmt, 1, userId)
- request.predicate = [NSPredicate predicateWithFormat:@"status = %@", statusValue]
recommendation: 'Use parameterized queries and predicates:

  - Always use %@ for objects and %d for integers in predicates

  - Use bound parameters in SQL queries (? placeholders)

  - Never concatenate user input directly into predicates or queries

  - Validate and sanitize all user input before use

  - Use SQLite3 prepared statements instead of string formatting

  - Implement input validation for expected types and ranges

  - Log and monitor injection attempts

  '
cwe:
- CWE-89
- CWE-90
mitigation: Use parameterized queries, validate input, use bound parameters.

---
id: objc-sec-008
name: Unvalidated URL Handling
category: Security
severity: High
description: Detect unvalidated URL schemes and deep linking vulnerabilities
pattern: "(NSURL|openURL|canOpenURL|openAppSettings|openExternalURL|\n application:openURL:options|application:handleOpenURL|\n\
  \ URLScheme|universalLinks|handlesURL).*\n"
check_type: url_handling
vulnerable_patterns:
- '[[UIApplication sharedApplication] openURL:url]'
- if ([UIApplication.shared canOpenURL:url]) { [UIApplication.shared openURL:url]
  }
- NSURL *url = [NSURL URLWithString:userInput]
- '- (BOOL)application:(UIApplication *)app openURL:(NSURL *)url ...'
- '[UIApplication.shared openURL:url options:@{} completionHandler:nil]'
safe_patterns:
- NSURL *url = [NSURL URLWithString:@"https://example.com"]
- if (url && [url.scheme isEqualToString:@"https"]) { ... }
- if ([url.host isEqualToString:@"trusted.domain.com"]) { ... }
- '- (BOOL)application:(UIApplication *)app openURL:(NSURL *)url options:(NSDictionary
  *)options'
- '{ if ([self validateURL:url]) { ... } return NO; }'
recommendation: 'Implement secure URL handling:

  - Validate URL scheme (whitelist https, custom schemes)

  - Validate URL host against trusted domains

  - Check source of URLs (prevent open redirects)

  - Validate deep link parameters

  - Use URL parsing (components, query parameters)

  - Implement input validation for all URL-based navigation

  - Prevent open redirects by validating domains

  - Log suspicious URL handling attempts

  '
cwe:
- CWE-601
- CWE-941
mitigation: Validate schemes, hosts, and parameters. Prevent open redirects.

---
id: objc-sec-009
name: Weak Cryptography Detection
category: Security
severity: High
description: Detect weak encryption algorithms and cryptographic misuse
pattern: "(CCCrypt|SecKeyCreateSignature|SecKeyCreateDecryptedData|\n MD5|SHA1|DES|AES|CommonCrypto|SecCertificateRef|\n\
  \ kCCAlgorithmAES|kCCAlgorithmDES|CCAlgorithm|SecKeyAlgorithm).*\n"
check_type: cryptography
vulnerable_patterns:
- 'kCCAlgorithm: kCCAlgorithmDES'
- HMAC-SHA1
- MD5 hash for password storage
- SecKeyCreateSignature without proper algorithm
- CCCrypt(kCCEncryptionMode,kCCAlgorithmDES,...)
safe_patterns:
- 'kCCAlgorithm: kCCAlgorithmAES128'
- 'CCMode: kCCModeCBC with random IV'
- HMAC-SHA256
- 'SecKeyAlgorithm: kSecKeyAlgorithmRSASignatureMessagePKCS1v15SHA256'
- CryptoKit.SHA256.hash(data:)
recommendation: 'Use modern cryptographic practices:

  - Use AES-128 minimum (prefer AES-256) for encryption

  - Use SHA-256+ for hashing (avoid MD5, SHA1)

  - Use HMAC for message authentication

  - Use random IVs and secure key derivation (PBKDF2, bcrypt)

  - Use CryptoKit framework (modern, audited)

  - Avoid ECB mode (use CBC, CTR, GCM)

  - Use authenticated encryption (AES-GCM)

  - Never reuse IVs with same key

  '
cwe:
- CWE-327
mitigation: Use AES-256, SHA-256+, HMAC, CryptoKit, authenticated encryption.

---
id: objc-sec-010
name: Insecure Deserialization Detection
category: Security
severity: High
description: Detect unsafe deserialization of untrusted data
pattern: "(NSKeyedUnarchiver|NSPropertyListSerialization|JSONSerialization|\n NSCoding|NSSecureCoding|unarchiveObjectWithData|unarchiveTopLevelObjectWithData|\n\
  \ propertyListWithData|JSONObjectWithData|decode).*\n"
check_type: deserialization
vulnerable_patterns:
- '[NSKeyedUnarchiver unarchiveObjectWithData:untrustedData]'
- '[NSPropertyListSerialization propertyListWithData:data options:0 format:nil error:nil]'
- '[NSPropertyListSerialization propertyListWithData:data options:NSPropertyListImmutable
  format:nil error:nil]'
- unarchiveTopLevelObjectWithData:data error:nil
- '[NSJSONSerialization JSONObjectWithData:untrustedData options:0 error:nil]'
safe_patterns:
- '[NSKeyedUnarchiver unarchivedObjectOfClass:[MyClass class] fromData:data error:nil]'
- '[NSKeyedUnarchiver unarchiveTopLevelObjectWithData:data error:nil]'
- 'NSPropertyListSerialization:unarchivedObjectOfClass:forKey:fromData:error:'
- NSSecureCoding protocol implementation
- 'JSONDecoder().decode(MyClass.self, from: data)'
recommendation: 'Implement secure deserialization:

  - Use NSSecureCoding protocol instead of NSCoding

  - Use unarchivedObjectOfClass: to restrict types

  - Validate deserialized objects before use

  - Never deserialize from untrusted sources without validation

  - Use Codable/JSONDecoder for JSON (type-safe)

  - Implement NSSecureCoding.supportsSecureCoding = YES

  - Validate all deserialized data

  - Avoid PropertyList deserialization from untrusted sources

  '
cwe:
- CWE-502
mitigation: Use NSSecureCoding, restrict types, validate objects.

---
id: objc-sec-011
name: Path Traversal Detection
category: Security
severity: High
description: Detect path traversal and directory traversal vulnerabilities
pattern: "(NSFileManager|fileManager|NSDocumentDirectory|NSCachesDirectory|\n NSApplicationSupportDirectory|NSTemporaryDirectory|stringByAppendingPathComponent|\n\
  \ stringByAppendingString|NSSearchPathForDirectoriesInDomains|pathWithComponents).*\n"
check_type: path_traversal
vulnerable_patterns:
- NSString *basePath = NSSearchPathForDirectoriesInDomains(...)[0]
- NSString *filePath = [basePath stringByAppendingPathComponent:userInput]
- NSString *path = [NSString stringWithFormat:@"%@/%@", basePath, userPath]
- '[fileManager fileExistsAtPath:[basePath stringByAppendingString:path]]'
safe_patterns:
- NSString *basePath = NSSearchPathForDirectoriesInDomains(...)[0]
- if ([userInput containsString:@".."]) { return nil }
- NSString *filePath = [basePath stringByAppendingPathComponent:filename]
- if (![filePath hasPrefix:basePath]) { return nil }
recommendation: 'Prevent path traversal attacks:

  - Validate all file paths before accessing

  - Check that resolved path is within base directory

  - Reject paths containing \"..\" or absolute paths

  - Use stringByAppendingPathComponent: instead of string concatenation

  - Implement whitelist of allowed files/directories

  - Validate user input matches expected filename format

  - Use realpath() equivalent to resolve full path

  - Check file permissions and ownership

  '
cwe:
- CWE-22
mitigation: Validate paths, reject .., use stringByAppendingPathComponent:.

---
id: objc-sec-012
name: Weak Random Number Generation
category: Security
severity: High
description: Detect insecure random number generation for cryptography
pattern: "(random|arc4random|drand48|rand|srand|srandom|\n RAND_bytes|SecRandomCopyBytes|getRandomBytes).*\n"
check_type: rng
vulnerable_patterns:
- int token = arc4random() % 1000
- srand(time(NULL)); int random = rand()
- double random = drand48()
- srandom(seed); int value = random()
safe_patterns:
- SecRandomCopyBytes(kSecRandomDefault, sizeof(bytes), bytes)
- uint8_t bytes[32]; SecRandomCopyBytes(kSecRandomDefault, 32, bytes)
- CommonCrypto arc4random_buf
- CryptoKit.SecureBytes
recommendation: 'Use cryptographically secure random generation:

  - Use SecRandomCopyBytes for cryptographic randomness

  - Use arc4random_buf for secure random bytes

  - Never use random(), rand(), drand48() for security

  - Use secure sources: /dev/urandom, SecRandomDefault

  - For tokens: use 32+ bytes of secure random data

  - For nonces: use cryptographically secure generation

  - Test randomness with statistical analysis tools

  '
cwe:
- CWE-338
mitigation: Use SecRandomCopyBytes, arc4random_buf, not random/rand.

---
id: objc-sec-013
name: Null Pointer Dereference Detection
category: Security
severity: Medium
description: Detect potential null pointer dereferences and unsafe access
pattern: "(nil|NULL|NSNull|@try|@catch|guard|if.*nil|unwrap|\n if\\s*\\(\\s*\\!\\\
  s*\\w+\\s*\\)|if\\s*\\(\\w+.*==\\s*nil|if\\s*\\(\\w+.*!=\\s*nil).*\n"
check_type: null_safety
vulnerable_patterns:
- '[nilObject method]'
- NSString *str = [dict objectForKey:@"key"]; [str uppercaseString]
- '[array lastObject]'
- self.property.subproperty
- '[results firstObject].property'
safe_patterns:
- if (obj) { [obj method] }
- 'NSString *str = [dict objectForKey:@"key"] ?: @""'
- if (array.count > 0) { id last = array.lastObject }
- if (obj && obj.property) { ... }
recommendation: 'Implement safe null checking:

  - Always check for nil before accessing objects

  - Use optional chaining for property access

  - Use ?: operator for nil coalescing

  - Use guard statements to ensure objects exist

  - Implement nullability annotations (__nullable, __nonnull)

  - Use @try/@catch for exception handling

  - Test with nullability warnings enabled

  - Use _Nonnull in method signatures

  '
cwe:
- CWE-476
mitigation: 'Check for nil, use ?: operator, nullability annotations.'

---
id: objc-sec-014
name: Race Condition Detection
category: Security
severity: Medium
description: Detect potential race conditions in multi-threaded code
pattern: "(@synchronized|dispatch_async|dispatch_sync|dispatch_barrier|\n pthread_mutex|NSLock|NSRecursiveLock|NSConditionLock|\n\
  \ @property.*nonatomic|@property.*atomic|GCD|DispatchQueue).*\n"
check_type: race_condition
vulnerable_patterns:
- '@property (nonatomic) NSMutableArray *array'
- dispatch_async(queue, ^{ [self.array addObject:obj] })
- '[self.dict setObject:val forKey:key] // from multiple threads'
- '@property NSMutableDictionary *mutableDict // accessed from multiple threads'
safe_patterns:
- '@property (atomic) NSMutableArray *array'
- dispatch_barrier_async(queue, ^{ [self.array addObject:obj] })
- '@synchronized(self) { [self.array addObject:obj] }'
- NSLock *lock; [lock lock]; [self.array addObject:obj]; [lock unlock]
recommendation: 'Implement thread-safe synchronization:

  - Use @synchronized for simple synchronization

  - Use NSLock/NSRecursiveLock for granular control

  - Use dispatch_barrier_async for synchronized writes

  - Use dispatch_sync for synchronized reads

  - Consider immutable objects for thread safety

  - Document thread-safety guarantees

  - Use atomic properties for simple properties

  - Test with ThreadSanitizer and race condition detectors

  '
cwe:
- CWE-362
mitigation: '@synchronized, NSLock, dispatch_barrier_async, atomic properties.'

---
id: objc-sec-015
name: Privilege Escalation Detection
category: Security
severity: High
description: Detect privilege escalation and insecure system operations
pattern: "(system|exec|fork|spawn|getuid|setuid|geteuid|seteuid|\n getgid|setgid|getegid|setegid|chmod|chown|sudo|elevated).*\n"
check_type: privilege
vulnerable_patterns:
- system(userCommand)
- execl("/bin/sh", userInput)
- setuid(0) // attempt to become root
- system("rm -rf /") // dangerous
safe_patterns:
- Task execution with checked parameters
- Limited user privileges for operations
- Capability-based security model
recommendation: 'Avoid privilege escalation vulnerabilities:

  - Never call system() with user input

  - Never use setuid/seteuid for privilege escalation

  - Run with least privilege principle

  - Drop privileges after operations

  - Implement capability-based security

  - Validate all shell commands

  - Use elevated privileges only when necessary

  '
cwe:
- CWE-250
- CWE-94
mitigation: Avoid system(), use least privilege, validate commands.

---
id: objc-perf-001
name: Autorelease Pool Management
category: Performance
severity: High
description: Detect inefficient autorelease pool usage causing memory spikes
pattern: "(@autoreleasepool|autorelease|NSAutoreleasePool|\n autoreleasingReferencingObjectAtIndex|drain).*\n"
check_type: autorelease_management
inefficient_patterns:
- for (int i = 0; i < 1000000; i++) { NSString *str = [NSString stringWithFormat:...]
  }
- // No @autoreleasepool in tight loops
- '[array enumerateObjectsUsingBlock:^(id obj, NSUInteger idx, BOOL *stop) { NSString
  *s = [obj description] }]'
efficient_patterns:
- '@autoreleasepool { for (int i = 0; i < 1000000; i++) { NSString *str = ... } }'
- for (int i = 0; i < 1000000; i++) { @autoreleasepool { NSString *str = ... } }
- '[array enumerateObjectsUsingBlock:^(id obj, NSUInteger idx, BOOL *stop) { @autoreleasepool
  { NSString *s = [obj description] } }]'
recommendation: 'Optimize autorelease pool usage:

  - Use @autoreleasepool in tight loops creating temporary objects

  - Drain autorelease pool in loops processing large collections

  - Move @autoreleasepool outside loop if possible

  - Use fast enumeration with autorelease pools

  - Consider using NSMutableString instead of string concatenation

  - Profile with Instruments to identify autorelease spikes

  - Use proper scoping for temporary objects

  '
mitigation: Add @autoreleasepool in tight loops and collection processing.

---
id: objc-perf-002
name: ARC Overhead Detection
category: Performance
severity: Medium
description: Detect excessive retain/release overhead in ARC-managed code
pattern: "(@property.*strong|@property.*copy|retain|release|\n __strong|__weak|__unsafe_unretained|copy|mutableCopy).*\n"
check_type: arc_overhead
inefficient_patterns:
- '@property (copy, nonatomic) NSString *string // string property'
- '@property (strong, nonatomic) NSArray *immutableArray // unnecessary strong'
- NSString *copy = [original copy] // in tight loop
efficient_patterns:
- '@property (nonatomic, copy) NSString *string // correct'
- '@property (strong, nonatomic) NSMutableArray *mutableArray'
- '@property (weak, nonatomic) id<MyDelegate> delegate // prevent cycles'
recommendation: 'Optimize ARC performance:

  - Use __weak for delegates and observers

  - Use __strong only for retained properties

  - Use __unsafe_unretained for performance-critical code with careful lifetime management

  - Avoid excessive copying of immutable objects

  - Use copy properties only for thread-safe immutable objects

  - Batch property updates to reduce retain/release pairs

  - Profile retain/release activity with Instruments

  - Use copy only when thread safety requires it

  '
mitigation: __weak for delegates, avoid unnecessary copy, profile with Instruments.

---
id: objc-perf-003
name: Collection Operation Inefficiency
category: Performance
severity: High
description: Detect inefficient collection operations (O(n) operations in loops)
pattern: "(NSArray|NSMutableArray|NSDictionary|NSMutableDictionary|NSSet|NSMutableSet|\n\
  \ enumerateObjectsUsingBlock|enumerateKeysAndObjectsUsingBlock|\n containsObject|indexOfObject|objectForKey|removeObjectForKey|\n\
  \ for.*in|addObject|removeObject|sortedArray).*\n"
check_type: collection_performance
inefficient_patterns:
- for (id obj in array) { if ([otherArray containsObject:obj]) { ... } } // O(n^2)
- for (id key in dict) { [mutableArray addObject:[dict objectForKey:key]] } // O(n)
- '[array sortedArrayUsingSelector:@selector(compare:)] // recreates array'
- while ([array containsObject:obj]) { [array removeObject:obj] } // O(n^2)
efficient_patterns:
- NSSet *otherSet = [NSSet setWithArray:otherArray]
- for (id obj in array) { if ([otherSet containsObject:obj]) { ... } } // O(n)
- '[array enumerateObjectsUsingBlock:^(id obj, NSUInteger idx, BOOL *stop) { ... }]'
- '[mutableArray sortUsingSelector:@selector(compare:)] // sorts in-place'
recommendation: 'Optimize collection operations:

  - Use NSSet instead of NSArray for containsObject: checks

  - Convert arrays to sets for membership testing (O(1) vs O(n))

  - Use enumerateObjectsUsingBlock: instead of for loops (better memory)

  - Use sortUsingSelector: on mutable arrays (in-place vs copy)

  - Batch removals instead of removing one at a time

  - Use predicates for filtering NSArray

  - Pre-allocate collections with capacity when size known

  - Cache results of expensive operations (count, etc.)

  '
mitigation: Use NSSet for membership, batch operations, use predicates.

---
id: objc-perf-004
name: String Handling Inefficiency
category: Performance
severity: High
description: Detect inefficient string operations and concatenation
pattern: "(stringByAppendingString|stringByAppendingFormat|NSMutableString|\n stringByReplacingOccurrencesOfString|componentsSeparatedByString|\n\
  \ lowercaseString|uppercaseString|stringWithFormat).*\n"
check_type: string_performance
inefficient_patterns:
- NSString *result = @""; for (...) { result = [result stringByAppendingString:part]
  } // O(n^2)
- for (id obj in array) { str = [str stringByAppendingString:[obj description]] }
- '[string stringByReplacingOccurrencesOfString:@"a" withString:@"b"] // multiple
  times'
efficient_patterns:
- NSMutableString *result = [NSMutableString string]
- for (...) { [result appendString:part] } // O(n)
- '[result appendString:[NSString stringWithFormat:@"%d", value]]'
- '[mutableString replaceOccurrencesOfString:@"a" withString:@"b" options:0 range:NSMakeRange(0,
  [mutableString length])]'
recommendation: 'Optimize string operations:

  - Use NSMutableString for concatenation in loops

  - Use stringWithFormat: instead of multiple appends

  - Cache string properties (length, substring operations)

  - Use componentsSeparatedByString: vs manual parsing

  - Use regular expressions for complex replacements

  - Consider using NSScanner for parsing

  - Avoid repeated case conversions

  - Use string pooling for repeated strings

  '
mitigation: Use NSMutableString, cache operations, avoid loops with append.

---
id: objc-perf-005
name: Image Caching Strategy
category: Performance
severity: High
description: Detect missing or inefficient image caching mechanisms
pattern: "(UIImage|imageNamed|imageWithContentsOfFile|imageWithData|\n UIImageView|setImage|sd_setImageWithURL|drawInRect|\n\
  \ NSURLCache|URLCache|ImageCache).*\n"
check_type: image_caching
inefficient_patterns:
- UIImage *image = [UIImage imageWithContentsOfFile:path] // in cell display
- for (NSString *filename in filenames) { UIImage *img = [UIImage imageNamed:filename]
  }
- '[imageView setImage:[UIImage imageWithData:data]] // no caching'
- UIImage *image = [UIImage imageWithData:[NSData dataWithContentsOfURL:url]] // blocking
  main thread
efficient_patterns:
- UIImage *image = [UIImage imageNamed:@"cached"] // cached in bundle
- NSCache *imageCache; id cachedImage = [imageCache objectForKey:path]
- '[imageView sd_setImageWithURL:url placeholderImage:placeholder]'
- dispatch_async(dispatch_get_global_queue(...), ^{ UIImage *img = [UIImage imageWithData:data]
  })
recommendation: 'Implement efficient image caching:

  - Use NSCache for in-memory image caching

  - Implement disk caching for downloaded images

  - Use SDWebImage or similar for network image loading

  - Decode images off main thread

  - Resize images to view size before caching

  - Implement memory warnings handling (clearCache)

  - Use imageWithData: for temporary images only

  - Profile image memory with Instruments

  - Pre-load critical images

  '
mitigation: Use NSCache, SDWebImage, decode off main, resize before cache.

---
id: objc-perf-006
name: Lazy Loading and Initialization
category: Performance
severity: Medium
description: Detect missing lazy loading and initialization patterns
pattern: "(init|initWithFrame|awakeFromNib|viewDidLoad|layoutSubviews|\n dealloc|_|lazy|@synthesize).*\n"
check_type: lazy_loading
inefficient_patterns:
- '- (id)init { ... [self initializeExpensiveObject] ... } // initialize everything'
- UIView *view = [[UIView alloc] init]; view.hidden = YES // never shown
- viewDidLoad loads all views even if not visible
efficient_patterns:
- '- (MyObject *)expensiveObject { if (!_expensiveObject) { _expensiveObject = ...;
  } return _expensiveObject; }'
- '@property (nonatomic, lazy) MyObject *object'
- '- (void)viewDidLoad { // load visible views only }'
recommendation: 'Implement lazy loading and initialization:

  - Defer expensive initialization to first use

  - Create properties on demand with getter checks

  - Use lazy evaluation patterns

  - Load views only when needed (table cell configuration)

  - Defer network requests until visible

  - Implement lazy property initialization

  - Use lazy collections and data structures

  - Monitor initialization performance with Instruments

  '
mitigation: Defer expensive init, check before creation, lazy properties.

---
id: objc-perf-007
name: Memory Allocation Inefficiency
category: Performance
severity: Medium
description: Detect excessive memory allocations and inefficient allocation patterns
pattern: "(malloc|calloc|realloc|alloca|sizeof|NSMutableArray|NSMutableDictionary|\n\
  \ NSMutableString|new|alloc|copy|mutableCopy).*\n"
check_type: memory_allocation
inefficient_patterns:
- for (int i = 0; i < n; i++) { NSMutableArray *arr = [NSMutableArray array] }
- NSMutableArray *array = [[NSMutableArray alloc] init]; // without capacity
- '[array addObject:obj]; [array addObject:obj2]; // repeated grows'
efficient_patterns:
- NSMutableArray *array = [NSMutableArray arrayWithCapacity:expectedSize]
- NSMutableDictionary *dict = [NSMutableDictionary dictionaryWithCapacity:size]
- Reuse collection objects instead of recreating
recommendation: 'Optimize memory allocations:

  - Pre-allocate collections with capacity when size known

  - Reuse collection objects instead of creating new ones

  - Use object pooling for frequently allocated objects

  - Avoid excessive small allocations

  - Use immutable collections when possible (less memory overhead)

  - Profile allocations with Instruments (Allocations tool)

  - Monitor high-water memory mark

  '
mitigation: Pre-allocate with capacity, reuse objects, use object pooling.

---
id: objc-perf-008
name: Main Thread Blocking
category: Performance
severity: Critical
description: Detect blocking operations on main thread causing UI freezes
pattern: "(dispatch_get_main_queue|dispatchOnMainQueue|performSelectorOnMainThread|\n\
  \ dispatch_async.*main|dispatch_sync.*main|\n NSData.*URL|NSString.*URL|URLSession|dataTaskWithURL|\n\
  \ viewDidLoad|viewWillAppear|drawRect|layoutSubviews).*\n"
check_type: main_thread_blocking
inefficient_patterns:
- viewDidLoad { NSData *data = [NSData dataWithContentsOfURL:url] } // blocking network
- dispatch_sync(dispatch_get_main_queue(), ^{ ... }) // deadlock risk
- '[NSString stringWithContentsOfURL:url encoding:NSUTF8StringEncoding error:nil]
  // blocking'
- '- (void)drawRect:(CGRect)rect { [self loadDataFromDisk] }'
efficient_patterns:
- dispatch_async(dispatch_get_global_queue(...), ^{
- '  NSData *data = [NSData dataWithContentsOfURL:url]'
- '  dispatch_async(dispatch_get_main_queue(), ^{ [self updateUI:data] })'
- '})'
recommendation: 'Avoid blocking main thread:

  - Move network operations to background queue

  - Move file I/O to background queue

  - Move database operations to background queue

  - Use dispatch_async to avoid deadlocks

  - Profile with Main Thread Checker (Xcode)

  - Set os_log points to identify blocking operations

  - Use NSURLSession for network (built-in queue support)

  - Keep main thread for UI updates only

  '
mitigation: dispatch_async for background work, Main Thread Checker.

---
id: objc-perf-009
name: View Hierarchy Complexity
category: Performance
severity: Medium
description: Detect overly complex view hierarchies causing rendering issues
pattern: "(UIView|addSubview|insertSubview|UIStackView|\n layoutSubviews|drawRect|CALayer|cornerRadius|\n\
  \ masksToBounds|shadowPath|opacity).*\n"
check_type: view_hierarchy
inefficient_patterns:
- // Deeply nested view hierarchies (>10 levels)
- view.layer.cornerRadius = 50; view.layer.masksToBounds = YES // rasterizes
- view.layer.shadowPath = nil // expensive shadow
- '[superview addSubview:view] // in loop without batching'
efficient_patterns:
- UIStackView for layout (flattens hierarchy)
- view.layer.cornerRadius = 50 WITH view.layer.shadowPath = [UIBezierPath bezierPathWithRoundedRect:...]
- Use CADisplayLink for smooth animations
- 'Batch view creation: CATransaction { [superview addSubview:v] }'
recommendation: 'Optimize view hierarchies:

  - Keep view depth < 10 levels

  - Use UIStackView for dynamic layouts

  - Set shadowPath explicitly (not nil/null)

  - Use rasterization sparingly (disable when animating)

  - Batch view updates with CATransaction

  - Pre-render static content (image snapshots)

  - Use CADisplayLink for animation

  - Profile rendering with Core Animation tool

  '
mitigation: Flatten hierarchy, UIStackView, explicit shadowPath, CATransaction.

---
id: objc-perf-010
name: Scrolling Performance Issues
category: Performance
severity: High
description: Detect UITableView/UICollectionView scrolling performance problems
pattern: "(UITableViewCell|UICollectionViewCell|cellForRowAtIndexPath|\n cellForItemAtIndexPath|dequeueReusableCellWithIdentifier|\n\
  \ heightForRowAtIndexPath|estimatedHeightForRowAtIndexPath|\n scrollViewDidScroll|willDisplayCell).*\n"
check_type: scroll_performance
inefficient_patterns:
- '- (UITableViewCell *)tableView:(UITableView *)table cellForRowAtIndexPath:(NSIndexPath
  *)indexPath {'
- '  cell.imageView.image = [UIImage imageWithContentsOfFile:path] // main thread'
- '  cell.textLabel.text = [expensiveObject description]'
- '  return cell'
- '}'
- '- (CGFloat)tableView:(UITableView *)table heightForRowAtIndexPath:(NSIndexPath
  *)indexPath {'
- '  return [self calculateHeightForRow:indexPath] // expensive'
- '}'
efficient_patterns:
- '- (void)configureCell:(UITableViewCell *)cell forIndexPath:(NSIndexPath *)indexPath'
- '  cell.imageView.image = [self.imageCache objectForKey:indexPath]'
- '}'
- '- (CGFloat)tableView:(UITableView *)table estimatedHeightForRowAtIndexPath:(NSIndexPath
  *)indexPath'
- '  return 44.0 // estimated'
- '}'
recommendation: 'Optimize table/collection view scrolling:

  - Use estimatedHeightForRowAtIndexPath instead of exact height

  - Pre-calculate row heights and cache

  - Cache cell heights in NSIndexPath-indexed dictionary

  - Load images asynchronously

  - Avoid complex calculations in cellForRowAtIndexPath

  - Reuse cells properly (dequeueReusableCellWithIdentifier)

  - Pre-render complex cell content

  - Use shouldHighlightRowAtIndexPath to skip unnecessary work

  - Profile with Core Animation and Time Profiler

  '
mitigation: Use estimated heights, cache, async images, pre-render cells.

---
id: objc-perf-011
name: Database Query Inefficiency
category: Performance
severity: High
description: Detect inefficient Core Data/SQLite queries
pattern: "(NSFetchRequest|NSPredicate|NSFetchedResultsController|\n sqlite3_prepare|sqlite3_step|sqlite3_finalize|\n\
  \ fetchAllObjects|executeFetchRequest|NSManagedObjectContext).*\n"
check_type: database_performance
inefficient_patterns:
- NSFetchRequest *request = [NSFetchRequest fetchRequestWithEntityName:@"Entity"]
- '[context executeFetchRequest:request error:nil] // no predicate, fetches all'
- for (Entity *obj in results) { [obj.relationships count] } // faults loaded
- NSFetchRequest without batch size // loads all to memory
efficient_patterns:
- NSFetchRequest *request = [NSFetchRequest fetchRequestWithEntityName:@"Entity"]
- request.predicate = [NSPredicate predicateWithFormat:@"status = %@", @"active"]
- request.relationshipKeyPathsForPrefetching = @[@"related"]
- request.returnsObjectsAsFaults = NO
- request.fetchBatchSize = 100
recommendation: 'Optimize database queries:

  - Use predicates to filter early (server-side filtering)

  - Use relationshipKeyPathsForPrefetching for joins

  - Set appropriate fetchBatchSize (default 0 = all)

  - Use returnsObjectsAsFaults for efficiency

  - Index frequently queried columns

  - Use NSFetchedResultsController for table views

  - Avoid N+1 queries (faulting relationships)

  - Profile with Core Data debugging tools

  '
mitigation: Use predicates, prefetch, batch size, NSFetchedResultsController.

---
id: objc-perf-012
name: Regular Expression Performance
category: Performance
severity: Medium
description: Detect inefficient regular expression usage
pattern: "(NSRegularExpression|enumerateMatchesInString|\n NSRegularExpressionSearch|componentsSeparatedByRegex|\n\
  \ stringByMatching|stringByReplacingOccurrences).*\n"
check_type: regex_performance
inefficient_patterns:
- for (NSString *str in array) {
- '  NSRegularExpression *regex = [NSRegularExpression ... pattern:regexPattern ...]'
- '  [regex enumerateMatchesInString:str ...]'
- '} // regex compiled each time'
efficient_patterns:
- NSRegularExpression *regex = [NSRegularExpression regularExpressionWithPattern:pattern
  options:0 error:nil]
- for (NSString *str in array) {
- '  [regex enumerateMatchesInString:str ...]'
- '}'
recommendation: 'Optimize regular expression usage:

  - Cache compiled regex objects (not create per match)

  - Use NSRegularExpressionCaseInsensitive option for optimization

  - Use NSRegularExpressionAnchorsMatchLines for multiline

  - Consider simple string operations for basic patterns

  - Use NSScanner for simple parsing instead of regex

  - Profile regex performance on large strings

  - Consider precompiled/optimized patterns

  '
mitigation: Cache regex, compile once, reuse for multiple matches.

---
id: objc-perf-013
name: Animation Performance
category: Performance
severity: Medium
description: Detect inefficient animation implementations
pattern: "(UIView.*animateWithDuration|CABasicAnimation|CAKeyframeAnimation|\n CADisplayLink|CATransaction|[CATransaction\
  \ begin|[CATransaction commit|\n animationWithKeyPath|opacity|position|scale|cornerRadius).*\n"
check_type: animation_performance
inefficient_patterns:
- '[UIView animateWithDuration:0.3 animations:^{'
- '  for (int i = 0; i < 100; i++) {'
- '    view.alpha = 1.0 - (i / 100.0) // expensive'
- '  }'
- '}]'
efficient_patterns:
- CABasicAnimation *anim = [CABasicAnimation animationWithKeyPath:@"opacity"]
- anim.fromValue = @(1.0); anim.toValue = @(0.0)
- anim.duration = 0.3
- '[view.layer addAnimation:anim forKey:@"opacity"]'
recommendation: 'Optimize animations:

  - Use CABasicAnimation instead of block animations for smooth 60fps

  - Use CADisplayLink for complex animations

  - Avoid animating complex views (pre-render/snapshot)

  - Disable shadows and rounded corners during animation

  - Use CATransaction for batch animations

  - Profile with Core Animation and Time Profiler

  - Use shouldRasterize sparingly (disable after animation)

  '
mitigation: Use CABasicAnimation, CADisplayLink, disable effects during anim.

---
id: objc-perf-014
name: Framework Load Time Optimization
category: Performance
severity: Medium
description: Detect excessive framework loading and initialization overhead
pattern: "(didFinishLaunchingWithOptions|applicationDidBecomeActive|\n import|@import|Framework|dyld|DYLD_|launch|startup|load).*\n"
check_type: framework_loading
inefficient_patterns:
- // Importing many frameworks in main thread
- '[self initializeAllFrameworks] // in didFinishLaunching'
- dispatch_sync(mainQueue) // in framework init
efficient_patterns:
- Lazy load frameworks on first use
- Initialize non-critical frameworks on background
- Use modulemap for framework management
recommendation: 'Optimize framework loading:

  - Measure app launch time with Instruments

  - Lazy load frameworks on first use

  - Defer non-critical initialization

  - Profile with Time Profiler tool

  - Consider removing unused frameworks

  - Use dynamic linking for optional features

  '
mitigation: Lazy load, defer init, measure launch time with Instruments.

---
id: objc-qual-001
name: Apple Naming Conventions
category: Quality
severity: Medium
description: Enforce Apple-style naming conventions for classes, methods, properties
pattern: '(^[A-Z]|^[a-z].*:|method|property|variable|class|protocol).*

  '
check_type: naming_conventions
violations:
- class myClass { } // should be MyClass
- '@property NSString *strValue; // should be string'
- '- (void)GetValue { } // should be getValue (camelCase)'
- '@property String *str; // should be NSString'
- '- (void)PROCESS_DATA { } // should be processData'
correct_usage:
- class MyClass { } // PascalCase for classes
- '@property (nonatomic) NSString *string; // camelCase for properties'
- '- (void)getValue { } // camelCase for methods'
- '- (void)setName:(NSString *)name { } // setter with (void)'
- '- (BOOL)isEnabled { } // boolean with ''is'' prefix'
recommendation: 'Follow Apple naming conventions:

  - Classes: PascalCase (MyViewController, NSMutableArray)

  - Properties/variables: camelCase (firstName, isEnabled)

  - Methods: camelCase (getValue:, processData)

  - Constants: ALL_CAPS with K prefix (kMaxRetries, kPIValue)

  - Protocols: PascalCase with Delegate/DataSource suffix (MyDelegate)

  - Getters: getPropertyName: only for expensive operations

  - Setters: setPropertyName: or property = syntax

  - Booleans: is/has/should prefix (isEnabled, hasData, shouldRetry)

  - Prefixes: Use 2-3 letter class prefix to avoid conflicts

  '
mitigation: Use PascalCase for classes, camelCase for methods/properties.

---
id: objc-qual-002
name: Property Attributes
category: Quality
severity: High
description: Enforce proper property attribute declarations
pattern: "(@property|nonatomic|atomic|strong|weak|copy|assign|\n readonly|readwrite|getter|setter).*\n"
check_type: property_attributes
violations:
- '@property NSString *name; // missing nonatomic'
- '@property (strong) NSMutableArray *array; // redundant, default'
- '@property (retain) NSString *str; // use strong in ARC'
- '@property (assign) id delegate; // should use weak'
correct_usage:
- '@property (nonatomic, copy) NSString *name; // immutable'
- '@property (nonatomic, strong) NSMutableArray *array; // mutable'
- '@property (weak, nonatomic) id<MyDelegate> delegate; // break cycles'
- '@property (nonatomic, readonly) NSInteger identifier; // read-only'
- '@property (nonatomic, getter=isEnabled) BOOL enabled; // custom getter'
recommendation: 'Use correct property attributes:

  - nonatomic: almost always (atomic expensive, rarely needed)

  - strong: for owned objects (default in ARC)

  - weak: for delegates, observers, self-referential objects

  - copy: for immutable objects (NSString, NSArray, NSData)

  - assign: rarely (for primitives and weak non-ARC pointers)

  - readonly/readwrite: explicit when limiting access

  - getter/setter: custom accessors for special handling

  - Avoid atomic unless truly thread-safe required

  '
mitigation: Add nonatomic, use weak for delegates, copy for immutable.

---
id: objc-qual-003
name: Nullability Annotations
category: Quality
severity: High
description: Enforce nullability annotations for type safety
pattern: "(_Nullable|_Nonnull|nullable|nonnull|null_resettable|\n __nullable|__nonnull|__null_unspecified).*\n"
check_type: nullability
violations:
- '- (NSString *)getName { } // unclear if can return nil'
- '- (void)setName:(NSString *)name { } // can name be nil?'
- '@property NSArray *items; // unclear if mutable'
correct_usage:
- '- (nullable NSString *)getName { } // can return nil'
- '- (void)setName:(nonnull NSString *)name { } // name cannot be nil'
- '@property (nonnull) NSArray *items; // never nil'
- '- (nullable NSError **)error; // out parameter'
- '@property (nonnull, copy) NSString *identifier; // never nil, copy for safety'
recommendation: 'Add nullability annotations:

  - Use _Nonnull for parameters and properties that cannot be nil

  - Use _Nullable for return values and out parameters that can be nil

  - Use __null_unspecified for legacy code (migrate over time)

  - Use NS_ASSUME_NONNULL_BEGIN/END for bulk annotations

  - Annotate all public API headers

  - Enable warnings: -Wnullability-completeness

  - Document nil behavior in method comments

  '
mitigation: Add _Nonnull/_Nullable, use NS_ASSUME_NONNULL_BEGIN.

---
id: objc-qual-004
name: Memory Management Patterns
category: Quality
severity: High
description: Enforce proper ARC memory management patterns
pattern: "(dealloc|release|retain|autorelease|copy|strong|weak|\n __strong|__weak|NSZombieEnabled|MallocStackLogging).*\n"
check_type: memory_patterns
violations:
- // Missing dealloc for observer cleanup
- // Retaining self in block causing cycle
- '- (void)setupBlock { self.block = ^{ [self doSomething] } }'
- __strong Object *obj = strongRef; // unnecessary
correct_usage:
- '- (void)dealloc { [[NSNotificationCenter defaultCenter] removeObserver:self] }'
- __weak typeof(self) weakSelf = self; self.block = ^{ [weakSelf doSomething] }
- '@property (weak) id<MyDelegate> delegate; // no retain cycle'
- '@interface MyClass { NSMutableArray *_internal; }'
recommendation: 'Follow ARC memory management:

  - Use ARC exclusively (no manual retain/release)

  - Use __weak in blocks to break retain cycles

  - Implement dealloc for cleanup (observers, timers, etc.)

  - Use __strong only when explicitly needed

  - Document retain cycle risks in comments

  - Test with Leaks and Allocations instruments

  - Use Weak/Strong Dance pattern: weakSelf/strongSelf in blocks

  - Avoid circular references through delegates

  '
mitigation: __weak in blocks, dealloc for cleanup, no manual ARC.

---
id: objc-qual-005
name: Protocol Design
category: Quality
severity: Medium
description: Enforce proper protocol design and compliance
pattern: "(@protocol|@required|@optional|delegate|dataSource|\n conformsToProtocol|respondsToSelector).*\n"
check_type: protocol_design
violations:
- '@protocol MyProtocol // no methods (unclear intent)'
- '- (void)update; // vague, unclear parameters'
- '@protocol MyDelegate <NSObject> // missing NSCopying adoption'
- // Protocol mixing delegates and data source responsibilities
correct_usage:
- '@protocol MyDelegate <NSObject>'
- '@required'
- '- (void)updateWithData:(nonnull NSData *)data;'
- '@optional'
- '- (void)didFinishWithError:(nullable NSError *)error;'
- '@end'
recommendation: 'Design protocols properly:

  - Inherit from NSObject for compatibility

  - Separate delegates from data sources (single responsibility)

  - Use @required for mandatory methods

  - Use @optional for optional behavior (with respondsToSelector checks)

  - Document parameter meanings (especially NULL/nil cases)

  - Use nullability annotations in protocol declarations

  - Keep protocols focused and cohesive

  - Use protocol composition for complex requirements

  '
mitigation: Separate concerns, use @required/@optional, annotate nullability.

---
id: objc-qual-006
name: Error Handling Patterns
category: Quality
severity: High
description: Enforce proper error handling and exception use
pattern: "(@try|@catch|@finally|@throw|NSError|NSException|\n error:|completionHandler|resultHandler).*\n"
check_type: error_handling
violations:
- '- (NSString *)loadFile:(NSString *)path error:(NSError **)error {'
- '  [UIImage imageNamed:path] // ignores error, crashes on missing'
- '  return data'
- '}'
- '@try { ... } @catch (...) { } @finally { } // bare catches'
correct_usage:
- '- (nullable NSData *)loadFile:(nonnull NSString *)path error:(NSError * _Nullable
  *)error {'
- '  if (!path || !path.length) {'
- '    if (error) { *error = [NSError errorWithDomain:... code:... userInfo:...] }'
- '    return nil'
- '  }'
- '  return data'
- '}'
- '@try { ... } @catch (NSException *ex) { /* handle */ } @finally { /* cleanup */
  }'
recommendation: 'Implement proper error handling:

  - Use NSError for recoverable errors (not exceptions)

  - Use exceptions only for programming errors

  - Return nil with error pointer for failures

  - Validate error pointer before dereferencing (if (error) { *error = ... })

  - Use error domains and codes for categorization

  - Include NSUnderlyingError for error chains

  - Log errors but don''t expose to users

  - Provide recovery suggestions in userInfo

  - Use @try/@catch only for exception handling

  '
mitigation: Use NSError for failures, check error pointer, validate inputs.

---
id: objc-qual-007
name: Constants and Enums
category: Quality
severity: Medium
description: Enforce proper constant and enum declarations
pattern: "(const|NS_ENUM|NS_OPTIONS|typedef|#define|static|\n extern|kConstant|Constant).*\n"
check_type: constants_enums
violations:
- '#define kMaxRetries 5 // unsafe macro'
- static NSString *GlobalString = nil // mutable global
- enum Status { OK = 0, ERROR = 1 } // untyped enum
- extern int MAX_SIZE; // unsafe, should be const
correct_usage:
- static const NSInteger kMaxRetries = 5; // type-safe
- static NSString * const kErrorDomain = @"com.app.error"; // immutable
- typedef NS_ENUM(NSInteger, Status) { StatusOK = 0, StatusError = 1 }
- typedef NS_OPTIONS(NSUInteger, Permissions) { PermissionRead = 1, PermissionWrite
  = 2 }
- extern NSString * const kMyConstant; // in header
recommendation: 'Declare constants properly:

  - Use static const for primitive constants

  - Use extern for public constants (declare in header, define in implementation)

  - Use NS_ENUM for enumeration types

  - Use NS_OPTIONS for bitmask options

  - Avoid #define (use const instead)

  - Prefix constants with k (kMaxRetries, kAppName)

  - Use stringification for string constants

  - Document constant meanings and usage

  '
mitigation: Use NS_ENUM/NS_OPTIONS, static const, extern for public.

---
id: objc-qual-008
name: Method Signature Clarity
category: Quality
severity: Medium
description: Enforce clear and descriptive method signatures
pattern: '(^-|^\\+|\\(void\\)|\\(BOOL\\)|\\(id\\)|parameter|argument|return).*

  '
check_type: method_clarity
violations:
- '- (id)process { } // vague return type'
- '- (void)set:(id)value { } // unclear parameter'
- '- (void)doSomething:(NSString *)str other:(id)o { } // unclear'
- '- (NSString *)getValue { } // unclear what gets value'
correct_usage:
- '- (nullable id<MyDelegate>)delegate { return _delegate; }'
- '- (void)setValue:(nonnull id)value forKey:(nonnull NSString *)key { }'
- '- (void)updateWithData:(nonnull NSData *)data error:(NSError **)error { }'
- '- (nullable NSError *)validateWithOptions:(NSDictionary *)options { }'
recommendation: 'Write clear method signatures:

  - Use descriptive parameter names (user, not u)

  - Indicate parameter purpose with descriptive names

  - Use completion blocks for asynchronous results

  - Return values that indicate success/failure

  - Use error pointers for recoverable errors

  - Document complex parameter meanings

  - Keep signatures to 3-4 parameters maximum

  - Use factory methods for common initializations

  '
mitigation: Use descriptive names, clear parameters, error pointers.

---
id: objc-qual-009
name: Documentation Standards
category: Quality
severity: Medium
description: Enforce proper code documentation and comments
pattern: '(///|@param|@return|@throws|@warning|@deprecated|MARK|pragma).*

  '
check_type: documentation
violations:
- '- (void)complexMethod { } // no documentation'
- /**/ // incomplete documentation
- '@property NSString *data; // no comment explaining purpose'
correct_usage:
- /**
- ' * Processes the input data and returns the result.'
- ' *'
- ' * @param data The data to process.'
- ' * @return The processed result, or nil if processing fails.'
- ' * @warning This method blocks the calling thread.'
- ' */'
- '- (nullable id)processData:(nonnull NSData *)data;'
recommendation: 'Document code properly:

  - Add header comments for all public methods

  - Document parameters, return values, and exceptions

  - Use @param, @return, @throws style documentation

  - Add @warning for non-obvious behavior

  - Use @deprecated for obsolete methods

  - Add @see for related methods

  - Use // for inline explanations

  - Use #pragma mark for section organization

  - Document thread safety guarantees

  '
mitigation: Add documentation comments, use standard format, document warnings.

---
id: objc-qual-010
name: Type Safety and Casting
category: Quality
severity: High
description: Enforce type safety and proper object casting
pattern: '(id|NSObject|cast|as|typeof|isKindOfClass|isMemberOfClass).*

  '
check_type: type_safety
violations:
- NSString *str = (NSString *)obj; // unsafe cast
- if ([obj class] == [NSString class]) { } // direct class comparison
- '[array objectAtIndex:0]; // returns id, type unknown'
- NSObject *obj = ...; [obj doSomething]; // unknown method
correct_usage:
- if ([obj isKindOfClass:[NSString class]]) { NSString *str = (NSString *)obj; }
- '@try { NSString *str = (NSString *)obj; } @catch (...) { }'
- id result = array.firstObject; NSString *str = nil;
- if ([result isKindOfClass:[NSString class]]) { str = result; }
recommendation: 'Ensure type safety:

  - Check type with isKindOfClass: before casting

  - Use respondsToSelector: for dynamic method calls

  - Avoid unsafe casts (use if statements with checks)

  - Use generic collections (NSArray<NSString *> *)

  - Document expected types in comments

  - Use explicit type annotations

  - Use -Wstrict-selector-match for selector checking

  '
mitigation: Check type before casting, use generics, respondsToSelector:.

---
id: objc-qual-011
name: Initialization Patterns
category: Quality
severity: Medium
description: Enforce proper initialization and designated initializers
pattern: '(init|initWith|new|alloc|designated|initializer).*

  '
check_type: initialization
violations:
- '- (id)init { return [super init]; } // incomplete init'
- // Multiple init methods without designated pattern
- '[[MyClass alloc] init] // correct usage'
- '- (id)initWithValue:(id)val { self.value = val; return self; }'
correct_usage:
- '- (instancetype)initWithValue:(nonnull id)value {'
- '  self = [super init];'
- '  if (!self) return nil;'
- '  _value = value;'
- '  return self;'
- '}'
- '- (instancetype)initWithValue:(nonnull id)value name:(nonnull NSString *)name {'
- '  // Call designated initializer'
- '  return [self initWithValue:value];'
- '}'
recommendation: 'Follow initialization patterns:

  - Use instancetype as return type (not id)

  - Implement [super init] first and check result

  - Establish one designated initializer per class

  - Other inits should call designated initializer

  - Return nil on initialization failure

  - Initialize all ivars in designated init

  - Use lazy initialization for expensive properties

  - Document required initialization steps

  '
mitigation: Use instancetype, call super init, designated pattern.

---
id: objc-qual-012
name: Category Usage
category: Quality
severity: Medium
description: Enforce proper category design and naming
pattern: '(@interface.*\\(|category|+|extension).*

  '
check_type: category_design
violations:
- '@interface NSString (MyExtension) // no prefix'
- '@interface MyClass (SomethingRandom) // multiple categories conflict'
- '@interface NSString (Transform) @property ... // property in category'
correct_usage:
- '@interface NSString (APP_MyExtension) // app-prefixed'
- '@interface MyClass (Transform)'
- '- (NSString *)uppercaseFirstLetter;'
- '@end'
- '// Separate files: MyClass+Transform.h/m'
recommendation: 'Use categories properly:

  - Use app/framework prefix to avoid collisions

  - One category per file (MyClass+Transform.m)

  - Don''t add properties to categories (use private class extension)

  - Don''t override existing methods in categories

  - Use for organizing related methods

  - Document cross-file dependencies

  - Consider private extensions for internal organization

  - Use +functionName for functions in categories

  '
mitigation: Use prefixes, one category per file, avoid properties.

---
id: objc-qual-013
name: Compatibility Annotations
category: Quality
severity: Medium
description: Enforce deprecation and availability annotations
pattern: "(DEPRECATED_ATTRIBUTE|NS_DEPRECATED|__attribute__|\n UNAVAILABLE_ATTRIBUTE|NS_UNAVAILABLE|available|introduced|deprecated).*\n"
check_type: compatibility
violations:
- '- (void)oldMethod { } // no deprecation marking'
- // No indication of iOS version requirements
- '- (void)newMethod // no availability marking'
correct_usage:
- '- (void)oldMethod NS_DEPRECATED_IOS(2_0, 5_0, "Use -newMethod instead")'
- '@available(iOS 13.0, *) - (void)modernMethod { }'
- API_AVAILABLE(ios(13.0)) - (void)newMethod { }
- NS_UNAVAILABLE - (void)unsupportedMethod { }
recommendation: 'Mark compatibility properly:

  - Use NS_DEPRECATED_IOS for deprecated methods

  - Use NS_UNAVAILABLE for removed methods

  - Use @available for version-specific code

  - Document replacement methods

  - Include minimum version requirements

  - Use __attribute__ for compiler directives

  - Test on minimum supported version

  '
mitigation: Add deprecation marks, @available, NS_UNAVAILABLE.

---
id: objc-qual-014
name: Assertion and Defensive Programming
category: Quality
severity: Medium
description: Enforce defensive programming with assertions
pattern: "(NSAssert|NSAssert[1-5]|NSParameterAssert|NSCAssert|\n assert|@throw|precondition|postcondition).*\n"
check_type: assertions
violations:
- '- (void)setName:(NSString *)name { _name = name; } // no validation'
- '- (NSString *)getName { return _name; } // could be nil'
- NSArray *array = ...; [array objectAtIndex:index] // no bounds check
correct_usage:
- '- (void)setName:(nonnull NSString *)name {'
- '  NSParameterAssert(name);'
- '  NSParameterAssert(name.length > 0);'
- '  _name = [name copy];'
- '}'
- '- (void)insertObject:(nonnull id)obj atIndex:(NSUInteger)idx {'
- '  NSAssert(idx <= [_items count], @"Index out of bounds");'
- '  [_items insertObject:obj atIndex:idx];'
- '}'
recommendation: 'Use assertions for defense:

  - Use NSParameterAssert for parameter validation

  - Use NSAssert for invariant validation

  - Use NSCAssert for C code assertions

  - Assertions compile out in Release builds

  - Add assertions after guard statements

  - Use descriptive assertion messages

  - Test with assertions enabled

  - Don''t rely on assertions for user input validation

  '
mitigation: Add NSParameterAssert, NSAssert for validation.

---
id: objc-qual-015
name: Code Organization and Structure
category: Quality
severity: Medium
description: Enforce proper code organization within files
pattern: "(pragma mark|MARK|interface|implementation|synthesize|\n extension|private|public|protected).*\n"
check_type: code_organization
violations:
- // No organization, methods in random order
- '@interface/@implementation mixed'
- // Private and public methods interleaved
correct_usage:
- '@interface MyClass ()'
- '@property (nonatomic) NSString *private;'
- '@end'
- '@implementation MyClass'
- '#pragma mark - Lifecycle'
- '- (instancetype)init { ... }'
- '- (void)dealloc { ... }'
- '#pragma mark - Public Methods'
- '- (void)publicMethod { ... }'
- '#pragma mark - Private Methods'
- '- (void)privateMethod { ... }'
- '#pragma mark - Getters & Setters'
- '@end'
recommendation: 'Organize code properly:

  - Use #pragma mark for section organization

  - Group related methods together

  - Order: lifecycle, public, private, getters/setters

  - Use private class extension for private properties

  - Keep implementation organized by functionality

  - Use separate files for categories

  - Limit file size (<500 lines is good)

  - Document complex sections with comments

  '
mitigation: 'Use #pragma mark, organize by section, private extensions.'

---
id: objc-qual-016
name: Performance Profiling Markers
category: Quality
severity: Low
description: Enforce use of performance profiling markers
pattern: "(os_log|os_signpost|signpost_interval_begin|signpost_interval_end|\n SIGNPOST_|OSLog).*\n"
check_type: profiling_markers
inefficient_patterns:
- // No profiling markers in critical paths
- // Complex operations without timing
efficient_patterns:
- 'os_log(.info, log: OSLog.default, "Starting operation")'
- 'os_signpost(.begin, log: OSLog.default, name: "DataLoad")'
- '[performWork]'
- 'os_signpost(.end, log: OSLog.default, name: "DataLoad")'
recommendation: 'Add profiling markers:

  - Use os_log for informational logging

  - Use os_signpost for performance intervals

  - Mark critical operations for profiling

  - Test with Instruments signpost tool

  - Use appropriate log levels (debug, info, error)

  - Profile on actual devices

  - Remove debug logs in production

  '
mitigation: Add os_signpost markers for profiling, use os_log.

---
id: react-xss-dangerously-inner-html
tags:
- Security
severity: error
message: 'Quality: XSS via dangerouslySetInnerHTML'
note: '|'
language: javascript

---
id: react-state-management-side-effects
tags:
- Quality
severity: error
message: 'Quality: State Management and Side Effects Anti-patterns'
note: '|'
language: javascript

---
id: react-unnecessary-rerenders
tags:
- Performance
severity: warning
message: 'Quality: Unnecessary Component Re-renders'
note: '|'
language: javascript

---
id: react-memo-prop-drilling
tags:
- Performance
severity: warning
message: 'Quality: memo() Ineffective with Deep Props'
note: '|'
language: javascript

---
id: react-useeffect-cleanup
tags:
- Quality
severity: error
message: 'Quality: Missing useEffect Cleanup Functions'
note: '|'
language: javascript

---
id: react-native-secure-storage
tags:
- Security
severity: error
message: 'Quality: Storing Sensitive Data Insecurely'
note: '|'
language: javascript

---
id: angular-template-injection
tags:
- Security
severity: error
message: 'Quality: Template Injection and XSS'
note: '|'
language: typescript

---
id: angular-change-detection
tags:
- Performance
severity: warning
message: 'Quality: Inefficient Change Detection Strategy'
note: '|'
language: typescript

---
id: angular-unsubscribe-memory-leak
tags:
- Quality
severity: error
message: 'Quality: Unsubscribed Observables Cause Memory Leaks'
note: '|'
language: typescript

---
id: angular-trackby-ngfor
tags:
- Performance
severity: warning
message: 'Quality: Missing trackBy in *ngFor Loops'
note: '|'
language: typescript

---
id: vue-v-html-xss
tags:
- Security
severity: error
message: 'Quality: XSS via v-html Directive'
note: '|'
language: javascript

---
id: vue-reactive-pitfalls
tags:
- Quality
severity: error
message: 'Quality: Reactive Object Pitfalls and Anti-patterns'
note: '|'
language: javascript

---
id: vue-computed-vs-methods
tags:
- Performance
severity: warning
message: 'Quality: Computed Properties vs Methods Optimization'
note: '|'
language: javascript

---
id: vue-watcher-cleanup
tags:
- Quality
severity: error
message: 'Quality: Watcher Memory Leaks and Cleanup'
note: '|'
language: javascript

---
id: django-sql-injection
tags:
- Security
severity: error
message: 'Quality: SQL Injection via Raw Queries'
note: '|'
language: python

---
id: django-xss-template-injection
tags:
- Security
severity: error
message: 'Quality: XSS and Template Injection'
note: '|'
language: python

---
id: django-csrf-bypass
tags:
- Security
severity: error
message: 'Quality: CSRF Protection Bypass'
note: '|'
language: python

---
id: django-n-plus-one-queries
tags:
- Performance
severity: error
message: 'Quality: N+1 Query Problem'
note: '|'
language: python

---
id: django-settings-security
tags:
- Security
severity: error
message: 'Quality: Insecure Settings Configuration'
note: '|'
language: python

---
id: django-model-mass-assignment
tags:
- Security
severity: warning
message: 'Quality: Mass Assignment / Over-posting'
note: '|'
language: python

---
id: spring-spel-injection
tags:
- Security
severity: error
message: 'Quality: SpEL (Spring Expression Language) Injection'
note: '|'
language: java

---
id: spring-mass-assignment
tags:
- Security
severity: error
message: 'Quality: Mass Assignment / Unsafe Deserialization'
note: '|'
language: java

---
id: spring-actuator-exposure
tags:
- Security
severity: error
message: 'Quality: Exposed Actuator Endpoints'
note: '|'
language: java

---
id: spring-connection-pooling
tags:
- Performance
severity: warning
message: 'Quality: Database Connection Pool Misconfiguration'
note: '|'
language: java

---
id: spring-caching-pitfalls
tags:
- Quality
severity: warning
message: 'Quality: Caching Issues and Cache Invalidation'
note: '|'
language: java

---
id: spring-bean-scope-confusion
tags:
- Quality
severity: warning
message: 'Quality: Bean Scope Misuse and Thread Safety'
note: '|'
language: java

---
id: rails-mass-assignment
tags:
- Security
severity: error
message: 'Quality: Mass Assignment Vulnerability'
note: '|'
language: ruby

---
id: rails-sql-injection
tags:
- Security
severity: error
message: 'Quality: SQL Injection'
note: '|'
language: ruby

---
id: rails-xss-prevention
tags:
- Security
severity: error
message: 'Quality: XSS via Unsafe HTML Rendering'
note: '|'
language: ruby

---
id: rails-n-plus-one-queries
tags:
- Performance
severity: error
message: 'Quality: N+1 Query Problem'
note: '|'
language: ruby

---
id: rails-caching-invalidation
tags:
- Performance
severity: warning
message: 'Quality: Cache Invalidation and Fragment Caching'
note: '|'
language: ruby

---
id: rails-convention-patterns
tags:
- Quality
severity: warning
message: 'Quality: Rails Convention Over Configuration Best Practices'
note: '|'
language: ruby

---
id: rails-testing-patterns
tags:
- Quality
severity: warning
message: 'Quality: Testing Best Practices and Patterns'
note: '|'
language: ruby
